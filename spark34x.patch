--- yt/spark/spark-over-yt/build.sbt	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/build.sbt	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -11,18 +11,6 @@ lazy val `spark-adapter-api` = (project in file("spark-adapter/api"))
     libraryDependencies ++= defaultSpark,
   )
 
-lazy val `spark-adapter-impl-322` = (project in file(s"spark-adapter/impl/spark-3.2.2"))
-  .dependsOn(`spark-adapter-api`)
-  .settings(
-    libraryDependencies ++= spark("3.2.2")
-  )
-
-lazy val `spark-adapter-impl-330` = (project in file(s"spark-adapter/impl/spark-3.3.0"))
-  .dependsOn(`spark-adapter-api`)
-  .settings(
-    libraryDependencies ++= spark("3.3.0")
-  )
-
 lazy val `spark-patch` = (project in file("spark-patch"))
   .dependsOn(`spark-adapter-api` % "compile->compile;test->test;provided->provided")
   .settings(
@@ -38,11 +26,31 @@ lazy val javaAgents = Def.task {
   ))
 }
 
+
+lazy val `spark-adapter-impl-322` = (project in file(s"spark-adapter/impl/spark-3.2.2"))
+  .dependsOn(`spark-adapter-api`, `spark-patch` % Provided)
+  .settings(
+    libraryDependencies ++= spark("3.2.2")
+  )
+
+lazy val `spark-adapter-impl-330` = (project in file(s"spark-adapter/impl/spark-3.3.0"))
+  .dependsOn(`spark-adapter-api`, `spark-patch` % Provided)
+  .settings(
+    libraryDependencies ++= spark("3.3.0")
+  )
+
+lazy val `spark-adapter-impl-340` = (project in file(s"spark-adapter/impl/spark-3.4.0"))
+  .dependsOn(`spark-adapter-api`, `spark-patch` % Provided)
+  .settings(
+    libraryDependencies ++= spark("3.4.0")
+  )
+
 lazy val `yt-wrapper` = (project in file("yt-wrapper"))
   .enablePlugins(BuildInfoPlugin)
   .dependsOn(`spark-adapter-api` % "compile->compile;test->test;provided->provided")
   .settings(
     libraryDependencies ++= circe,
+    libraryDependencies ++= shapeless,
     libraryDependencies ++= sttp,
     libraryDependencies ++= ytsaurusClient,
     libraryDependencies ++= logging,
@@ -56,10 +64,15 @@ lazy val `file-system` = (project in file("file-system"))
   .dependsOn(`yt-wrapper` % "compile->compile;test->test;provided->provided")
 
 lazy val `data-source-base` = (project in file("data-source"))
-  .dependsOn(`file-system` % "compile->compile;test->test;provided->provided", `spark-adapter-impl-322` % Test, `spark-adapter-impl-330` % Test)
+  .dependsOn(
+    `file-system` % "compile->compile;test->test;provided->provided",
+    `spark-adapter-impl-322` % Test,
+    `spark-adapter-impl-330` % Test,
+    `spark-adapter-impl-340` % Test
+  )
 
 lazy val `data-source-extended` = (project in file("data-source-extended"))
-  .dependsOn(`data-source-base` % "compile->compile;test->test;provided->provided", `spark-patch` % Provided)
+  .dependsOn(`data-source-base` % "compile->compile;test->test;provided->provided")
   .enablePlugins(JavaAgent)
   .settings(
     resolvedJavaAgents := javaAgents.value
@@ -94,7 +107,8 @@ lazy val `spyt-package` = (project in file("spyt-package"))
     `spark-submit` % "compile->compile;test->test;provided->provided",
     `spark-patch`,
     `spark-adapter-impl-322`,
-    `spark-adapter-impl-330`
+    `spark-adapter-impl-330`,
+    `spark-adapter-impl-340`
   )
   .settings(
 
@@ -104,7 +118,6 @@ lazy val `spyt-package` = (project in file("spyt-package"))
       "org.apache.commons" % "commons-lang3",
       "org.typelevel" %% "cats-kernel",
       "org.lz4" % "lz4-java",
-      "com.chuusai" %% "shapeless",
       "io.dropwizard.metrics" % "metrics-core",
       "org.slf4j" % "slf4j-api",
       "org.scala-lang.modules" %% "scala-parser-combinators",
@@ -215,6 +228,7 @@ lazy val root = (project in file("."))
     `spark-adapter-api`,
     `spark-adapter-impl-322`,
     `spark-adapter-impl-330`,
+    `spark-adapter-impl-340`,
     `yt-wrapper`,
     `file-system`,
     `data-source-base`,
@@ -257,6 +271,7 @@ lazy val root = (project in file("."))
         `spark-adapter-api` / publishSigned,
         `spark-adapter-impl-322` / publishSigned,
         `spark-adapter-impl-330` / publishSigned,
+        `spark-adapter-impl-340` / publishSigned,
         `yt-wrapper` / publishSigned,
         `file-system` / publishSigned,
         `spark-patch` / publishSigned,
--- yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtFilePartition.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtFilePartition.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -122,7 +122,6 @@ object YtFilePartition {
   def splitFiles(sparkSession: SparkSession,
                  file: FileStatus,
                  filePath: Path,
-                 isSplitable: Boolean,
                  maxSplitBytes: Long,
                  partitionValues: InternalRow,
                  readDataSchema: Option[StructType] = None): Seq[PartitionedFile] = {
@@ -292,8 +291,8 @@ object YtFilePartition {
 
   private[v2] def getPivotKeys(schema: StructType, keys: Seq[String], files: Seq[YtPartitionedFile])
                           (implicit yt: CompoundClient): Seq[TuplePoint] = {
-    val filepath = files.head.filePath
-    val basePath = YPathEnriched.fromPath(new Path(filepath)).toYPath.withColumns(keys: _*)
+    val hadoopPath = SparkAdapter.instance.getHadoopFilePath(files.head)
+    val basePath = YPathEnriched.fromPath(hadoopPath).toYPath.withColumns(keys: _*)
     val pathWithRanges = files.tail.map(_.delegate.beginRow)
       .foldLeft(basePath) { case (path, br) => path.withRange(br, br + 1) }
     val tableIterator = YtWrapper.readTable(
--- yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,192 +0,0 @@
-package org.apache.spark.sql.v2
-
-import org.apache.hadoop.mapreduce.{InputSplit, RecordReader, TaskAttemptContext}
-import org.apache.spark.TaskContext
-import org.apache.spark.broadcast.Broadcast
-import org.apache.spark.internal.Logging
-import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.catalyst.expressions.UnsafeProjection
-import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader}
-import org.apache.spark.sql.execution.datasources.PartitionedFile
-import org.apache.spark.sql.execution.datasources.v2.{FilePartitionReaderFactory, PartitionReaderWithPartitionValues}
-import org.apache.spark.sql.internal.SQLConf
-import org.apache.spark.sql.types._
-import org.apache.spark.sql.v2.YtUtils.bytesReadReporter
-import org.apache.spark.sql.vectorized.{ColumnVector, ColumnarBatch, SingleValueColumnVector, YtVectorizedReader}
-import org.apache.spark.util.SerializableConfiguration
-import org.slf4j.LoggerFactory
-import tech.ytsaurus.client.CompoundClient
-import tech.ytsaurus.spyt.common.utils.SegmentSet
-import tech.ytsaurus.spyt.format.YtInputSplit
-import tech.ytsaurus.spyt.format.YtPartitionedFileDelegate.YtPartitionedFile
-import tech.ytsaurus.spyt.format.conf.FilterPushdownConfig
-import tech.ytsaurus.spyt.format.conf.SparkYtConfiguration.Read.{CountOptimizationEnabled, VectorizedCapacity}
-import tech.ytsaurus.spyt.fs.YtClientConfigurationConverter.ytClientConfiguration
-import tech.ytsaurus.spyt.fs.YtHadoopPath
-import tech.ytsaurus.spyt.fs.conf._
-import tech.ytsaurus.spyt.logger.{TaskInfo, YtDynTableLoggerConfig}
-import tech.ytsaurus.spyt.serializers.InternalRowDeserializer
-import tech.ytsaurus.spyt.wrapper.YtWrapper
-import tech.ytsaurus.spyt.wrapper.client.YtClientProvider
-
-import java.util.UUID
-
-case class YtPartitionReaderFactory(sqlConf: SQLConf,
-                                    broadcastedConf: Broadcast[SerializableConfiguration],
-                                    dataSchema: StructType,
-                                    readDataSchema: StructType,
-                                    partitionSchema: StructType,
-                                    options: Map[String, String],
-                                    pushedFilterSegments: SegmentSet,
-                                    filterPushdownConf: FilterPushdownConfig,
-                                    ytLoggerConfig: Option[YtDynTableLoggerConfig])
-  extends FilePartitionReaderFactory with Logging {
-
-  private val idPrefix: String = s"YtPartitionReaderFactory-${UUID.randomUUID()}"
-
-  private val resultSchema = StructType(readDataSchema.fields)
-  private val ytClientConf = ytClientConfiguration(sqlConf)
-  private val arrowEnabled: Boolean = YtReaderOptions.arrowEnabled(options, sqlConf)
-  private val optimizedForScan: Boolean = YtReaderOptions.optimizedForScan(options)
-  private val readBatch: Boolean = YtReaderOptions.canReadBatch(readDataSchema, optimizedForScan, arrowEnabled)
-  private val returnBatch: Boolean = readBatch && YtReaderOptions.supportBatch(resultSchema, sqlConf)
-  private val batchMaxSize = sqlConf.ytConf(VectorizedCapacity)
-  private val countOptimizationEnabled = sqlConf.ytConf(CountOptimizationEnabled)
-
-  override def supportColumnarReads(partition: InputPartition): Boolean = {
-    returnBatch
-  }
-
-  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {
-    buildLockedSplitReader(file) { case (split, path) =>
-      implicit val yt: CompoundClient = YtClientProvider.ytClientWithProxy(ytClientConf, path.ypath.cluster, idPrefix)
-      val reader = if (readBatch) {
-        createVectorizedReader(split, returnBatch = false, path)
-      } else {
-        createRowBaseReader(split, path)
-      }
-
-      val fileReader = new PartitionReader[InternalRow] {
-        override def next(): Boolean = reader.nextKeyValue()
-
-        override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]
-
-        override def close(): Unit = {
-          reader.close()
-        }
-      }
-
-      new PartitionReaderWithPartitionValues(fileReader, readDataSchema,
-        partitionSchema, file.partitionValues)
-    }
-  }
-
-  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {
-    buildLockedSplitReader(file) { case (split, path) =>
-      implicit val yt: CompoundClient = YtClientProvider.ytClientWithProxy(ytClientConf, path.ypath.cluster, idPrefix)
-      val vectorizedReader = createVectorizedReader(split, returnBatch = true, path)
-      new PartitionReader[ColumnarBatch] {
-        override def next(): Boolean = vectorizedReader.nextKeyValue()
-
-        override def get(): ColumnarBatch = {
-          val sourceBatch = vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]
-          val capacity = sourceBatch.numRows()
-          val schemaCols = sourceBatch.numCols()
-          if (partitionSchema != null && partitionSchema.nonEmpty) {
-            val columnVectors = new Array[ColumnVector](sourceBatch.numCols() + partitionSchema.fields.length)
-            for (i <- 0 until schemaCols) {
-              columnVectors(i) = sourceBatch.column(i)
-            }
-            partitionSchema.fields.zipWithIndex.foreach { case (field, index) =>
-              columnVectors(index + schemaCols) = new SingleValueColumnVector(capacity, field.dataType,
-                file.partitionValues, index)
-            }
-            new ColumnarBatch(columnVectors, capacity)
-          }
-          else
-            sourceBatch
-        }
-
-        override def close(): Unit = {
-          vectorizedReader.close()
-        }
-      }
-    }
-  }
-
-  private def buildLockedSplitReader[T](file: PartitionedFile)
-                                       (splitReader: (YtInputSplit, YtHadoopPath) => PartitionReader[T]): PartitionReader[T] = {
-    file match {
-      case ypf: YtPartitionedFile =>
-        val split = createSplit(ypf)
-        splitReader(split, ypf.delegate.hadoopPath)
-      case _ =>
-        throw new IllegalArgumentException(s"Partitions of type ${file.getClass.getSimpleName} are not supported")
-    }
-  }
-
-  private def createSplit(file: YtPartitionedFile): YtInputSplit = {
-    val log = LoggerFactory.getLogger(getClass)
-    val ytLoggerConfigWithTaskInfo = ytLoggerConfig.map(_.copy(taskContext = Some(TaskInfo(TaskContext.get()))))
-    val split = YtInputSplit(file, resultSchema, pushedFilterSegments, filterPushdownConf, ytLoggerConfigWithTaskInfo)
-
-    log.info(s"Reading ${split.ytPath}, " +
-      s"read batch: $readBatch, return batch: $returnBatch, arrowEnabled: $arrowEnabled, " +
-      s"pushdown config: $filterPushdownConf, detailed yPath: ${split.ytPathWithFiltersDetailed}")
-
-    split
-  }
-
-  private def createRowBaseReader(split: YtInputSplit, hadoopPath: YtHadoopPath)
-                                 (implicit yt: CompoundClient): RecordReader[Void, InternalRow] = {
-    val iter = YtWrapper.readTable(
-      split.ytPathWithFiltersDetailed,
-      InternalRowDeserializer.getOrCreate(resultSchema),
-      ytClientConf.timeout, hadoopPath.ypath.transaction,
-      bytesReadReporter(broadcastedConf)
-    )
-    val unsafeProjection = UnsafeProjection.create(resultSchema)
-
-    new RecordReader[Void, InternalRow] {
-      private var current: InternalRow = _
-
-      override def initialize(split: InputSplit, context: TaskAttemptContext): Unit = {}
-
-      override def nextKeyValue(): Boolean = {
-        if (iter.hasNext) {
-          current = unsafeProjection.apply(iter.next())
-          true
-        } else false
-      }
-
-      override def getCurrentKey: Void = {
-        null
-      }
-
-      override def getCurrentValue: InternalRow = {
-        current
-      }
-
-      override def getProgress: Float = 0.0f
-
-      override def close(): Unit = {
-        iter.close()
-      }
-    }
-  }
-
-  private def createVectorizedReader(split: YtInputSplit, returnBatch: Boolean, hadoopPath: YtHadoopPath)
-                                    (implicit yt: CompoundClient): YtVectorizedReader = {
-    new YtVectorizedReader(
-      split = split,
-      batchMaxSize = batchMaxSize,
-      returnBatch = returnBatch,
-      arrowEnabled = arrowEnabled,
-      optimizedForScan = optimizedForScan,
-      timeout = ytClientConf.timeout,
-      reportBytesRead = bytesReadReporter(broadcastedConf),
-      countOptimizationEnabled = countOptimizationEnabled,
-      hadoopPath = hadoopPath,
-    )
-  }
-}
--- yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactoryAdapter.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactoryAdapter.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,192 @@
+package org.apache.spark.sql.v2
+
+import org.apache.hadoop.mapreduce.{InputSplit, RecordReader, TaskAttemptContext}
+import org.apache.spark.TaskContext
+import org.apache.spark.broadcast.Broadcast
+import org.apache.spark.internal.Logging
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection
+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader}
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import org.apache.spark.sql.execution.datasources.v2.{FilePartitionReaderFactory, PartitionReaderWithPartitionValues}
+import org.apache.spark.sql.internal.SQLConf
+import org.apache.spark.sql.types._
+import org.apache.spark.sql.v2.YtUtils.bytesReadReporter
+import org.apache.spark.sql.vectorized.{ColumnVector, ColumnarBatch, SingleValueColumnVector, YtVectorizedReader}
+import org.apache.spark.util.SerializableConfiguration
+import org.slf4j.LoggerFactory
+import tech.ytsaurus.client.CompoundClient
+import tech.ytsaurus.spyt.common.utils.SegmentSet
+import tech.ytsaurus.spyt.format.YtInputSplit
+import tech.ytsaurus.spyt.format.YtPartitionedFileDelegate.YtPartitionedFile
+import tech.ytsaurus.spyt.format.conf.FilterPushdownConfig
+import tech.ytsaurus.spyt.format.conf.SparkYtConfiguration.Read.{CountOptimizationEnabled, VectorizedCapacity}
+import tech.ytsaurus.spyt.fs.YtClientConfigurationConverter.ytClientConfiguration
+import tech.ytsaurus.spyt.fs.YtHadoopPath
+import tech.ytsaurus.spyt.fs.conf._
+import tech.ytsaurus.spyt.logger.{TaskInfo, YtDynTableLoggerConfig}
+import tech.ytsaurus.spyt.serializers.InternalRowDeserializer
+import tech.ytsaurus.spyt.wrapper.YtWrapper
+import tech.ytsaurus.spyt.wrapper.client.YtClientProvider
+
+import java.util.UUID
+
+case class YtPartitionReaderFactoryAdapter(sqlConf: SQLConf,
+                                           broadcastedConf: Broadcast[SerializableConfiguration],
+                                           dataSchema: StructType,
+                                           readDataSchema: StructType,
+                                           partitionSchema: StructType,
+                                           options: Map[String, String],
+                                           pushedFilterSegments: SegmentSet,
+                                           filterPushdownConf: FilterPushdownConfig,
+                                           ytLoggerConfig: Option[YtDynTableLoggerConfig])
+  extends PartitionReaderFactoryAdapter with Logging {
+
+  private val idPrefix: String = s"YtPartitionReaderFactory-${UUID.randomUUID()}"
+
+  private val resultSchema = StructType(readDataSchema.fields)
+  private val ytClientConf = ytClientConfiguration(sqlConf)
+  private val arrowEnabled: Boolean = YtReaderOptions.arrowEnabled(options, sqlConf)
+  private val optimizedForScan: Boolean = YtReaderOptions.optimizedForScan(options)
+  private val readBatch: Boolean = YtReaderOptions.canReadBatch(readDataSchema, optimizedForScan, arrowEnabled)
+  private val returnBatch: Boolean = readBatch && YtReaderOptions.supportBatch(resultSchema, sqlConf)
+  private val batchMaxSize = sqlConf.ytConf(VectorizedCapacity)
+  private val countOptimizationEnabled = sqlConf.ytConf(CountOptimizationEnabled)
+
+  override def supportColumnarReads(partition: InputPartition): Boolean = {
+    returnBatch
+  }
+
+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {
+    buildLockedSplitReader(file) { case (split, path) =>
+      implicit val yt: CompoundClient = YtClientProvider.ytClientWithProxy(ytClientConf, path.ypath.cluster, idPrefix)
+      val reader = if (readBatch) {
+        createVectorizedReader(split, returnBatch = false, path)
+      } else {
+        createRowBaseReader(split, path)
+      }
+
+      val fileReader = new PartitionReader[InternalRow] {
+        override def next(): Boolean = reader.nextKeyValue()
+
+        override def get(): InternalRow = reader.getCurrentValue.asInstanceOf[InternalRow]
+
+        override def close(): Unit = {
+          reader.close()
+        }
+      }
+
+      new PartitionReaderWithPartitionValues(fileReader, readDataSchema,
+        partitionSchema, file.partitionValues)
+    }
+  }
+
+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] = {
+    buildLockedSplitReader(file) { case (split, path) =>
+      implicit val yt: CompoundClient = YtClientProvider.ytClientWithProxy(ytClientConf, path.ypath.cluster, idPrefix)
+      val vectorizedReader = createVectorizedReader(split, returnBatch = true, path)
+      new PartitionReader[ColumnarBatch] {
+        override def next(): Boolean = vectorizedReader.nextKeyValue()
+
+        override def get(): ColumnarBatch = {
+          val sourceBatch = vectorizedReader.getCurrentValue.asInstanceOf[ColumnarBatch]
+          val capacity = sourceBatch.numRows()
+          val schemaCols = sourceBatch.numCols()
+          if (partitionSchema != null && partitionSchema.nonEmpty) {
+            val columnVectors = new Array[ColumnVector](sourceBatch.numCols() + partitionSchema.fields.length)
+            for (i <- 0 until schemaCols) {
+              columnVectors(i) = sourceBatch.column(i)
+            }
+            partitionSchema.fields.zipWithIndex.foreach { case (field, index) =>
+              columnVectors(index + schemaCols) = new SingleValueColumnVector(capacity, field.dataType,
+                file.partitionValues, index)
+            }
+            new ColumnarBatch(columnVectors, capacity)
+          }
+          else
+            sourceBatch
+        }
+
+        override def close(): Unit = {
+          vectorizedReader.close()
+        }
+      }
+    }
+  }
+
+  private def buildLockedSplitReader[T](file: PartitionedFile)
+                                       (splitReader: (YtInputSplit, YtHadoopPath) => PartitionReader[T]): PartitionReader[T] = {
+    file match {
+      case ypf: YtPartitionedFile =>
+        val split = createSplit(ypf)
+        splitReader(split, ypf.delegate.hadoopPath)
+      case _ =>
+        throw new IllegalArgumentException(s"Partitions of type ${file.getClass.getSimpleName} are not supported")
+    }
+  }
+
+  private def createSplit(file: YtPartitionedFile): YtInputSplit = {
+    val log = LoggerFactory.getLogger(getClass)
+    val ytLoggerConfigWithTaskInfo = ytLoggerConfig.map(_.copy(taskContext = Some(TaskInfo(TaskContext.get()))))
+    val split = YtInputSplit(file, resultSchema, pushedFilterSegments, filterPushdownConf, ytLoggerConfigWithTaskInfo)
+
+    log.info(s"Reading ${split.ytPath}, " +
+      s"read batch: $readBatch, return batch: $returnBatch, arrowEnabled: $arrowEnabled, " +
+      s"pushdown config: $filterPushdownConf, detailed yPath: ${split.ytPathWithFiltersDetailed}")
+
+    split
+  }
+
+  private def createRowBaseReader(split: YtInputSplit, hadoopPath: YtHadoopPath)
+                                 (implicit yt: CompoundClient): RecordReader[Void, InternalRow] = {
+    val iter = YtWrapper.readTable(
+      split.ytPathWithFiltersDetailed,
+      InternalRowDeserializer.getOrCreate(resultSchema),
+      ytClientConf.timeout, hadoopPath.ypath.transaction,
+      bytesReadReporter(broadcastedConf)
+    )
+    val unsafeProjection = UnsafeProjection.create(resultSchema)
+
+    new RecordReader[Void, InternalRow] {
+      private var current: InternalRow = _
+
+      override def initialize(split: InputSplit, context: TaskAttemptContext): Unit = {}
+
+      override def nextKeyValue(): Boolean = {
+        if (iter.hasNext) {
+          current = unsafeProjection.apply(iter.next())
+          true
+        } else false
+      }
+
+      override def getCurrentKey: Void = {
+        null
+      }
+
+      override def getCurrentValue: InternalRow = {
+        current
+      }
+
+      override def getProgress: Float = 0.0f
+
+      override def close(): Unit = {
+        iter.close()
+      }
+    }
+  }
+
+  private def createVectorizedReader(split: YtInputSplit, returnBatch: Boolean, hadoopPath: YtHadoopPath)
+                                    (implicit yt: CompoundClient): YtVectorizedReader = {
+    new YtVectorizedReader(
+      split = split,
+      batchMaxSize = batchMaxSize,
+      returnBatch = returnBatch,
+      arrowEnabled = arrowEnabled,
+      optimizedForScan = optimizedForScan,
+      timeout = ytClientConf.timeout,
+      reportBytesRead = bytesReadReporter(broadcastedConf),
+      countOptimizationEnabled = countOptimizationEnabled,
+      hadoopPath = hadoopPath,
+    )
+  }
+}
--- yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtScan.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/main/scala/org/apache/spark/sql/v2/YtScan.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -49,11 +49,12 @@ case class YtScan(sparkSession: SparkSession,
     val broadcastedConf = sparkSession.sparkContext.broadcast(
       new SerializableConfiguration(hadoopConf))
     val keyPartitionedOptions = Map(YtTableSparkSettings.KeyPartitioned.name -> supportsKeyPartitioning.toString)
-    YtPartitionReaderFactory(sparkSession.sessionState.conf, broadcastedConf,
+    val adapter = YtPartitionReaderFactoryAdapter(sparkSession.sessionState.conf, broadcastedConf,
       dataSchema, readDataSchema, readPartitionSchema,
       options.asScala.toMap ++ keyPartitionedOptions,
       pushedFilterSegments, filterPushdownConf, YtDynTableLoggerConfig.fromSpark(sparkSession)
     )
+    SparkAdapter.instance.createYtPartitionReaderFactory(adapter)
   }
 
   override def equals(obj: Any): Boolean = obj match {
@@ -141,7 +142,6 @@ case class YtScan(sparkSession: SparkSession,
           sparkSession = sparkSession,
           file = file,
           filePath = filePath,
-          isSplitable = isSplitable(filePath),
           maxSplitBytes = maxSplitBytes,
           partitionValues = partitionValues,
           readDataSchema = Some(readDataSchema)
--- yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/TestPartitionedFile.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/TestPartitionedFile.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,6 +1,7 @@
 package org.apache.spark.sql.v2
 
 import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.SparkAdapter
 import tech.ytsaurus.spyt.format.YtPartitionedFileDelegate
 import tech.ytsaurus.spyt.format.YtPartitionedFileDelegate.YtPartitionedFileExt
 import tech.ytsaurus.spyt.fs.path.YPathEnriched
@@ -17,7 +18,7 @@ object TestPartitionedFile {
         Static(ytFile.path, ytFile.delegate.beginRow, ytFile.delegate.endRow)
       case ytFile: YtPartitionedFileExt =>
         Dynamic(ytFile.path, ytFile.length)
-      case _ => Csv(file.filePath, file.length)
+      case _ => Csv(SparkAdapter.instance.getStringFilePath(file), file.length)
     }
   }
 
@@ -35,7 +36,8 @@ object TestPartitionedFile {
   }
 
   case class Csv(path: String, length: Long) extends TestPartitionedFile {
-    override def toPartitionedFile: PartitionedFile =
-      PartitionedFile(YtPartitionedFileDelegate.emptyInternalRow, path, 0, length)
+    override def toPartitionedFile: PartitionedFile = {
+      SparkAdapter.instance.createPartitionedFile(YtPartitionedFileDelegate.emptyInternalRow, path, 0, length)
+    }
   }
 }
--- yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/YtFilePartitionTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/YtFilePartitionTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -7,11 +7,11 @@ import scala.util.Random
 
 class YtFilePartitionTest extends FlatSpec with Matchers with TableDrivenPropertyChecks {
 
-  behavior of "YtFilePartitionTest"
+  behavior of "YtFilePartition"
 
   import TestPartitionedFile._
 
-  it should "order partitionedFiles" in {
+  it should "order partitionedFiles using partitionedFilesOrdering" in {
     val expected = Seq(
       Dynamic("//path0", 1), // ordered by path
 
--- yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/YtTableStatisticsTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/test/scala/org/apache/spark/sql/v2/YtTableStatisticsTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -8,8 +8,7 @@ import org.apache.spark.sql.internal.SQLConf.{CODEGEN_FACTORY_MODE, WHOLESTAGE_C
 import org.apache.spark.sql.v2.Utils.getStatistics
 import org.mockito.scalatest.MockitoSugar
 import org.scalatest.{FlatSpec, Matchers}
-import tech.ytsaurus.spyt.YtReader
-import tech.ytsaurus.spyt.YtWriter
+import tech.ytsaurus.spyt.{SparkAdapter, YtReader, YtWriter}
 import tech.ytsaurus.spyt.test.{DynTableTestUtils, LocalSpark, TmpDir}
 import tech.ytsaurus.spyt.wrapper.YtWrapper
 
@@ -67,7 +66,9 @@ class YtTableStatisticsTest extends FlatSpec with Matchers with LocalSpark
       val expected = (0 until 100).map(x => Row(x / 20, -x / 2, 2 * (x / 20) + x % 2, "string"))
       res should contain theSameElementsAs expected
 
-      val files = findAllScans(query).map(_.getPartitions.head.files.head.filePath).map(new Path(_).getName)
+      val files = findAllScans(query).map { scan =>
+        SparkAdapter.instance.getHadoopFilePath(scan.getPartitions.head.files.head).getName
+      }
       files should contain theSameElementsAs Seq("t1", "t2", "t3")
       files shouldNot contain theSameElementsInOrderAs Seq("t1", "t2", "t3")
     }
--- yt/spark/spark-over-yt/data-source/src/test/scala/tech/ytsaurus/spyt/format/YtDynamicTableWriterTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source/src/test/scala/tech/ytsaurus/spyt/format/YtDynamicTableWriterTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -6,6 +6,7 @@ import org.scalatest.flatspec.AnyFlatSpec
 import org.scalatest.matchers.should.Matchers
 import tech.ytsaurus.core.cypress.YPath
 import tech.ytsaurus.core.tables.{ColumnValueType, TableSchema}
+import tech.ytsaurus.spyt.SparkVersionUtils
 import tech.ytsaurus.spyt.exceptions._
 import tech.ytsaurus.spyt.serializers.{InternalRowSerializer, SchemaConverter, WriteSchemaConverter}
 import tech.ytsaurus.spyt.serializers.SchemaConverter.{Sorted, Unordered}
@@ -51,7 +52,11 @@ class YtDynamicTableWriterTest extends AnyFlatSpec with TmpDir with LocalSpark w
       doTheTest(externalTableSchema = Some(tableSchema))
     }
 
-    sparkException.getMessage shouldEqual "Job aborted."
+    if (SparkVersionUtils.lessThan("3.4.0")) {
+      sparkException.getMessage shouldEqual "Job aborted."
+    } else {
+      sparkException.getCause.getMessage should startWith ("[TASK_WRITE_FAILED]")
+    }
   }
 
   it should "write the data to the table when dataframe contains some part of the table's columns" in {
@@ -76,7 +81,11 @@ class YtDynamicTableWriterTest extends AnyFlatSpec with TmpDir with LocalSpark w
         })
     }
 
-    sparkException.getMessage shouldEqual "Job aborted."
+    if (SparkVersionUtils.lessThan("3.4.0")) {
+      sparkException.getMessage shouldEqual "Job aborted."
+    } else {
+      sparkException.getCause.getMessage should startWith ("[TASK_WRITE_FAILED]")
+    }
   }
 
   it should "check that the saveMode is set to Append" in {
--- yt/spark/spark-over-yt/data-source-extended/src/main/java/org/apache/spark/sql/catalyst/expressions/SpecializedGettersReaderDecorator.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/java/org/apache/spark/sql/catalyst/expressions/SpecializedGettersReaderDecorator.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,38 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions;
-
-import org.apache.spark.sql.types.DataType;
-import org.apache.spark.sql.types.NullType;
-import org.apache.spark.sql.spyt.types.UInt64Type;
-import tech.ytsaurus.spyt.patch.annotations.Decorate;
-import tech.ytsaurus.spyt.patch.annotations.DecoratedMethod;
-import tech.ytsaurus.spyt.patch.annotations.OriginClass;
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.SpecializedGettersReader")
-public class SpecializedGettersReaderDecorator {
-
-    @DecoratedMethod
-    public static Object read(
-            SpecializedGetters obj,
-            int ordinal,
-            DataType dataType,
-            boolean handleNull,
-            boolean handleUserDefinedType) {
-        if (handleNull && (obj.isNullAt(ordinal) || dataType instanceof NullType)) {
-            return null;
-        }
-        if (dataType instanceof UInt64Type) {
-            return obj.getLong(ordinal);
-        }
-        return __read(obj, ordinal, dataType, handleNull, handleUserDefinedType);
-    }
-
-    public static Object __read(
-            SpecializedGetters obj,
-            int ordinal,
-            DataType dataType,
-            boolean handleNull,
-            boolean handleUserDefinedType) {
-        throw new RuntimeException("Must be replaced with original method");
-    }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.StorageSupport	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.StorageSupport	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.adapter.YTsaurusStorageSupport
--- yt/spark/spark-over-yt/data-source-extended/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.TypeSupport	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.TypeSupport	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.adapter.YTsaurusTypeSupport
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/SparkConfDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/SparkConfDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,34 +0,0 @@
-package org.apache.spark
-
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.SparkConf")
-class SparkConfDecorators {
-
-  @DecoratedMethod
-  private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = {
-    val self = __loadFromSystemProperties(silent)
-    SparkConfExtensions.loadFromEnvironment(self, silent)
-    self
-  }
-
-  private[spark] def __loadFromSystemProperties(silent: Boolean): SparkConf = ???
-}
-
-private[spark] object SparkConfExtensions {
-  private[spark] def loadFromEnvironment(conf: SparkConf, silent: Boolean): SparkConf = {
-    for ((key, value) <- sys.env if key.startsWith("SPARK_")) {
-      conf.set(SparkConfExtensions.envToConfName(key), value, silent)
-    }
-    conf
-  }
-
-  private[spark] def envToConfName(envName: String): String = {
-    envName.toLowerCase().replace("_", ".")
-  }
-
-  private[spark] def confToEnvName(confName: String): String = {
-    confName.replace(".", "_").toUpperCase()
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/SparkSessionBuilderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/SparkSessionBuilderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,18 +0,0 @@
-package org.apache.spark.sql
-
-import tech.ytsaurus.spyt.format.optimizer.YtSortedTableMarkerRule
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.SparkSession$Builder")
-class SparkSessionBuilderDecorators {
-
-  @DecoratedMethod
-  def getOrCreate(): SparkSession = {
-    val spark = __getOrCreate()
-    spark.experimental.extraOptimizations ++= Seq(new YtSortedTableMarkerRule(spark))
-    spark
-  }
-
-  def __getOrCreate(): SparkSession = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,43 +0,0 @@
-package org.apache.spark.sql.catalyst
-
-import org.apache.spark.sql.catalyst.ScalaReflection.universe.Type
-import org.apache.spark.sql.catalyst.expressions.Expression
-import org.apache.spark.sql.types.{DataType, ObjectType}
-import org.apache.spark.sql.spyt.types.UInt64Long
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.ScalaReflection")
-object ScalaReflectionDecorators {
-
-  @DecoratedMethod
-  private def serializerFor(inputObject: Expression,
-                            tpe: `Type`,
-                            walkedTypePath: WalkedTypePath,
-                            seenTypeSet: Set[`Type`]): Expression = baseType(tpe) match {
-    case _ if !inputObject.dataType.isInstanceOf[ObjectType] => inputObject
-    case t if isSubtype(t, ScalaReflection.localTypeOf[UInt64Long]) => UInt64Long.createSerializer(inputObject)
-    case _ => __serializerFor(inputObject, tpe, walkedTypePath, seenTypeSet)
-  }
-
-
-  private def __serializerFor(inputObject: Expression,
-                              tpe: `Type`,
-                              walkedTypePath: WalkedTypePath,
-                              seenTypeSet: Set[`Type`]): Expression = ???
-
-  @DecoratedMethod
-  private def deserializerFor(tpe: `Type`,
-                              path: Expression,
-                              walkedTypePath: WalkedTypePath): Expression = baseType(tpe) match {
-    case t if !dataTypeFor(t).isInstanceOf[ObjectType] => path
-    case t if isSubtype (t, ScalaReflection.localTypeOf[UInt64Long]) => UInt64Long.createDeserializer (path)
-    case _ => __deserializerFor (tpe, path, walkedTypePath)
-  }
-
-  private def __deserializerFor(tpe: `Type`, path: Expression, walkedTypePath: WalkedTypePath): Expression = ???
-
-  private def baseType(tpe: `Type`): `Type` = ???
-  private[catalyst] def isSubtype(tpe1: `Type`, tpe2: `Type`): Boolean = ???
-  private def dataTypeFor(tpe: `Type`): DataType = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,72 +0,0 @@
-package org.apache.spark.sql.catalyst.catalog
-
-import org.apache.spark.internal.Logging
-import org.apache.spark.sql.catalyst.TableIdentifier
-import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException
-import org.apache.spark.sql.errors.QueryCompilationErrors
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import java.net.URI
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.catalog.SessionCatalog")
-class SessionCatalogDecorators {
-
-  @DecoratedMethod
-  def createTable(
-      tableDefinition: CatalogTable,
-      ignoreIfExists: Boolean,
-      validateLocation: Boolean = true): Unit = {
-    val isExternal = tableDefinition.tableType == CatalogTableType.EXTERNAL
-    if (isExternal && tableDefinition.storage.locationUri.isEmpty) {
-      throw QueryCompilationErrors.createExternalTableWithoutLocationError
-    }
-
-    val db = formatDatabaseName(tableDefinition.identifier.database.getOrElse(getCurrentDatabase))
-    val table = formatTableName(tableDefinition.identifier.table)
-    val tableIdentifier = TableIdentifier(table, Some(db))
-
-    val newTableDefinition = if (tableDefinition.storage.locationUri.isDefined
-      && !tableDefinition.storage.locationUri.get.isAbsolute) {
-      // make the location of the table qualified.
-      val qualifiedTableLocation = makeQualifiedTablePath(tableDefinition.storage.locationUri.get, db)
-      tableDefinition.copy(
-        storage = tableDefinition.storage.copy(locationUri = Some(qualifiedTableLocation)),
-        identifier = tableIdentifier)
-    } else {
-      tableDefinition.copy(identifier = tableIdentifier)
-    }
-
-    requireDbExists(db)
-    if (tableExists(newTableDefinition.identifier)) {
-      if (!ignoreIfExists) {
-        if (validateLocation) {
-          throw new TableAlreadyExistsException(db = db, table = table)
-        } else {
-          // Table could be already created by insert operation
-          SessionCatalogDecorators.logInsertWarning()
-        }
-      }
-    } else if (validateLocation) {
-      validateTableLocation(newTableDefinition)
-    }
-    externalCatalog.createTable(newTableDefinition, ignoreIfExists)
-  }
-
-  // Stubs for methods from base class
-  protected[this] def formatDatabaseName(name: String): String = ???
-  def getCurrentDatabase: String = ???
-  protected[this] def formatTableName(name: String): String = ???
-  private def makeQualifiedTablePath(locationUri: URI, database: String): URI = ???
-  private def requireDbExists(db: String): Unit = ???
-  def tableExists(name: TableIdentifier): Boolean = ???
-  def validateTableLocation(table: CatalogTable): Unit = ???
-  lazy val externalCatalog: ExternalCatalog  = ???
-}
-
-object SessionCatalogDecorators extends Logging {
-  def logInsertWarning(): Unit = {
-    logWarning("Table existence should not be ignored, but location is already validated. " +
-      "So modifiable operation has inserted data already")
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,46 +0,0 @@
-package org.apache.spark.sql.catalyst.encoders
-
-import org.apache.spark.sql.catalyst.expressions.Expression
-import org.apache.spark.sql.types.{DataType, ObjectType}
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
-import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.encoders.RowEncoder")
-object RowEncoderDecorators {
-
-  @DecoratedMethod
-  def externalDataTypeFor(dt: DataType): DataType = dt match {
-    case UInt64Type => ObjectType(classOf[UInt64Long])
-    case _ => __externalDataTypeFor(dt)
-  }
-
-  def __externalDataTypeFor(dt: DataType): DataType = ???
-
-  @DecoratedMethod
-  @Applicability(to = "3.2.4")
-  private def serializerFor(inputObject: Expression, inputType: DataType): Expression = inputType match {
-    case UInt64Type => UInt64Long.createSerializer(inputObject)
-    case _ => __serializerFor(inputObject, inputType)
-  }
-  private def __serializerFor(inputObject: Expression, inputType: DataType): Expression = ???
-
-  @DecoratedMethod
-  @Applicability(from = "3.3.0")
-  private def serializerFor(inputObject: Expression, inputType: DataType, lenient: Boolean): Expression =
-    inputType match {
-      case UInt64Type => UInt64Long.createSerializer(inputObject)
-      case _ => __serializerFor(inputObject, inputType, lenient)
-    }
-
-  private def __serializerFor(inputObject: Expression, inputType: DataType, lenient: Boolean): Expression = ???
-
-  @DecoratedMethod(
-    signature = "(Lorg/apache/spark/sql/catalyst/expressions/Expression;Lorg/apache/spark/sql/types/DataType;)Lorg/apache/spark/sql/catalyst/expressions/Expression;"
-  )
-  private def deserializerFor(input: Expression, dataType: DataType): Expression = dataType match {
-    case UInt64Type => UInt64Long.createDeserializer(input)
-    case _ => __deserializerFor(input, dataType)
-  }
-  private def __deserializerFor(input: Expression, dataType: DataType): Expression = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastBaseDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastBaseDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,74 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions
-
-import org.apache.spark.sql.catalyst.expressions.codegen.{Block, CodegenContext, ExprValue}
-import org.apache.spark.sql.errors.QueryExecutionErrors
-import org.apache.spark.sql.types.{DataType, NullType}
-import org.apache.spark.sql.spyt.types._
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.CastBase")
-class CastBaseDecorators {
-
-  @DecoratedMethod
-  private[this] def castToString(from: DataType): Any => Any = from match {
-    case UInt64Type => UInt64CastToString
-    case _ => __castToString(from)
-  }
-
-  private[this] def __castToString(from: DataType): Any => Any = ???
-
-  @DecoratedMethod
-  private[this] def castToBinary(from: DataType): Any => Any = from match {
-    case YsonType => YsonCastToBinary
-    case _ => __castToBinary(from)
-  }
-  private[this] def __castToBinary(from: DataType): Any => Any = ???
-
-  @DecoratedMethod
-  protected[this] def cast(from: DataType, to: DataType): Any => Any = {
-    if (DataType.equalsStructurally(from, to)) {
-      __cast(from, to)
-    } else if (from == NullType) {
-      YTsaurusCastUtils.cannotCastFromNullTypeError(to)
-    } else {
-      to match {
-        case UInt64Type => UInt64Long.cast(from)
-        case YsonType => YsonBinary.cast(from)
-        case _ => __cast(from, to)
-      }
-    }
-  }
-  protected[this] def __cast(from: DataType, to: DataType): Any => Any = ???
-
-  protected[this] type CastFunction = (ExprValue, ExprValue, ExprValue) => Block
-
-  @DecoratedMethod
-  private[this] def nullSafeCastFunction(from: DataType, to: DataType, ctx: CodegenContext): CastFunction = to match {
-    case YsonType if !(from == NullType || to == from) => BinaryCastToYsonCode
-    case _ => __nullSafeCastFunction(from, to, ctx)
-  }
-
-  private[this] def __nullSafeCastFunction(from: DataType, to: DataType, ctx: CodegenContext): CastFunction = ???
-
-  @DecoratedMethod
-  private[this] def castToStringCode(from: DataType, ctx: CodegenContext): CastFunction = from match {
-    case UInt64Type => UInt64CastToStringCode
-    case _ => __castToStringCode(from, ctx)
-  }
-
-  private[this] def __castToStringCode(from: DataType, ctx: CodegenContext): CastFunction = ???
-
-  @DecoratedMethod
-  private[this] def castToBinaryCode(from: DataType): CastFunction = from match {
-    case YsonType => YsonCastToBinaryCode
-    case _ => __castToBinaryCode(from)
-  }
-  private[this] def __castToBinaryCode(from: DataType): CastFunction = ???
-}
-
-object YTsaurusCastUtils {
-  def cannotCastFromNullTypeError(to: DataType): Any => Any = {
-    _ => throw QueryExecutionErrors.cannotCastFromNullTypeError(to)
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,19 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions
-
-import org.apache.spark.sql.types.{BinaryType, DataType}
-import org.apache.spark.sql.spyt.types.YsonType
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.Cast")
-object CastDecorators {
-
-  @DecoratedMethod
-  def canCast(from: DataType, to: DataType): Boolean = (from, to) match {
-    case (YsonType, BinaryType) => true
-    case (BinaryType, YsonType) => true
-    case _ => __canCast(from, to)
-  }
-
-  def __canCast(from: DataType, to: DataType): Boolean = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/HashExpressionSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/HashExpressionSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,19 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions
-
-import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-@Subclass
-@OriginClass("org.apache.spark.sql.catalyst.expressions.HashExpression")
-abstract class HashExpressionSpyt[E] extends HashExpression[E] {
-
-  override protected def computeHash(input: String,
-                                     dataType: DataType,
-                                     result: String,
-                                     ctx: CodegenContext): String = dataType match {
-    case UInt64Type => genHashLong(input, result)
-    case _ => super.computeHash(input, dataType, result, ctx)
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRowDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRowDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,18 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions
-
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.SpecificInternalRow")
-class SpecificInternalRowDecorators {
-
-  @DecoratedMethod
-  private[this] def dataTypeToMutableValue(dataType: DataType): MutableValue = dataType match {
-    case UInt64Type => new MutableLong
-    case _ => __dataTypeToMutableValue(dataType)
-  }
-
-  private[this] def __dataTypeToMutableValue(dataType: DataType): MutableValue = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGeneratorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGeneratorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,27 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions.codegen
-
-import org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.JAVA_LONG
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator")
-object CodeGeneratorDecorators {
-
-  @DecoratedMethod
-  def javaType(dt: DataType): String = dt match {
-    case UInt64Type => JAVA_LONG
-    case _ => __javaType(dt)
-  }
-
-  def __javaType(dt: DataType): String = ???
-
-  @DecoratedMethod
-  def javaClass(dt: DataType): Class[_] = dt match {
-    case UInt64Type => java.lang.Long.TYPE
-    case _ => __javaClass(dt)
-  }
-
-  def __javaClass(dt: DataType): Class[_] = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenContextDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenContextDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,19 +0,0 @@
-package org.apache.spark.sql.catalyst.expressions.codegen
-
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext")
-class CodegenContextDecorators {
-
-  @DecoratedMethod
-  def genComp(dataType: DataType, c1: String, c2: String): String = dataType match {
-    case UInt64Type => s"java.lang.Long.compareUnsigned($c1, $c2)"
-    case _ => __genComp(dataType, c1, c2)
-  }
-
-  def __genComp(dataType: DataType, c1: String, c2: String): String = ???
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilderSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilderSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,38 +0,0 @@
-package org.apache.spark.sql.catalyst.parser
-
-import org.apache.spark.sql.catalyst.parser.AstBuilderSpyt.extractUint64Opt
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.SparkAdapter
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import java.util.Locale
-
-@Decorate
-@OriginClass("org.apache.spark.sql.catalyst.parser.AstBuilder")
-class AstBuilderSpyt {
-
-  @DecoratedMethod
-  def visitPrimitiveDataType(ctx: SqlBaseParser.PrimitiveDataTypeContext): DataType = {
-    val uint64Opt = extractUint64Opt(ctx)
-
-    if (uint64Opt.isDefined) {
-      uint64Opt.get
-    } else {
-      __visitPrimitiveDataType(ctx)
-    }
-  }
-
-  def __visitPrimitiveDataType(ctx: SqlBaseParser.PrimitiveDataTypeContext): DataType = ???
-}
-
-object AstBuilderSpyt {
-  def extractUint64Opt(ctx: SqlBaseParser.PrimitiveDataTypeContext): Option[UInt64Type] =
-    SparkAdapter.instance.parserUtilsWithOrigin(ctx) {
-      val dataType = ctx.identifier.getText.toLowerCase(Locale.ROOT)
-      dataType match {
-        case "uint64" => Some(UInt64Type)
-        case _ => None
-      }
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/DependentHashShuffleExchangeExec.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/DependentHashShuffleExchangeExec.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -10,6 +10,7 @@ import org.apache.spark.sql.execution.exchange.{REPARTITION_BY_NUM, ShuffleExcha
 import org.apache.spark.sql.execution.metric.{SQLMetric, SQLMetrics, SQLShuffleWriteMetricsReporter}
 import org.apache.spark.util.MutablePair
 import org.apache.spark.{Partitioner, ShuffleDependency}
+import tech.ytsaurus.spyt.SparkAdapter
 import tech.ytsaurus.spyt.common.utils.TuplePoint
 import tech.ytsaurus.spyt.common.utils.ExpressionTransformer
 
@@ -98,7 +99,7 @@ class DependentHashShuffleExchangeExec(dependentPartitioning: Partitioning,
     val dependency =
       new ShuffleDependency[Int, InternalRow, InternalRow](
         rddWithPartitionIds,
-        new PartitionIdPassthrough(part.numPartitions),
+        SparkAdapter.instance.createShufflePartitioner(part.numPartitions),
         serializer,
         shuffleWriterProcessor = createShuffleWriteProcessor(writeMetrics))
 
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtilDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtilDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,36 +0,0 @@
-package org.apache.spark.sql.execution
-
-import org.apache.hadoop.fs.{FileStatus, Path}
-import org.apache.spark.sql.SparkSession
-import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.execution.datasources.PartitionedFile
-import org.apache.spark.sql.v2.YtFilePartition
-import org.apache.spark.sql.yt.YtSourceScanExec
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.PartitionedFileUtil")
-object PartitionedFileUtilDecorators {
-
-  @DecoratedMethod
-  def splitFiles(sparkSession: SparkSession,
-                 file: FileStatus,
-                 filePath: Path,
-                 isSplitable: Boolean,
-                 maxSplitBytes: Long,
-                 partitionValues: InternalRow): Seq[PartitionedFile] = {
-    if (YtSourceScanExec.currentThreadInstance.get() != null) {
-      YtFilePartition.splitFiles(sparkSession, file, filePath, isSplitable, maxSplitBytes, partitionValues)
-    } else {
-      __splitFiles(sparkSession, file, filePath, isSplitable, maxSplitBytes, partitionValues)
-    }
-  }
-
-  def __splitFiles(sparkSession: SparkSession,
-                 file: FileStatus,
-                 filePath: Path,
-                 isSplitable: Boolean,
-                 maxSplitBytes: Long,
-                 partitionValues: InternalRow): Seq[PartitionedFile] = ???
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGeneratorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGeneratorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,27 +0,0 @@
-package org.apache.spark.sql.execution.aggregate
-
-import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.aggregate.HashMapGenerator")
-class HashMapGeneratorDecorators {
-
-  @DecoratedMethod
-  protected final def genComputeHash(ctx: CodegenContext,
-                                     input: String,
-                                     dataType: DataType,
-                                     result: String): String = dataType match {
-    case UInt64Type => s"long $result = $input;"
-    case _ => __genComputeHash(ctx, input, dataType, result)
-  }
-
-
-  protected final def __genComputeHash(ctx: CodegenContext,
-                                       input: String,
-                                       dataType: DataType,
-                                       result: String): String = ???
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,30 +0,0 @@
-package org.apache.spark.sql.execution.columnar
-
-import org.apache.spark.sql.spyt.types.UInt64Type
-import org.apache.spark.sql.types.DataType
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.columnar.ColumnBuilder")
-object ColumnBuilderDecorators {
-
-  @DecoratedMethod
-  def apply(dataType: DataType,
-            initialSize: Int,
-            columnName: String,
-            useCompression: Boolean): ColumnBuilder = {
-    dataType match {
-      case UInt64Type =>
-        val builder = new LongColumnBuilder
-        builder.initialize(initialSize, columnName, useCompression)
-        builder
-      case _ => __apply(dataType, initialSize, columnName, useCompression)
-    }
-  }
-
-  def __apply(dataType: DataType,
-            initialSize: Int,
-            columnName: String,
-            useCompression: Boolean): ColumnBuilder = ???
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,25 +0,0 @@
-package org.apache.spark.sql.execution.columnar
-
-import org.apache.spark.sql.execution.columnar.GenerateColumnAccessorDecoratorsUtils.patchColumnTypes
-import org.apache.spark.sql.spyt.types.UInt64Type
-import org.apache.spark.sql.types.{DataType, LongType}
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.columnar.GenerateColumnAccessor")
-object GenerateColumnAccessorDecorators {
-
-  @DecoratedMethod
-  protected def create(columnTypes: Seq[DataType]): ColumnarIterator = {
-    __create(patchColumnTypes(columnTypes))
-  }
-
-  protected def __create(columnTypes: Seq[DataType]): ColumnarIterator = ???
-}
-
-object GenerateColumnAccessorDecoratorsUtils {
-  def patchColumnTypes(columnTypes: Seq[DataType]): Seq[DataType] = columnTypes.map {
-    case UInt64Type => LongType
-    case other => other
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteTaskStatsTrackerSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteTaskStatsTrackerSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,59 +0,0 @@
-package org.apache.spark.sql.execution.datasources
-
-import org.apache.hadoop.conf.Configuration
-import org.apache.spark.TaskContext
-import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.execution.metric.SQLMetric
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-import scala.collection.mutable
-
-
-@Subclass
-@OriginClass("org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker")
-class BasicWriteTaskStatsTrackerSpyt(hadoopConf: Configuration, taskCommitTimeMetric: Option[SQLMetric] = None)
-  extends BasicWriteTaskStatsTracker(hadoopConf, taskCommitTimeMetric) {
-
-  private val submittedFiles = getSuperField("submittedFiles").asInstanceOf[mutable.HashSet[String]]
-  private val updateFileStatsS = this.getClass.getSuperclass.getDeclaredMethod("updateFileStats", classOf[String])
-
-  private def getSuperField(name: String): AnyRef = {
-    val field = this.getClass.getSuperclass.getDeclaredField(name)
-    field.setAccessible(true)
-    val result = field.get(this)
-    field.setAccessible(false)
-    result
-  }
-
-  override def closeFile(filePath: String): Unit = {
-    updateFileStats(filePath)
-    submittedFiles.remove(filePath)
-  }
-
-  private def updateFileStats(filePath: String): Unit = {
-    if (filePath.startsWith("ytTable:/")) return
-
-    updateFileStatsS.setAccessible(true)
-    updateFileStatsS.invoke(this, filePath).asInstanceOf[Option[Long]]
-    updateFileStatsS.setAccessible(false)
-  }
-
-  override def getFinalStats(taskCommitTime: Long): WriteTaskStats = {
-    submittedFiles.foreach(updateFileStats)
-    submittedFiles.clear()
-
-    val partitions = getSuperField("partitions").asInstanceOf[mutable.ArrayBuffer[InternalRow]]
-    val numFiles = getSuperField("numFiles").asInstanceOf[Int]
-    var numBytes = getSuperField("numBytes").asInstanceOf[Long]
-    val numRows = getSuperField("numRows").asInstanceOf[Long]
-
-    Option(TaskContext.get()).map(_.taskMetrics().outputMetrics).foreach { metrics =>
-      numBytes += metrics.bytesWritten
-      metrics.setBytesWritten(numBytes)
-      metrics.setRecordsWritten(numRows)
-    }
-
-    taskCommitTimeMetric.foreach(_ += taskCommitTime)
-    BasicWriteTaskStats(partitions, numFiles, numBytes, numRows)
-  }
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartitionDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartitionDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,24 +0,0 @@
-package org.apache.spark.sql.execution.datasources
-
-import org.apache.spark.sql.SparkSession
-import org.apache.spark.sql.v2.YtFilePartition
-import org.apache.spark.sql.yt.YtSourceScanExec
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.datasources.FilePartition")
-object FilePartitionDecorators {
-
-  @DecoratedMethod
-  def maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = {
-    val ytSourceScanExec = YtSourceScanExec.currentThreadInstance.get()
-    if (ytSourceScanExec != null) {
-      YtFilePartition.maxSplitBytes(sparkSession, selectedPartitions, ytSourceScanExec.maybeReadParallelism)
-    } else {
-      __maxSplitBytes(sparkSession, selectedPartitions)
-    }
-  }
-
-  def __maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = ???
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioningDecorator.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioningDecorator.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,12 +0,0 @@
-package org.apache.spark.sql.execution.datasources.v2
-
-import tech.ytsaurus.spyt.patch.annotations.{AddInterfaces, OriginClass}
-
-import java.io.Serializable
-
-
-@AddInterfaces(Array(classOf[Serializable]))
-@OriginClass("org.apache.spark.sql.execution.datasources.v2.DataSourcePartitioning")
-class DataSourcePartitioningDecorator {
-
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePythonDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePythonDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,31 +0,0 @@
-package org.apache.spark.sql.execution.python
-
-import org.apache.spark.sql.types.DataType
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.execution.python.EvaluatePython")
-object EvaluatePythonDecorators {
-
-  @DecoratedMethod
-  def makeFromJava(dataType: DataType): Any => Any = dataType match {
-    case UInt64Type => EvaluatePythonUint64MakeFromJava
-    case other => __makeFromJava(other)
-  }
-
-  def __makeFromJava(dataType: DataType): Any => Any = ???
-}
-
-
-object EvaluatePythonUint64MakeFromJava extends (Any => Any) {
-
-  override def apply(input: Any): Any = input match {
-    case null => null
-    case c: Byte => c.toLong
-    case c: Short => c.toLong
-    case c: Int => c.toLong
-    case c: Long => c
-    case _ => null
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/internal/SharedStateDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/internal/SharedStateDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,20 +0,0 @@
-package org.apache.spark.sql.internal
-
-import org.apache.spark.SparkConf
-import org.apache.spark.sql.catalyst.catalog.YTsaurusExternalCatalog
-import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.internal.SharedState$")
-object SharedStateDecorators {
-  @DecoratedMethod
-  private[sql] def org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf: SparkConf): String = {
-    conf.get(CATALOG_IMPLEMENTATION) match {
-      case "in-memory" => classOf[YTsaurusExternalCatalog].getCanonicalName
-      case _ => __org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf)
-    }
-  }
-
-  private[sql] def __org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf: SparkConf): String = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/spyt/types/UInt64Type.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/spyt/types/UInt64Type.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -8,6 +8,7 @@ import org.apache.spark.sql.expressions.UserDefinedFunction
 import org.apache.spark.sql.functions.udf
 import org.apache.spark.sql.types._
 import org.apache.spark.unsafe.types.UTF8String
+import tech.ytsaurus.spyt.types.UInt64Long
 
 import scala.math.Numeric.LongIsIntegral
 import scala.reflect.runtime.universe.typeTag
@@ -31,6 +32,7 @@ class UInt64Type private() extends IntegralType {
 
   private[sql] val integral = UInt64IsIntegral
 
+  // exactNumeric is not yet implemented for UInt64
 }
 
 object UInt64IsIntegral extends LongIsIntegral with Ordering[Long] {
@@ -39,18 +41,7 @@ object UInt64IsIntegral extends LongIsIntegral with Ordering[Long] {
 
 case object UInt64Type extends UInt64Type
 
-case class UInt64Long(value: Long) {
-  def toLong: Long = value
-
-  override def toString: String = UInt64Long.toString(value)
-
-  override def hashCode(): Int = value.toInt
-}
-
-object UInt64Long {
-  def apply(number: String): UInt64Long = {
-    UInt64Long(fromString(number))
-  }
+object UInt64Support {
 
   val toStringUdf: UserDefinedFunction = udf((number: UInt64Long) => number match {
     case null => null
@@ -62,10 +53,6 @@ object UInt64Long {
     case _ => UInt64Long(number)
   })
 
-  def fromString(number: String): Long = java.lang.Long.parseUnsignedLong(number)
-
-  def toString(value: Long): String = java.lang.Long.toUnsignedString(value)
-
   def createSerializer(inputObject: Expression): Expression = {
     Invoke(inputObject, "toLong", UInt64Type)
   }
@@ -79,9 +66,8 @@ object UInt64Long {
       returnNullable = false)
   }
 
-  // Actually - from a certain type to Long
   def cast(from: DataType): Any => Any = from match {
-    case StringType => s => fromString(s.asInstanceOf[UTF8String].toString)
+    case StringType => s => UInt64Long.fromString(s.asInstanceOf[UTF8String].toString)
     case BooleanType => b => if (b.asInstanceOf[Boolean]) 1L else 0L
     case x: NumericType => b => x.numeric.asInstanceOf[Numeric[Any]].toLong(b)
   }
@@ -93,5 +79,5 @@ object UInt64CastToString extends (Any => Any) {
 
 object UInt64CastToStringCode extends ((ExprValue, ExprValue, ExprValue) => Block) {
   def apply(c: ExprValue, evPrim: ExprValue, evNull: ExprValue): Block =
-    code"$evPrim = UTF8String.fromString(org.apache.spark.sql.spyt.types.UInt64Long$$.MODULE$$.toString($c));"
+    code"$evPrim = UTF8String.fromString(tech.ytsaurus.spyt.types.UInt64Long$$.MODULE$$.toString($c));"
 }
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/types/DataTypeDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/org/apache/spark/sql/types/DataTypeDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,17 +0,0 @@
-package org.apache.spark.sql.types
-
-import org.apache.spark.sql.spyt.types.UInt64Type
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.sql.types.DataType")
-object DataTypeDecorators {
-
-  @DecoratedMethod
-  private def nameToType(name: String): DataType = name match {
-    case "uint64" => UInt64Type
-    case _ => __nameToType(name)
-  }
-
-  private def __nameToType(name: String): DataType = ???
-}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusStorageSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusStorageSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,34 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.hadoop.fs.{FileStatus, Path}
+import org.apache.spark.sql.SparkSession
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.catalog.YTsaurusExternalCatalog
+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
+import org.apache.spark.sql.catalyst.rules.Rule
+import org.apache.spark.sql.execution.datasources.{PartitionDirectory, PartitionedFile}
+import org.apache.spark.sql.v2.YtFilePartition
+import org.apache.spark.sql.yt.YtSourceScanExec
+import tech.ytsaurus.spyt.format.optimizer.YtSortedTableMarkerRule
+
+class YTsaurusStorageSupport extends StorageSupport {
+  override def shouldUseYtSplitFiles(): Boolean = {
+    YtSourceScanExec.currentThreadInstance.get() != null
+  }
+
+  override def splitFiles(sparkSession: SparkSession, file: FileStatus, filePath: Path,
+                          maxSplitBytes: Long, partitionValues: InternalRow): Seq[PartitionedFile] = {
+    YtFilePartition.splitFiles(sparkSession, file, filePath, maxSplitBytes, partitionValues)
+  }
+
+  override def maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = {
+    val ytSourceScanExec = YtSourceScanExec.currentThreadInstance.get()
+    YtFilePartition.maxSplitBytes(sparkSession, selectedPartitions, ytSourceScanExec.maybeReadParallelism)
+  }
+
+  override def createExtraOptimizations(spark: SparkSession): Seq[Rule[LogicalPlan]] = {
+    Seq(new YtSortedTableMarkerRule(spark))
+  }
+
+  override val ytsaurusExternalCatalogName: String = classOf[YTsaurusExternalCatalog].getCanonicalName
+}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusTypeSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusTypeSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,37 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.catalyst.parser.SqlBaseParser
+import org.apache.spark.sql.spyt.types._
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.SparkAdapter
+import tech.ytsaurus.spyt.adapter.TypeSupport.CastFunction
+
+import java.util.Locale
+
+class YTsaurusTypeSupport extends TypeSupport {
+
+  override val uInt64DataType: DataType = UInt64Type
+  override def uInt64Serializer(inputObject: Expression): Expression = UInt64Support.createSerializer(inputObject)
+  override def uInt64Deserializer(path: Expression): Expression = UInt64Support.createDeserializer(path)
+
+  override def uInt64Cast(from: DataType): Any => Any = UInt64Support.cast(from)
+  override val uInt64CastToString: Any => Any = UInt64CastToString
+  override val uInt64CastToStringCode: CastFunction = UInt64CastToStringCode
+
+  override def extractUint64Opt(ctx: SqlBaseParser.PrimitiveDataTypeContext): Option[DataType] = {
+    SparkAdapter.instance.parserUtilsWithOrigin(ctx) {
+      val dataType = ctx.identifier.getText.toLowerCase(Locale.ROOT)
+      dataType match {
+        case "uint64" => Some(UInt64Type)
+        case _ => None
+      }
+    }
+  }
+
+  override val ysonDataType: DataType = YsonType
+  override def ysonCast(from: DataType): Any => Any = YsonBinary.cast(from)
+  override val ysonCastToBinary: Any => Any = YsonCastToBinary
+  override val ysonCastToBinaryCode: CastFunction = YsonCastToBinaryCode
+  override val binaryCastToYsonCode: CastFunction = BinaryCastToYsonCode
+}
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/format/optimizer/YtSourceStrategy.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/format/optimizer/YtSourceStrategy.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -11,6 +11,7 @@ import org.apache.spark.sql.execution.datasources.{BucketingUtils, HadoopFsRelat
 import org.apache.spark.sql.yt.YtSourceScanExec
 import org.apache.spark.sql.{Strategy, execution}
 import org.apache.spark.util.collection.BitSet
+import tech.ytsaurus.spyt.SparkVersionUtils
 
 class YtSourceStrategy extends Strategy with Logging {
 
@@ -144,7 +145,16 @@ class YtSourceStrategy extends Strategy with Logging {
       val dataFilters = normalizedFilters.filter(_.references.intersect(partitionSet).isEmpty)
 
       // Predicates with both partition keys and attributes need to be evaluated after the scan.
-      val afterScanFilters = filterSet -- partitionKeyFilters.filter(_.references.nonEmpty)
+      val filtersToExclude = partitionKeyFilters.filter(_.references.nonEmpty)
+      val afterScanFilters = if (SparkVersionUtils.lessThan("3.4.0") ||
+        SparkVersionUtils.greaterThanOrEqual("3.5.2") ||
+        SparkVersionUtils.is("3.4.4")) {
+        // According to https://issues.apache.org/jira/browse/SPARK-47897 method -- in ExpressionSet was removed in
+        // Spark 3.4.0 and reverted back in 3.4.4 and 3.5.2 versions because of performance degradation.
+        filterSet -- filtersToExclude
+      } else {
+        filtersToExclude.foldLeft(filterSet)((fs, filter) => fs - filter)
+      }
       logInfo(s"Post-Scan Filters: ${afterScanFilters.mkString(",")}")
 
       val filterAttributes = AttributeSet(afterScanFilters)
--- yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/types/ExtendedYtsaurusTypes.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/main/scala/tech/ytsaurus/spyt/types/ExtendedYtsaurusTypes.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -2,7 +2,7 @@ package tech.ytsaurus.spyt.types
 
 import org.apache.spark.sql.Row
 import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type, YsonBinary, YsonType}
+import org.apache.spark.sql.spyt.types.{UInt64Type, YsonBinary, YsonType}
 import org.apache.spark.sql.types.DataType
 import tech.ytsaurus.client.rows.{UnversionedValue, WireProtocolWriteable}
 import tech.ytsaurus.core.tables.ColumnValueType
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/org/apache/spark/sql/PythonSpytExtensionsTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/org/apache/spark/sql/PythonSpytExtensionsTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -4,11 +4,12 @@ import org.apache.spark.api.java.JavaRDD
 import org.apache.spark.api.python.SerDeUtil
 import org.apache.spark.sql.api.python.PythonSQLUtils
 import org.apache.spark.sql.types.{DataType, IntegerType, Metadata, StructField, StructType}
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
+import org.apache.spark.sql.spyt.types.UInt64Type
 import org.json4s.JsonAST.JString
 import org.scalatest.flatspec.AnyFlatSpec
 import org.scalatest.matchers.should.Matchers
 import tech.ytsaurus.spyt.test.LocalSpark
+import tech.ytsaurus.spyt.types.UInt64Long
 
 
 class PythonSpytExtensionsTest extends AnyFlatSpec with Matchers with LocalSpark {
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/common/utils/CityHashTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/common/utils/CityHashTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -2,10 +2,10 @@ package tech.ytsaurus.spyt.common.utils
 
 import org.apache.spark.sql.Row
 import org.apache.spark.sql.functions.col
-import org.apache.spark.sql.spyt.types.UInt64Long
 import org.scalatest.{FlatSpec, Matchers}
-import tech.ytsaurus.spyt.common.utils.CityHash.{registerFunction, cityHashUdf}
+import tech.ytsaurus.spyt.common.utils.CityHash.{cityHashUdf, registerFunction}
 import tech.ytsaurus.spyt.test.{LocalSpark, TmpDir}
+import tech.ytsaurus.spyt.types.UInt64Long
 
 class CityHashTest extends FlatSpec with Matchers with LocalSpark with TmpDir {
   behavior of "CityHashTest"
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/common/utils/XxHash64Test.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/common/utils/XxHash64Test.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -2,10 +2,10 @@ package tech.ytsaurus.spyt.common.utils
 
 import org.apache.spark.sql.Row
 import org.apache.spark.sql.functions.col
-import org.apache.spark.sql.spyt.types.UInt64Long
 import org.scalatest.{FlatSpec, Matchers}
 import tech.ytsaurus.spyt.common.utils.XxHash64ZeroSeed.{registerFunction, xxHash64ZeroSeedUdf}
 import tech.ytsaurus.spyt.test.{LocalSpark, TmpDir}
+import tech.ytsaurus.spyt.types.UInt64Long
 
 class XxHash64Test extends FlatSpec with Matchers with LocalSpark with TmpDir {
   behavior of "XxHash64Test"
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/format/ExtendedYtFileFormatTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/format/ExtendedYtFileFormatTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,7 +1,7 @@
 package tech.ytsaurus.spyt.format
 
 import org.apache.spark.sql.Row
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
+import org.apache.spark.sql.spyt.types.UInt64Type
 import org.apache.spark.sql.types.{BooleanType, ByteType, DoubleType, FloatType, LongType, StructField, StructType}
 import org.scalatest.flatspec.AnyFlatSpec
 import org.scalatest.matchers.should.Matchers
@@ -9,6 +9,7 @@ import tech.ytsaurus.client.rows.{UnversionedRow, UnversionedValue}
 import tech.ytsaurus.core.tables.{ColumnValueType, TableSchema}
 import tech.ytsaurus.spyt._
 import tech.ytsaurus.spyt.test.{LocalSpark, TestUtils, TmpDir}
+import tech.ytsaurus.spyt.types.UInt64Long
 
 class ExtendedYtFileFormatTest extends AnyFlatSpec with Matchers with LocalSpark with TmpDir with TestUtils {
 
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/format/types/UInt64Test.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/format/types/UInt64Test.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -2,15 +2,15 @@ package tech.ytsaurus.spyt.format.types
 
 import org.apache.spark.sql.{DataFrame, Row}
 import org.apache.spark.sql.functions._
-import org.apache.spark.sql.types.{LongType, Metadata, StringType, StructField, StructType}
-import org.apache.spark.sql.spyt.types.UInt64Long.{fromStringUdf, toStringUdf}
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
+import org.apache.spark.sql.types.{LongType, Metadata, StringType, StructField}
+import org.apache.spark.sql.spyt.types.UInt64Support.{fromStringUdf, toStringUdf}
+import org.apache.spark.sql.spyt.types.UInt64Type
 import org.apache.spark.storage.StorageLevel
-import org.apache.spark.storage.StorageLevel._
 import org.scalatest.{FlatSpec, Matchers}
 import tech.ytsaurus.spyt._
 import tech.ytsaurus.spyt.test.{LocalSpark, TestUtils, TmpDir}
 import tech.ytsaurus.core.tables.{ColumnValueType, TableSchema}
+import tech.ytsaurus.spyt.types.UInt64Long
 import tech.ytsaurus.spyt.wrapper.YtWrapper
 
 class UInt64Test extends FlatSpec with Matchers with LocalSpark with TmpDir with TestUtils {
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/serialization/ExtendedYsonDecoderTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/serialization/ExtendedYsonDecoderTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,8 +1,9 @@
 package tech.ytsaurus.spyt.serialization
 
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
+import org.apache.spark.sql.spyt.types.UInt64Type
 import org.scalatest.flatspec.AnyFlatSpec
 import org.scalatest.matchers.should.Matchers
+import tech.ytsaurus.spyt.types.UInt64Long
 
 class ExtendedYsonDecoderTest extends AnyFlatSpec with Matchers {
 
--- yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/serializers/ExtendedYsonRowConverterTest.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/data-source-extended/src/test/scala/tech/ytsaurus/spyt/serializers/ExtendedYsonRowConverterTest.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,10 +1,11 @@
 package tech.ytsaurus.spyt.serializers
 
 import org.apache.spark.sql.Row
-import org.apache.spark.sql.spyt.types.{UInt64Long, UInt64Type}
+import org.apache.spark.sql.spyt.types.UInt64Type
 import org.apache.spark.sql.types.{StructField, StructType}
 import org.scalatest.flatspec.AnyFlatSpec
 import org.scalatest.matchers.should.Matchers
+import tech.ytsaurus.spyt.types.UInt64Long
 
 class ExtendedYsonRowConverterTest extends AnyFlatSpec with Matchers {
 
--- yt/spark/spark-over-yt/e2e-test/run-tests.sh	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/e2e-test/run-tests.sh	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -86,7 +86,7 @@ if [ "$deploy" = "true" ]; then
     # Deploy with SPYT image
     docker run --network=host \
                -e YT_PROXY="localhost:8000" -e YT_USER="root" -e YT_TOKEN="token" \
-               -e EXTRA_SPARK_VERSIONS="--use-cache 3.2.2 3.2.4 3.3.0 3.3.4" \
+               -e EXTRA_SPARK_VERSIONS="--use-cache 3.2.2 3.2.4 3.3.0 3.3.4 3.4.0 3.4.4" \
                -v /tmp:/tmp \
                --rm \
                ghcr.io/ytsaurus/spyt:$spyt_version
--- yt/spark/spark-over-yt/e2e-test/tox.ini	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/e2e-test/tox.ini	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -2,7 +2,7 @@
 min_version=4
 envlist =
     py{39, 311, 312}-spark322
-    py311-spark{324, 330, 334}
+    py311-spark{324, 330, 334, 340, 344}
 skipsdist = True
 
 [testenv]
@@ -17,6 +17,8 @@ deps =
     spark324: pyspark==3.2.4
     spark330: pyspark==3.3.0
     spark334: pyspark==3.3.4
+    spark340: pyspark==3.4.0
+    spark344: pyspark==3.4.4
 commands =
     bash -c 'set -e; \
              pip uninstall -y ytsaurus-spyt; \
--- yt/spark/spark-over-yt/project/Dependencies.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/project/Dependencies.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -3,6 +3,7 @@ import sbt._
 object Dependencies {
   lazy val circeVersion = "0.12.3"
   lazy val circeYamlVersion = "0.12.0"
+  lazy val shapelessVersion = "2.3.7"
   lazy val scalatestVersion = "3.1.0"
   lazy val livyVersion = "0.8.0-incubating"
   lazy val ytsaurusClientVersion = "1.2.6"
@@ -18,6 +19,10 @@ object Dependencies {
     "io.circe" %% "circe-parser"
   ).map(_ % circeVersion)
 
+  lazy val shapeless = Seq(
+    "com.chuusai" %% "shapeless" % shapelessVersion
+  )
+
   lazy val mockito = Seq(
     "org.mockito" %% "mockito-scala-scalatest" % mockitoVersion % Test,
     "org.mockito" %% "mockito-scala" % mockitoVersion % Test
--- yt/spark/spark-over-yt/resource-manager/src/main/scala/org/apache/spark/scheduler/cluster/ytsaurus/YTsaurusOperationManager.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/resource-manager/src/main/scala/org/apache/spark/scheduler/cluster/ytsaurus/YTsaurusOperationManager.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -17,6 +17,7 @@ import tech.ytsaurus.client.operations.{Spec, VanillaSpec}
 import tech.ytsaurus.client.request.{CompleteOperation, GetOperation, UpdateOperationParameters, VanillaOperation}
 import tech.ytsaurus.client.rpc.YTsaurusClientAuth
 import tech.ytsaurus.core.GUID
+import tech.ytsaurus.spyt.{SparkAdapter, SparkVersionUtils}
 import tech.ytsaurus.spyt.wrapper.YtWrapper
 import tech.ytsaurus.ysontree._
 
@@ -237,6 +238,8 @@ private[spark] class YTsaurusOperationManager(val ytClient: YTsaurusClient,
       environment.put("Y_BINARY_EXECUTABLE", YTree.stringNode(binaryExecutable))
     }
 
+    val execCores = SparkAdapter.instance.getExecutorCores(execResources)
+
     var executorCommand = (Seq(
       prepareEnvCommand,
       "&&",
@@ -250,7 +253,7 @@ private[spark] class YTsaurusOperationManager(val ytClient: YTsaurusClient,
       "org.apache.spark.executor.YTsaurusCoarseGrainedExecutorBackend",
       "--driver-url", driverUrl,
       "--executor-id", "$YT_TASK_JOB_INDEX",
-      "--cores", execResources.cores.toString,
+      "--cores", execCores.toString,
       "--app-id", appId,
       "--hostname", "$HOSTNAME"
     )).mkString(" ")
@@ -264,7 +267,7 @@ private[spark] class YTsaurusOperationManager(val ytClient: YTsaurusClient,
       specBuilder.beginMap()
         .key("command").value(executorCommand)
         .key("job_count").value(numExecutors)
-        .key("cpu_limit").value(execResources.cores)
+        .key("cpu_limit").value(execCores)
         .key("memory_limit").value(memoryLimit)
         .key("layer_paths").value(portoLayers)
         .key("file_paths").value(filePaths)
@@ -361,10 +364,17 @@ private[spark] object YTsaurusOperationManager extends Logging {
       conf.set("spark.executor.resource.gpu.discoveryScript", s"$spytHome/bin/getGpusResources.sh")
 
       val ytsaurusJavaOptions = ArrayBuffer[String]()
-      if (conf.getBoolean("spark.hadoop.yt.preferenceIpv6.enabled", defaultValue = false)) {
-        ytsaurusJavaOptions += "-Djava.net.preferIPv6Addresses=true"
-      }
       ytsaurusJavaOptions += s"$$(cat $spytHome/conf/java-opts)"
+      if (conf.getBoolean("spark.hadoop.yt.preferenceIpv6.enabled", defaultValue = false)) {
+        if (SparkVersionUtils.lessThan("3.4.0")) {
+          ytsaurusJavaOptions += "-Djava.net.preferIPv6Addresses=true"
+        } else {
+          val driverJavaOptions = conf.get(DRIVER_JAVA_OPTIONS).getOrElse("")
+          if (!driverJavaOptions.contains("java.net.preferIPv6Addresses")) {
+            conf.set(DRIVER_JAVA_OPTIONS, s"-Djava.net.preferIPv6Addresses=true $driverJavaOptions")
+          }
+        }
+      }
 
       if (!conf.contains(YTSAURUS_CUDA_VERSION)) {
         val cudaVersion = globalConfig.getStringO("cuda_toolkit_version").orElse("11.0")
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/deploy/SubmitSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/deploy/SubmitSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,18 @@
+package org.apache.spark.deploy
+
+import org.apache.spark.internal.config.{ConfigEntry, OptionalConfigEntry}
+
+import java.util.ServiceLoader
+
+trait SubmitSupport {
+  val YTSAURUS_IS_PYTHON: ConfigEntry[Boolean]
+  val YTSAURUS_IS_PYTHON_BINARY: ConfigEntry[Boolean]
+  val YTSAURUS_POOL: OptionalConfigEntry[String]
+  val YTSAURUS_PYTHON_BINARY_ENTRY_POINT: OptionalConfigEntry[String]
+  val YTSAURUS_PYTHON_EXECUTABLE: OptionalConfigEntry[String]
+  def pythonBinaryWrapperPath(spytHome: String): String
+}
+
+object SubmitSupport {
+  lazy val instance: SubmitSupport = ServiceLoader.load(classOf[SubmitSupport]).findFirst().get()
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/deploy/rest/RestSubmitSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/deploy/rest/RestSubmitSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package org.apache.spark.deploy.rest
+
+import org.apache.spark.SparkConf
+import org.apache.spark.rpc.RpcEndpointRef
+
+import java.util.ServiceLoader
+
+trait RestSubmitSupport {
+  def createSubmission(appResource: String,
+                       mainClass: String,
+                       appArgs: Array[String],
+                       conf: SparkConf,
+                       env: Map[String, String],
+                       master: String): SubmitRestProtocolResponse
+  def statusRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): StatusRequestServlet
+  def masterStateRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet
+  def appIdRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet
+  def appStatusRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet
+}
+
+object RestSubmitSupport {
+  lazy val instance: RestSubmitSupport = ServiceLoader.load(classOf[RestSubmitSupport]).findFirst().get()
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/sql/v2/PartitionReaderFactoryAdapter.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/sql/v2/PartitionReaderFactoryAdapter.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,13 @@
+package org.apache.spark.sql.v2
+
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader}
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import org.apache.spark.sql.vectorized.ColumnarBatch
+
+trait PartitionReaderFactoryAdapter {
+  def supportColumnarReads(partition: InputPartition): Boolean
+  def buildReader(file: PartitionedFile): PartitionReader[InternalRow]
+  def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch]
+  def options: Map[String, String]
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactoryBase.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactoryBase.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,21 @@
+package org.apache.spark.sql.v2
+
+import org.apache.spark.internal.Logging
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader}
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import org.apache.spark.sql.execution.datasources.v2.FilePartitionReaderFactory
+import org.apache.spark.sql.vectorized.ColumnarBatch
+
+abstract class YtPartitionReaderFactoryBase(adapter: PartitionReaderFactoryAdapter)
+  extends FilePartitionReaderFactory with Logging {
+
+  override def supportColumnarReads(partition: InputPartition): Boolean =
+    adapter.supportColumnarReads(partition)
+
+  override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] =
+    adapter.buildReader(file)
+
+  override def buildColumnarReader(file: PartitionedFile): PartitionReader[ColumnarBatch] =
+    adapter.buildColumnarReader(file)
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/MinSparkVersion.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/MinSparkVersion.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,12 @@
+package tech.ytsaurus.spyt;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Retention(RetentionPolicy.RUNTIME)
+@Target({ElementType.TYPE})
+public @interface MinSparkVersion {
+    String value();
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/SparkAdapter.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/SparkAdapter.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,17 +1,20 @@
 package tech.ytsaurus.spyt
 
 import org.antlr.v4.runtime.ParserRuleContext
+import org.apache.hadoop.fs.Path
 import org.apache.log4j.{Category, Level}
 import org.apache.log4j.spi.LoggingEvent
+import org.apache.spark.Partitioner
 import org.apache.spark.executor.ExecutorBackendFactory
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions.Expression
-import org.apache.spark.sql.connector.read.{InputPartition, ScanBuilder}
+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, ScanBuilder}
 import org.apache.spark.sql.connector.read.partitioning.Partitioning
 import org.apache.spark.sql.execution.datasources.PartitionedFile
 import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, FileScanBuilder}
 import org.apache.spark.sql.sources
-import org.apache.spark.sql.v2.ScanBuilderAdapter
+import org.apache.spark.sql.sources.Filter
+import org.apache.spark.sql.v2.{PartitionReaderFactoryAdapter, ScanBuilderAdapter}
 import org.slf4j.LoggerFactory
 import tech.ytsaurus.spyt.format.YtPartitioningDelegate
 import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
@@ -19,13 +22,18 @@ import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
 import java.util.ServiceLoader
 import scala.collection.JavaConverters._
 
-trait SparkAdapter {
-  def minSparkVersion: String
+trait SparkAdapter
+  extends SparkAdapterBase
+  with PartitionedFileAdapter
+  with PartitionedFilePathAdapter
+  with YtPartitionReaderFactoryCreator
+  with ShuffleAdapter
+  with ResourcesAdapter
+
+trait SparkAdapterBase {
   def createYtScanOutputPartitioning(numPartitions: Int): Partitioning
   def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder
   def executorBackendFactory: ExecutorBackendFactory
-  def createPartitionedFile(partitionValues: InternalRow, filePath: String, start: Long, length: Long): PartitionedFile
-  def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T]
   def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T
 
   //These methods are used for backward compatibility with Spark 3.2.2
@@ -38,20 +46,111 @@ trait SparkAdapter {
                          throwable: Throwable): LoggingEvent
 }
 
+trait PartitionedFileAdapter {
+  def createPartitionedFile(partitionValues: InternalRow, filePath: String, start: Long, length: Long): PartitionedFile
+  def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T]
+}
+
+trait PartitionedFilePathAdapter {
+  def getStringFilePath(pf: PartitionedFile): String
+  def getHadoopFilePath(pf: PartitionedFile): Path
+}
+
+trait YtPartitionReaderFactoryCreator {
+  def createYtPartitionReaderFactory(adapter: PartitionReaderFactoryAdapter): PartitionReaderFactory
+}
+
+trait ShuffleAdapter {
+  def createShufflePartitioner(numPartitions: Int): Partitioner
+}
+
+trait ResourcesAdapter {
+  // Here we use Product instead of ResourceProfile.ExecutorResourcesOrDefaults class because this class
+  // has private visibility. We must explicitly cast the argument to this class in implementations that should
+  // reside in org.apache.spark.resource package
+  def getExecutorCores(execResources: Product): Int
+}
+
+private class SparkAdapterImpl(base: SparkAdapterBase,
+                               pfAdapter: PartitionedFileAdapter,
+                               pfPathAdapter: PartitionedFilePathAdapter,
+                               ytprfc: YtPartitionReaderFactoryCreator,
+                               shuffleAdapter: ShuffleAdapter,
+                               resourcesAdapter: ResourcesAdapter
+                              ) extends SparkAdapter {
+
+  override def createYtScanOutputPartitioning(numPartitions: Int): Partitioning =
+    base.createYtScanOutputPartitioning(numPartitions)
+
+  override def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder =
+    base.createYtScanBuilder(scanBuilderAdapter)
+
+  override def executorBackendFactory: ExecutorBackendFactory =
+    base.executorBackendFactory
+
+  override def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T =
+    base.parserUtilsWithOrigin(ctx)(f)
+
+  override def checkPushedFilters(scanBuilder: ScanBuilder, filters: Seq[Expression],
+                                  expected: Seq[Filter]): (Seq[Any], Seq[Any]) =
+    base.checkPushedFilters(scanBuilder, filters, expected)
+
+  override def getInputPartition(dsrddPartition: DataSourceRDDPartition): InputPartition =
+    base.getInputPartition(dsrddPartition)
+
+  override def createLoggingEvent(fqnOfCategoryClass: String, logger: Category, timeStamp: Long, level: Level,
+                                  message: String, throwable: Throwable): LoggingEvent =
+    base.createLoggingEvent(fqnOfCategoryClass, logger, timeStamp, level, message, throwable)
+
+  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
+                                     start: Long, length: Long): PartitionedFile =
+    pfAdapter.createPartitionedFile(partitionValues, filePath, start, length)
+
+  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] =
+    pfAdapter.createYtPartitionedFile(delegate)
+
+  override def getStringFilePath(pf: PartitionedFile): String =
+    pfPathAdapter.getStringFilePath(pf)
+
+  override def getHadoopFilePath(pf: PartitionedFile): Path =
+    pfPathAdapter.getHadoopFilePath(pf)
+
+  override def createYtPartitionReaderFactory(adapter: PartitionReaderFactoryAdapter): PartitionReaderFactory =
+    ytprfc.createYtPartitionReaderFactory(adapter)
+
+  override def createShufflePartitioner(numPartitions: Int): Partitioner =
+    shuffleAdapter.createShufflePartitioner(numPartitions)
+
+  override def getExecutorCores(execResources: Product): Int =
+    resourcesAdapter.getExecutorCores(execResources)
+}
+
 object SparkAdapter {
 
   private val log = LoggerFactory.getLogger(getClass)
 
-  lazy val instance: SparkAdapter = loadInstance()
-
-  private def loadInstance(): SparkAdapter = {
-    log.debug(s"Runtime Spark version: ${SparkVersionUtils.currentVersion}")
-    val instances = ServiceLoader.load(classOf[SparkAdapter]).asScala
-    log.debug(s"Num of available SparkAdapter instances: ${instances.size}")
-    val instance = instances.filter { i =>
-      SparkVersionUtils.ordering.lteq(i.minSparkVersion, SparkVersionUtils.currentVersion)
-    }.toList.sortBy(_.minSparkVersion)(SparkVersionUtils.ordering.reverse).head
-    log.debug(s"Runtime Spark version: ${SparkVersionUtils.currentVersion}, using SparkAdapter for ${instance.minSparkVersion}")
-    instance
+  lazy val instance: SparkAdapter = createInstance()
+
+  private def createInstance(): SparkAdapter = {
+    val componentInterfaces = classOf[SparkAdapter].getInterfaces
+    val components = componentInterfaces.map(getAdapterComponent)
+    val implConstructor = classOf[SparkAdapterImpl].getConstructor(componentInterfaces: _*)
+    implConstructor.newInstance(components: _*)
+  }
+
+  private def getAdapterComponent(componentClass: Class[_]): AnyRef = {
+    val instances = ServiceLoader.load(componentClass).asScala
+    log.debug(s"Num of available ${componentClass.getName} instances: ${instances.size}")
+    val (minSparkVersion, instance) = instances.map { instance =>
+      val minSparkVersion = instance.getClass.getAnnotation(classOf[MinSparkVersion]).value()
+      (minSparkVersion, instance)
+    }.filter { case (minSparkVersion, _) =>
+      SparkVersionUtils.ordering.lteq(minSparkVersion, SparkVersionUtils.currentVersion)
+    }.toList.sortBy(_._1)(SparkVersionUtils.ordering.reverse).head
+
+    log.debug(s"Runtime Spark version: ${SparkVersionUtils.currentVersion}," +
+      s" using ${componentClass.getName} for $minSparkVersion")
+
+    instance.asInstanceOf[AnyRef]
   }
 }
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/SparkVersionUtils.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/SparkVersionUtils.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -17,4 +17,8 @@ object SparkVersionUtils {
   }
 
   def lessThan(sparkVersion: String): Boolean = ordering.lt(currentVersion, sparkVersion)
+
+  def greaterThanOrEqual(sparkVersion: String): Boolean = ordering.gteq(currentVersion, sparkVersion)
+
+  def is(sparkVersion: String): Boolean = currentVersion == sparkVersion
 }
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/Utils.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/Utils.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package tech.ytsaurus.spyt;
+
+public class Utils {
+    private static boolean isIpV6Host(String host) {
+        return host != null && host.contains(":");
+    }
+
+    public static String removeBracketsIfIpV6Host(String host) {
+        if (isIpV6Host(host) && host.startsWith("[")) {
+            return host.substring(1, host.length() - 1);
+        } else {
+            return host;
+        }
+    }
+
+    public static String addBracketsIfIpV6Host(String host) {
+        if (isIpV6Host(host) && !host.startsWith("[")) {
+            return "[" + host + "]";
+        } else {
+            return host;
+        }
+    }
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/ClusterSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/ClusterSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,16 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.spark.SparkConf
+
+import java.util.ServiceLoader
+
+trait ClusterSupport {
+  val masterEndpointName: String
+
+  def msgRegisterDriverToAppId(driverId: String, appId: String): Any
+  def msgUnregisterDriverToAppId(driverId: String): Any
+}
+
+object ClusterSupport {
+  lazy val instance: ClusterSupport = ServiceLoader.load(classOf[ClusterSupport]).findFirst().get()
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/StorageSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/StorageSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.hadoop.fs.{FileStatus, Path}
+import org.apache.spark.sql.SparkSession
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
+import org.apache.spark.sql.catalyst.rules.Rule
+import org.apache.spark.sql.execution.datasources.{PartitionDirectory, PartitionedFile}
+
+import java.util.ServiceLoader
+
+trait StorageSupport {
+  def shouldUseYtSplitFiles(): Boolean
+  def splitFiles(sparkSession: SparkSession, file: FileStatus, filePath: Path,
+                 maxSplitBytes: Long, partitionValues: InternalRow): Seq[PartitionedFile]
+  def maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long
+  def createExtraOptimizations(spark: SparkSession): Seq[Rule[LogicalPlan]]
+  val ytsaurusExternalCatalogName: String
+}
+
+object StorageSupport {
+  lazy val instance: StorageSupport = ServiceLoader.load(classOf[StorageSupport]).findFirst().get()
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/TypeSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/adapter/TypeSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,33 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.catalyst.expressions.codegen.{Block, ExprValue}
+import org.apache.spark.sql.catalyst.parser.SqlBaseParser
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.CastFunction
+
+import java.util.ServiceLoader
+
+trait TypeSupport {
+  val uInt64DataType: DataType
+  def uInt64Serializer(inputObject: Expression): Expression
+  def uInt64Deserializer(path: Expression): Expression
+  def uInt64Cast(from: DataType): Any => Any
+  val uInt64CastToString: Any => Any
+  val uInt64CastToStringCode: CastFunction
+  def extractUint64Opt(ctx: SqlBaseParser.PrimitiveDataTypeContext): Option[DataType]
+
+  val ysonDataType: DataType
+  def ysonCast(from: DataType): Any => Any
+  val ysonCastToBinary: Any => Any
+  val ysonCastToBinaryCode: CastFunction
+  val binaryCastToYsonCode: CastFunction
+}
+
+object TypeSupport {
+  lazy val instance: TypeSupport = ServiceLoader.load(classOf[TypeSupport]).findFirst().get()
+
+  // A copy of org.apache.spark.sql.catalyst.expressions.CastBase#CastFunction which was moved
+  // to org.apache.spark.sql.catalyst.expressions.Cast#CastFunction in Spark 3.4.0, but wasn't changed
+  type CastFunction = (ExprValue, ExprValue, ExprValue) => Block
+}
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/format/YtPartitioningSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/format/YtPartitioningSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -3,9 +3,9 @@ package tech.ytsaurus.spyt.format
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.execution.datasources.PartitionedFile
 
-trait YtPartitioningSupport[T <: YtPartitioningDelegate] { this: PartitionedFile =>
+trait YtPartitioningSupport[T <: YtPartitioningDelegate] {
   val delegate: T
-  def path: String = filePath
+  def path: String = delegate.filePath
 }
 
 object YtPartitioningSupport {
--- yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/types/UInt64Long.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/api/src/main/scala/tech/ytsaurus/spyt/types/UInt64Long.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,20 @@
+package tech.ytsaurus.spyt.types
+
+case class UInt64Long(value: Long) {
+  def toLong: Long = value
+
+  override def toString: String = UInt64Long.toString(value)
+
+  override def hashCode(): Int = value.toInt
+}
+
+object UInt64Long {
+
+  def apply(number: String): UInt64Long = {
+    UInt64Long(fromString(number))
+  }
+
+  def fromString(number: String): Long = java.lang.Long.parseUnsignedLong(number)
+
+  def toString(value: Long): String = java.lang.Long.toUnsignedString(value)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/java/org/apache/spark/sql/catalyst/expressions/SpecializedGettersReaderDecorator.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/java/org/apache/spark/sql/catalyst/expressions/SpecializedGettersReaderDecorator.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,40 @@
+package org.apache.spark.sql.catalyst.expressions;
+
+import org.apache.spark.sql.types.DataType;
+import org.apache.spark.sql.types.NullType;
+import tech.ytsaurus.spyt.adapter.TypeSupport;
+import tech.ytsaurus.spyt.patch.annotations.Applicability;
+import tech.ytsaurus.spyt.patch.annotations.Decorate;
+import tech.ytsaurus.spyt.patch.annotations.DecoratedMethod;
+import tech.ytsaurus.spyt.patch.annotations.OriginClass;
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.SpecializedGettersReader")
+@Applicability(to = "3.3.4")
+public class SpecializedGettersReaderDecorator {
+
+    @DecoratedMethod
+    public static Object read(
+            SpecializedGetters obj,
+            int ordinal,
+            DataType dataType,
+            boolean handleNull,
+            boolean handleUserDefinedType) {
+        if (handleNull && (obj.isNullAt(ordinal) || dataType instanceof NullType)) {
+            return null;
+        }
+        if (TypeSupport.instance().uInt64DataType().getClass().isInstance(dataType)) {
+            return obj.getLong(ordinal);
+        }
+        return __read(obj, ordinal, dataType, handleNull, handleUserDefinedType);
+    }
+
+    public static Object __read(
+            SpecializedGetters obj,
+            int ordinal,
+            DataType dataType,
+            boolean handleNull,
+            boolean handleUserDefinedType) {
+        throw new RuntimeException("Must be replaced with original method");
+    }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.PartitionedFileAdapter322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFilePathAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFilePathAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.PartitionedFilePathAdapter322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ResourcesAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ResourcesAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+org.apache.spark.resource.ResourcesAdapter322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ShuffleAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ShuffleAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.ShuffleAdapter322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1 +0,0 @@
-tech.ytsaurus.spyt.SparkAdapter322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapterBase	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapterBase	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.SparkAdapterBase322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/resources/META-INF/services/tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator322
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/SparkConfDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/SparkConfDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,34 @@
+package org.apache.spark
+
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.SparkConf")
+class SparkConfDecorators {
+
+  @DecoratedMethod
+  private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = {
+    val self = __loadFromSystemProperties(silent)
+    SparkConfExtensions.loadFromEnvironment(self, silent)
+    self
+  }
+
+  private[spark] def __loadFromSystemProperties(silent: Boolean): SparkConf = ???
+}
+
+private[spark] object SparkConfExtensions {
+  private[spark] def loadFromEnvironment(conf: SparkConf, silent: Boolean): SparkConf = {
+    for ((key, value) <- sys.env if key.startsWith("SPARK_")) {
+      conf.set(SparkConfExtensions.envToConfName(key), value, silent)
+    }
+    conf
+  }
+
+  private[spark] def envToConfName(envName: String): String = {
+    envName.toLowerCase().replace("_", ".")
+  }
+
+  private[spark] def confToEnvName(confName: String): String = {
+    confName.replace(".", "_").toUpperCase()
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/api/python/spyt/patch/PythonFunction.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/api/python/spyt/patch/PythonFunction.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,33 @@
+package org.apache.spark.api.python.spyt.patch
+
+import java.util.{List => JList, Map => JMap}
+import org.apache.spark.api.python.{PythonAccumulatorV2, PythonBroadcast}
+import org.apache.spark.broadcast.Broadcast
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, OriginClass}
+
+@OriginClass("org.apache.spark.api.python.PythonFunction")
+@Applicability(to = "3.3.4")
+private[spark] case class PythonFunction(
+    command: Seq[Byte],
+    envVars: JMap[String, String],
+    pythonIncludes: JList[String],
+    private val _pythonExec: String,
+    pythonVer: String,
+    broadcastVars: JList[Broadcast[PythonBroadcast]],
+    accumulator: PythonAccumulatorV2) {
+
+  def this(
+      command: Array[Byte],
+      envVars: JMap[String, String],
+      pythonIncludes: JList[String],
+      _pythonExec: String,
+      pythonVer: String,
+      broadcastVars: JList[Broadcast[PythonBroadcast]],
+      accumulator: PythonAccumulatorV2) = {
+    this(command.toSeq, envVars, pythonIncludes, _pythonExec, pythonVer, broadcastVars, accumulator)
+  }
+
+  def pythonExec: String = {
+    sys.env.getOrElse("PYSPARK_EXECUTOR_PYTHON", _pythonExec)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/PythonRunnerDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/PythonRunnerDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,19 @@
+package org.apache.spark.deploy
+
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.PythonRunner$")
+object PythonRunnerDecorators {
+
+  @DecoratedMethod
+  def main(args: Array[String]): Unit = {
+    val redirectToStderr = System.getProperty("spark.ytsaurus.redirectToStderr", "false").toBoolean
+    if (redirectToStderr) {
+      System.setOut(System.err)
+    }
+    __main(args)
+  }
+
+  def __main(args: Array[String]): Unit = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/SparkSubmitSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/SparkSubmitSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,354 @@
+package org.apache.spark.deploy
+
+import javassist.CtMethod
+import javassist.bytecode.{CodeAttribute, ConstPool, Opcode, StackMapTable, YTsaurusBytecodeUtils}
+import org.apache.hadoop.conf.{Configuration => HadoopConfiguration}
+import org.apache.spark.SparkConf
+import org.apache.spark.deploy.SubmitSupport.instance._
+import org.apache.spark.internal.config._
+import org.apache.spark.launcher.SparkLauncher
+import org.apache.spark.util.{DependencyUtils, Utils}
+import tech.ytsaurus.spyt.patch.MethodProcesor
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+import scala.collection.mutable.ArrayBuffer
+import scala.sys.process.Process
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.SparkSubmit")
+private[spark] class SparkSubmitSpyt {
+  import YTsaurusConstants._
+  import SparkSubmitSpyt._
+
+  @DecoratedMethod(baseMethodProcessors = Array(classOf[ClusterManagerInitializerBytecodeModifier]))
+  private[deploy] def prepareSubmitEnvironment(args: SparkSubmitArguments,
+                                               conf: Option[HadoopConfiguration] = None)
+  : (Seq[String], Seq[String], SparkConf, String) = {
+
+    // Exact copy of the corresponding construct in base method
+    val deployMode: Int = args.deployMode match {
+      case "client" | null => CLIENT
+      case "cluster" => CLUSTER
+      case _ =>
+        error("Deploy mode must be either client or cluster")
+        -1
+    }
+
+    // Other cluster managers are processed inside __prepareSubmitEnvironment
+    val clusterManager: Int = if (args.master.startsWith(YTSAURUS_MASTER)) YTSAURUS else 0
+    val isYTsaurusCluster = clusterManager == YTSAURUS && deployMode == CLUSTER
+
+    // here we use args.master.startsWith instead of STANDALONE because it should be processed inside base method
+    if (args.isPython && args.master.startsWith("spark") && deployMode == CLUSTER) {
+      args.mainClass = "org.apache.spark.deploy.PythonRunner"
+      args.childArgs = ArrayBuffer("{{USER_JAR}}", "{{PY_FILES}}") ++ args.childArgs
+      args.files = DependencyUtils.mergeFileLists(args.files, args.pyFiles)
+    }
+
+    if (clusterManager == YTSAURUS) {
+
+      if (!Utils.classIsLoadable(YTSAURUS_CLUSTER_SUBMIT_CLASS) && !Utils.isTesting) {
+        error(
+          "Could not load YTSAURUS classes. " +
+            "It seems that YTSAURUS libraries are not in the environment. " +
+            "To add them the following steps should be performed:\n\n" +
+            "1. Install ytsaurus-spyt python package via \033[1mpip install ytsaurus-spyt\033[0m\n" +
+            "2. Activate SPYT configuration in environment by running " +
+            "\033[1msource spyt-env\033[0m command\n")
+      }
+
+      // This property is used  to initialize ytsaurus file system which is subclass of
+      // org.apache.hadoop.fs.FileSystem via spark hadoop configuration
+      args.sparkProperties += "spark.hadoop.yt.proxy" -> args.master.substring("ytsaurus://".length)
+    }
+
+    // This is a potential security issue, should be fixed ASAP, see SPYT-604 for details
+    val ytToken = sys.env.get("SPARK_YT_TOKEN")
+    if (ytToken.isDefined) {
+      args.sparkProperties += "spark.hadoop.yt.token" -> ytToken.get
+    }
+
+    var (childArgsSeq, childClasspath, sparkConf, childMainClass) = __prepareSubmitEnvironment(args, conf)
+
+    val childArgs = new ArrayBuffer[String]()
+
+    // This section is a copy of the corresponding section of super.prepareSubmitEnvironment
+    val options = List[OptionAssigner](
+      // Updated copy from SparkSubmit
+      OptionAssigner(args.packages, YTSAURUS, CLUSTER, confKey = JAR_PACKAGES.key),
+      OptionAssigner(args.repositories, YTSAURUS, CLUSTER, confKey = JAR_REPOSITORIES.key),
+      OptionAssigner(args.ivyRepoPath, YTSAURUS, CLUSTER, confKey = JAR_IVY_REPO_PATH.key),
+      OptionAssigner(args.packagesExclusions, YTSAURUS, CLUSTER, confKey = JAR_PACKAGES_EXCLUSIONS.key),
+
+      OptionAssigner(args.numExecutors, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_INSTANCES.key),
+      OptionAssigner(args.executorCores, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_CORES.key),
+      OptionAssigner(args.executorMemory, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_MEMORY.key),
+      OptionAssigner(args.files, YTSAURUS, ALL_DEPLOY_MODES, confKey = FILES.key),
+      OptionAssigner(args.archives, YTSAURUS, ALL_DEPLOY_MODES, confKey = ARCHIVES.key),
+      OptionAssigner(args.jars, YTSAURUS, ALL_DEPLOY_MODES, confKey = JARS.key),
+      OptionAssigner(args.driverMemory, YTSAURUS, CLUSTER, confKey = DRIVER_MEMORY.key),
+      OptionAssigner(args.driverCores, YTSAURUS, CLUSTER, confKey = DRIVER_CORES.key),
+
+      // YTsaurus only
+      OptionAssigner(args.queue, YTSAURUS, ALL_DEPLOY_MODES, confKey = YTSAURUS_POOL.key),
+      OptionAssigner(args.isPython.toString, YTSAURUS, ALL_DEPLOY_MODES, confKey = YTSAURUS_IS_PYTHON.key),
+      OptionAssigner(System.getenv("Y_PYTHON_ENTRY_POINT"), YTSAURUS, CLUSTER,
+        confKey = YTSAURUS_PYTHON_BINARY_ENTRY_POINT.key, mergeFn = Some(confOverEnv))
+    )
+
+    childArgs ++= processOptions(options, deployMode, clusterManager, sparkConf)
+
+    if (isBinary(args.primaryResource)) {
+      sparkConf.set(YTSAURUS_IS_PYTHON_BINARY, true)
+    }
+
+    if (clusterManager == YTSAURUS && args.isPython && deployMode == CLIENT &&
+        !sparkConf.contains(YTSAURUS_PYTHON_EXECUTABLE.key) && !sparkConf.get(YTSAURUS_IS_PYTHON_BINARY)) {
+      val basePythonExecutable = getBasePythonExec(sparkConf)
+      sparkConf.set(YTSAURUS_PYTHON_EXECUTABLE.key, basePythonExecutable)
+    }
+
+    if (sparkConf.get(YTSAURUS_IS_PYTHON_BINARY) && deployMode == CLIENT) {
+      val pyBinaryWrapper = pythonBinaryWrapperPath(sys.env("SPYT_ROOT"))
+      sparkConf.set(PYSPARK_DRIVER_PYTHON, pyBinaryWrapper)
+    }
+
+    if (isYTsaurusCluster) {
+      childMainClass = YTSAURUS_CLUSTER_SUBMIT_CLASS
+      if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
+        if (args.isPython) {
+          childArgs ++= Array("--primary-py-file", args.primaryResource)
+          childArgs ++= Array("--main-class", "org.apache.spark.deploy.PythonRunner")
+        } else if (args.isR) {
+          childArgs ++= Array("--primary-r-file", args.primaryResource)
+          childArgs ++= Array("--main-class", "org.apache.spark.deploy.RRunner")
+        }
+        else {
+          childArgs ++= Array("--primary-java-resource", args.primaryResource)
+          childArgs ++= Array("--main-class", args.mainClass)
+        }
+      } else {
+        childArgs ++= Array("--main-class", args.mainClass)
+      }
+
+      if (args.isPython) {
+        sparkConf.set("spark.ytsaurus.redirectToStderr", "true")
+      }
+
+      if (args.childArgs != null) {
+        appendChildArgs(args, childArgs)
+      }
+    }
+
+    if (clusterManager == YTSAURUS) {
+      sparkConf.set("spark.ytsaurus.primary.resource", args.primaryResource)
+    }
+
+    (childArgsSeq ++ childArgs, childClasspath, sparkConf, childMainClass)
+  }
+
+  private[deploy] def __prepareSubmitEnvironment(args: SparkSubmitArguments, conf: Option[HadoopConfiguration] = None)
+  : (Seq[String], Seq[String], SparkConf, String) = ???
+
+  @DecoratedMethod
+  private def error(msg: String): Unit = {
+    if (!msg.equals("Cluster deploy mode is currently not supported for python applications on standalone clusters.")) {
+      __error(msg)
+    }
+  }
+
+  def __error(msg: String): Unit = ???
+}
+
+object SparkSubmitSpyt {
+  import YTsaurusConstants._
+
+  // YTsaurus additions
+  private val ALL_CLUSTER_MGRS: Int = {
+    val cls = SparkSubmit.getClass
+    val field = cls.getDeclaredField("org$apache$spark$deploy$SparkSubmit$$ALL_CLUSTER_MGRS")
+    field.setAccessible(true)
+    val value = field.getInt(SparkSubmit) | YTSAURUS
+    field.set(SparkSubmit, value)
+    field.setAccessible(false)
+    value
+  }
+
+  // Exact copy from object SparkSubmit
+  private val CLIENT = 1
+  private val CLUSTER = 2
+  private val ALL_DEPLOY_MODES = CLIENT | CLUSTER
+
+  private[deploy] val YTSAURUS_CLUSTER_SUBMIT_CLASS =
+    "org.apache.spark.deploy.ytsaurus.YTsaurusClusterApplication"
+
+  private[deploy] def processOptions(options: List[OptionAssigner],
+                                     deployMode: Int,
+                                     clusterManager: Int,
+                                     sparkConf: SparkConf): Seq[String] = {
+    val childArgs = new ArrayBuffer[String]()
+    for (opt <- options) {
+      if (opt.value != null &&
+        (deployMode & opt.deployMode) != 0 &&
+        (clusterManager & opt.clusterManager) != 0) {
+        if (opt.clOption != null) { childArgs += (opt.clOption, opt.value) }
+        if (opt.confKey != null) {
+          if (opt.mergeFn.isDefined && sparkConf.contains(opt.confKey)) {
+            sparkConf.set(opt.confKey, opt.mergeFn.get.apply(sparkConf.get(opt.confKey), opt.value))
+          } else {
+            sparkConf.set(opt.confKey, opt.value)
+          }
+        }
+      }
+    }
+    childArgs
+  }
+
+  private[deploy] def appendChildArgs(args: SparkSubmitArguments, childArgs: ArrayBuffer[String]): Unit = {
+    args.childArgs.foreach { arg =>
+      childArgs += ("--arg", arg)
+    }
+  }
+
+  private[deploy] def getBasePythonExec(sparkConf: SparkConf): String = {
+    // Exact copy from main method of org.apache.spark.deploy.PythonRunner
+    val pythonExec = sparkConf.get(PYSPARK_DRIVER_PYTHON)
+      .orElse(sparkConf.get(PYSPARK_PYTHON))
+      .orElse(sys.env.get("PYSPARK_DRIVER_PYTHON"))
+      .orElse(sys.env.get("PYSPARK_PYTHON"))
+      .getOrElse("python3")
+
+    val pythonVersion = Process(pythonExec, Seq("-V")).!!.replace("Python","").trim()
+    val pythonVersionShort = pythonVersion.substring(0, pythonVersion.indexOf('.', pythonVersion.indexOf('.') + 1))
+    s"python$pythonVersionShort"
+  }
+
+  private val knownExtensions = Set("jar", "py", "r")
+
+  private[deploy] def isBinary(res: String): Boolean = {
+    if (res == null || SparkSubmit.isShell(res) || SparkSubmit.isInternal(res)) {
+      false
+    } else {
+      val lastDot = res.lastIndexOf(".")
+      lastDot == -1 || !knownExtensions.contains(res.substring(lastDot + 1).toLowerCase)
+    }
+  }
+
+  private val confOverEnv: (String, String) => String = (confValue, _) => confValue
+}
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.SparkSubmit$")
+object SparkSubmitDecorators {
+
+  @DecoratedMethod
+  private[deploy] def isPython(res: String): Boolean = {
+    __isPython(res) || SparkSubmitSpyt.isBinary(res)
+  }
+
+  private[deploy] def __isPython(res: String): Boolean = ???
+}
+
+
+object YTsaurusConstants {
+  val YTSAURUS = 32
+  val YTSAURUS_MASTER = "ytsaurus"
+}
+
+class ClusterManagerInitializerBytecodeModifier extends MethodProcesor {
+  import YTsaurusBytecodeUtils._
+  import YTsaurusConstants._
+
+  override def process(method: CtMethod): Unit = {
+    val methodInfo = method.getMethodInfo
+    val cp = methodInfo.getConstPool
+    val ca = methodInfo.getCodeAttribute
+
+    updateErrorMessage(cp)
+    addYTsaurusClause(cp, ca)
+  }
+
+  private def addYTsaurusClause(cp: ConstPool, ca: CodeAttribute): Unit = {
+    val localConstId = findStringConstant(cp, _.equals("local"))
+
+    val ci = ca.iterator()
+
+    // Looking for start of the "local" cluster manager clause
+    var found: Boolean = false
+    var samplePos = 0
+    while (ci.hasNext && !found) {
+      val index = ci.next()
+      val op = ci.byteAt(index);
+      if (op == Opcode.ALOAD && ci.byteAt(ci.lookAhead()) == Opcode.LDC_W) {
+        val ldcwOpIndex = ci.next()
+        val ldcwArgument = ci.u16bitAt(ldcwOpIndex + 1)
+        if (ldcwArgument == localConstId) {
+          found = true
+          samplePos = index
+        }
+      }
+    }
+
+    // Looking for the end of the "local" cluster manager clause
+    found = false
+    var insertPos = 0
+    while (ci.hasNext && !found) {
+      val index = ci.next()
+      val op = ci.byteAt(index)
+      if (op == Opcode.GOTO && ci.u16bitAt(index + 1) == 3) { // found goto 3
+        found = true
+        insertPos = ci.lookAhead()
+      }
+    }
+
+    val sampleLength = insertPos - samplePos
+    val sample = new Array[Byte](sampleLength)
+
+    System.arraycopy(ca.getCode, samplePos, sample, 0, sampleLength)
+
+    val ytsaurusConstId = cp.addStringInfo(YTSAURUS_MASTER)
+    val ytsaurusCodeId = cp.addIntegerInfo(YTSAURUS)
+
+    val si = new CodeAttribute(cp, ca.getMaxStack, ca.getMaxLocals, sample, ca.getExceptionTable).iterator
+
+    var skipInvokeVirtual = false
+    val sameFramePositions = new java.util.ArrayList[Integer]()
+    while (si.hasNext) {
+      val index = si.next
+      val op = si.byteAt(index)
+
+      if (op == Opcode.LDC_W) {
+        si.write16bit(ytsaurusConstId, index + 1)
+      } else if (op == Opcode.GETSTATIC) {
+        skipInvokeVirtual = true
+        si.writeByte(Opcode.LDC_W, index)
+        si.write16bit(ytsaurusCodeId, index + 1)
+      } else if (op == Opcode.INVOKEVIRTUAL && skipInvokeVirtual) {
+        // fill second invokevirtual with nop because we use constant instead of accessing object fields
+        for (pos <- index until si.lookAhead) {
+          si.writeByte(Opcode.NOP, pos)
+        }
+      } else if (op == Opcode.IFEQ || op == Opcode.GOTO) {
+        val offset = si.u16bitAt(index + 1)
+        if (index + offset <= sampleLength) {
+          sameFramePositions.add(insertPos + index + offset)
+        }
+      }
+    }
+
+    ci.insertAt(insertPos, sample)
+
+    val originTable = ca.getAttribute(StackMapTable.tag).asInstanceOf[StackMapTable]
+    val modifiedTable = addStackMapTableFrames(originTable, sameFramePositions, insertPos, sampleLength)
+    ca.setAttribute(modifiedTable)
+  }
+
+  private def updateErrorMessage(cp: ConstPool): Unit = {
+    val invalidClusterManagerMsgId = findStringConstant(cp, _.startsWith("Master must either"))
+    val invalidClusterManagerMsg = cp.getStringInfo(invalidClusterManagerMsgId);
+    updateUtf8Constant(
+      cp, getUtf8ConstantId(cp, invalidClusterManagerMsgId),
+      invalidClusterManagerMsg.replace("k8s,", "k8s, ytsaurus")
+    )
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/FsHistoryProviderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/FsHistoryProviderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,33 @@
+package org.apache.spark.deploy.history
+
+import org.apache.hadoop.fs.{FileSystem, Path}
+import org.apache.spark.SparkConf
+import tech.ytsaurus.spyt.SparkVersionUtils
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.history.FsHistoryProvider")
+class FsHistoryProviderDecorators {
+
+  @DecoratedMethod
+  private def startPolling(): Unit = {
+    val path = new Path(logDir)
+    val confFieldName = if (SparkVersionUtils.lessThan("3.3.0")) {
+      "conf"
+    } else {
+      "org$apache$spark$deploy$history$FsHistoryProvider$$conf"
+    }
+
+    val conf: SparkConf = this.getClass.getDeclaredField(confFieldName).get(this).asInstanceOf[SparkConf]
+
+    if (!fs.exists(path) && conf.getBoolean("spark.history.fs.createLogDirectory", defaultValue = false)) {
+      fs.mkdirs(path)
+    }
+
+    __startPolling()
+  }
+
+  private def __startPolling(): Unit = ???
+  private val logDir: String = ???
+  private[history] val fs: FileSystem = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/RollingEventLogFilesWriterSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/RollingEventLogFilesWriterSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package org.apache.spark.deploy.history
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.spark.SparkConf
+import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
+
+import java.net.URI
+
+@Subclass
+@OriginClass("org.apache.spark.deploy.history.RollingEventLogFilesWriter")
+class RollingEventLogFilesWriterSpyt(
+    appId: String,
+    appAttemptId : Option[String],
+    logBaseDir: URI,
+    sparkConf: SparkConf,
+    hadoopConf: Configuration)
+  extends RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {
+
+  override def stop(): Unit = {
+    super.stop()
+    fileSystem.close()
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/SingleEventLogFileWriterSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/history/SingleEventLogFileWriterSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package org.apache.spark.deploy.history
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.spark.SparkConf
+import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
+
+import java.net.URI
+
+@Subclass
+@OriginClass("org.apache.spark.deploy.history.SingleEventLogFileWriter")
+class SingleEventLogFileWriterSpyt(
+    appId: String,
+    appAttemptId : Option[String],
+    logBaseDir: URI,
+    sparkConf: SparkConf,
+    hadoopConf: Configuration)
+  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {
+
+  override def stop(): Unit = {
+    super.stop()
+    fileSystem.close()
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/master/MasterDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/master/MasterDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,64 @@
+package org.apache.spark.deploy.master
+
+import org.apache.spark.deploy.DriverDescription
+import org.apache.spark.deploy.master.DriverState.DriverState
+import org.apache.spark.internal.Logging
+import org.apache.spark.rpc.RpcEndpointRef
+import tech.ytsaurus.spyt.adapter.ClusterSupport.{instance => cs}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+import scala.collection.mutable
+import scala.collection.mutable.ArrayBuffer
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.master.Master")
+class MasterDecorators {
+
+  @DecoratedMethod
+  def org$apache$spark$deploy$master$Master$$removeDriver(
+    driverId: String,
+    finalState: DriverState,
+    exception: Option[Exception]): Unit = {
+    val completedDrivers = org$apache$spark$deploy$master$Master$$completedDrivers
+    val before = new ArrayBuffer[DriverInfo]
+    completedDrivers.copyToBuffer(before)
+
+    __org$apache$spark$deploy$master$Master$$removeDriver(driverId, finalState, exception)
+
+    MasterDecorators.checkAndRemoveDriverToAppId(self, before, completedDrivers)
+  }
+
+  def __org$apache$spark$deploy$master$Master$$removeDriver(
+    driverId: String,
+    finalState: DriverState,
+    exception: Option[Exception]): Unit = ???
+
+  @DecoratedMethod
+  def org$apache$spark$deploy$master$Master$$createDriver(desc: DriverDescription): DriverInfo = {
+    val dInfo = __org$apache$spark$deploy$master$Master$$createDriver(desc)
+    val newDesc = dInfo.desc.copy(command =
+      desc.command.copy(javaOpts = s"-Dspark.driverId=${dInfo.id}" +: desc.command.javaOpts))
+    new DriverInfo(dInfo.startTime, dInfo.id, newDesc, dInfo.submitDate)
+  }
+
+  def __org$apache$spark$deploy$master$Master$$createDriver(desc: DriverDescription): DriverInfo = ???
+
+  final def self: RpcEndpointRef = ???
+
+  private val org$apache$spark$deploy$master$Master$$completedDrivers: mutable.ArrayBuffer[DriverInfo] = ???
+}
+
+object MasterDecorators extends Logging {
+  def checkAndRemoveDriverToAppId(
+    self: RpcEndpointRef,
+    completedDriversBefore: mutable.ArrayBuffer[DriverInfo],
+    completedDriversAfter: mutable.ArrayBuffer[DriverInfo]): Unit = {
+    var i = 0
+    while (i < completedDriversBefore.length && completedDriversBefore(i).id != completedDriversAfter.head.id) {
+      val driverId = completedDriversBefore(i).id
+      logInfo(s"Requesting to unregister driverId $driverId to app")
+      self.send(cs.msgUnregisterDriverToAppId(driverId))
+      i += 1
+    }
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClientAppSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClientAppSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,135 @@
+package org.apache.spark.deploy.rest
+
+import org.apache.commons.io.FileUtils
+import org.apache.hadoop.fs.FileSystem
+import org.apache.spark.deploy.SparkHadoopUtil
+import org.apache.spark.internal.Logging
+import org.apache.spark.{SparkConf, SparkException}
+import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
+
+import java.io.File
+import java.net.URI
+import scala.annotation.tailrec
+import scala.concurrent.duration._
+import scala.util.{Failure, Random, Success, Try}
+
+@Subclass
+@OriginClass("org.apache.spark.deploy.rest.RestSubmissionClientApp")
+class RestSubmissionClientAppSpyt extends RestSubmissionClientApp with Logging {
+
+  override def run(appResource: String,
+                   mainClass: String,
+                   appArgs: Array[String],
+                   conf: SparkConf,
+                   env: Map[String, String]): SubmitRestProtocolResponse = {
+    // Almost exact copy of super.run(...) method, with spark.rest.master property instead of spark.master
+    val master = conf.getOption("spark.rest.master").getOrElse {
+      throw new IllegalArgumentException("'spark.rest.master' must be set.")
+    }
+
+    RestSubmitSupport.instance.createSubmission(appResource, mainClass, appArgs, conf, env, master)
+  }
+
+  @tailrec
+  private def getSubmissionStatus(submissionId: String,
+                                  client: RestSubmissionClient,
+                                  retry: Int,
+                                  retryInterval: Duration,
+                                  rnd: Random = new Random): SubmissionStatusResponse = {
+    val response = Try(client.requestSubmissionStatus(submissionId)
+      .asInstanceOf[SubmissionStatusResponse])
+    response match {
+      case Success(value) => value
+      case Failure(exception) if retry > 0 =>
+        log.error(s"Exception while getting submission status: ${exception.getMessage}")
+        val sleepInterval = if (retryInterval > 1.second) {
+          1000 + rnd.nextInt(retryInterval.toMillis.toInt - 1000)
+        } else rnd.nextInt(retryInterval.toMillis.toInt)
+        Thread.sleep(sleepInterval)
+        getSubmissionStatus(submissionId, client, retry - 1, retryInterval, rnd)
+      case Failure(exception) => throw exception
+    }
+  }
+
+  def awaitAppTermination(submissionId: String,
+                          conf: SparkConf,
+                          checkStatusInterval: Duration): Unit = {
+    import org.apache.spark.deploy.master.DriverState._
+
+    val master = conf.getOption("spark.rest.master").getOrElse {
+      throw new IllegalArgumentException("'spark.rest.master' must be set.")
+    }
+    val client = new RestSubmissionClient(master)
+    val runningStates = Set(RUNNING.toString, SUBMITTED.toString)
+    val finalStatus = Stream.continually {
+      Thread.sleep(checkStatusInterval.toMillis)
+      val response = getSubmissionStatus(submissionId, client, retry = 3, checkStatusInterval)
+      logInfo(s"Driver report for $submissionId (state: ${response.driverState})")
+      response
+    }.find(response => !runningStates.contains(response.driverState)).get
+    logInfo(s"Driver $submissionId finished with status ${finalStatus.driverState}")
+    finalStatus.driverState match {
+      case s if s == FINISHED.toString => // success
+      case s if s == FAILED.toString =>
+        throw new SparkException(s"Driver $submissionId failed")
+      case _ =>
+        throw new SparkException(s"Driver $submissionId failed with unexpected error")
+    }
+  }
+
+  def shutdownYtClient(sparkConf: SparkConf): Unit = {
+    val hadoopConf = SparkHadoopUtil.newConfiguration(sparkConf)
+    val fs = FileSystem.get(new URI("yt:///"), hadoopConf)
+    fs.close()
+  }
+
+  private def writeToFile(file: File, message: String): Unit = {
+    val tmpFile = new File(file.getParentFile, s"${file.getName}_tmp")
+    FileUtils.writeStringToFile(tmpFile, message)
+    FileUtils.moveFile(tmpFile, file)
+  }
+
+  override def start(args: Array[String], conf: SparkConf): Unit = {
+    val submissionIdFile = conf.getOption("spark.rest.client.submissionIdFile").map(new File(_))
+    val submissionErrorFile = conf.getOption("spark.rest.client.submissionErrorFile")
+      .map(new File(_))
+    try {
+      // Here starts an almost exact copy of super.start(...) method
+      if (args.length < 2) {
+        sys.error("Usage: RestSubmissionClient [app resource] [main class] [app args*]")
+        sys.exit(1)
+      }
+      val appResource = args(0)
+      val mainClass = args(1)
+      val appArgs = args.slice(2, args.length)
+      val confEnv = conf.getAll.filter {
+        case (key, _) => key.startsWith("spark.yt") || key.startsWith("spark.hadoop.yt")
+      }.map {
+        case (key, value) => key.toUpperCase().replace(".", "_") -> value
+      }.toMap
+      val env = RestSubmissionClient.filterSystemEnvironment(sys.env) ++ confEnv
+
+      val submissionId = try {
+        val response = run(appResource, mainClass, appArgs, conf, env)
+        response match {
+          case r: CreateSubmissionResponse => r.submissionId
+          case _ => throw new IllegalStateException("Job is not submitted")
+        }
+      } finally {
+        shutdownYtClient(conf)
+      }
+
+      submissionIdFile.foreach(writeToFile(_, submissionId))
+
+      if (conf.getOption("spark.rest.client.awaitTermination.enabled").forall(_.toBoolean)) {
+        val checkStatusInterval = conf.getOption("spark.rest.client.statusInterval")
+          .map(_.toInt.seconds).getOrElse(5.seconds)
+        awaitAppTermination(submissionId, conf, checkStatusInterval)
+      }
+    } catch {
+      case e: Throwable =>
+        submissionErrorFile.foreach(writeToFile(_, e.getMessage))
+        throw e
+    }
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/rest/spyt/patch/StandaloneRestServer.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/rest/spyt/patch/StandaloneRestServer.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,43 @@
+package org.apache.spark.deploy.rest.spyt.patch
+
+import org.apache.spark.SparkConf
+import org.apache.spark.deploy.rest.RestSubmitSupport.{instance => rss}
+import org.apache.spark.deploy.rest._
+import org.apache.spark.rpc.RpcEndpointRef
+import tech.ytsaurus.spyt.patch.annotations.OriginClass
+
+
+/**
+ * Patches:
+ * 1. Set `host` parameter to null in superclass constructor. Main reason: we need to bind RPC endpoint to wildcard
+ *    network interface for Kubernetes deployments with host network.
+ */
+@OriginClass("org.apache.spark.deploy.rest.StandaloneRestServer")
+private[deploy] class StandaloneRestServer(host: String,
+                                           requestedPort: Int,
+                                           masterConf: SparkConf,
+                                           masterEndpoint: RpcEndpointRef,
+                                           masterUrl: String)
+  extends RestSubmissionServer(null, requestedPort, masterConf) {
+
+  protected override val submitRequestServlet =
+    new StandaloneSubmitRequestServlet(masterEndpoint, masterUrl, masterConf)
+  protected override val killRequestServlet =
+    new StandaloneKillRequestServlet(masterEndpoint, masterConf)
+  protected override val statusRequestServlet: StatusRequestServlet =
+    rss.statusRequestServlet(masterEndpoint, masterConf)
+
+  private val masterStateRequestServlet = rss.masterStateRequestServlet(masterEndpoint, masterConf)
+  private val appIdRequestServlet = rss.appIdRequestServlet(masterEndpoint, masterConf)
+  private val appStatusRequestServlet = rss.appStatusRequestServlet(masterEndpoint, masterConf)
+
+  protected override lazy val contextToServlet: Map[String, RestServlet] = Map(
+    s"$baseContext/create/*" -> submitRequestServlet,
+    s"$baseContext/kill/*" -> killRequestServlet,
+    s"$baseContext/status/*" -> statusRequestServlet,
+    s"$baseContext/master/*" -> masterStateRequestServlet,
+    s"$baseContext/getAppId/*" -> appIdRequestServlet,
+    s"$baseContext/getAppStatus/*" -> appStatusRequestServlet,
+    "/*" -> new ErrorServlet // default handler
+  )
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/DriverRunnerDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/DriverRunnerDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,69 @@
+package org.apache.spark.deploy.worker
+
+import org.apache.spark.SparkConf
+import org.apache.spark.deploy.{DriverDescription, SparkHadoopUtil}
+import org.apache.spark.deploy.worker.DriverRunnerDecorators.addLocalPyFiles
+import org.apache.spark.internal.Logging
+import org.apache.spark.internal.config.SUBMIT_PYTHON_FILES
+import org.apache.spark.util.Utils
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+import java.io.{File, IOException}
+import java.net.URI
+import java.util.stream.Collectors
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.worker.DriverRunner")
+class DriverRunnerDecorators {
+
+  @DecoratedMethod
+  private def runDriver(builder: ProcessBuilder, baseDir: File, supervise: Boolean): Int = {
+    val conf = this.getClass.getDeclaredField("conf").get(this).asInstanceOf[SparkConf]
+    val driverDesc = this.getClass.getDeclaredField("driverDesc").get(this).asInstanceOf[DriverDescription]
+    val localPyFiles = DriverRunnerDecorators.downloadPythonFiles(driverDesc, baseDir, conf)
+
+    builder.command(addLocalPyFiles(builder.command(), localPyFiles))
+
+    __runDriver(builder, baseDir, supervise)
+  }
+
+  private def __runDriver(builder: ProcessBuilder, baseDir: File, supervise: Boolean): Int = ???
+}
+
+object DriverRunnerDecorators extends Logging {
+  def downloadPythonFiles(driverDesc: DriverDescription, driverDir: File, conf: SparkConf): Seq[String] = {
+    val pyFiles = driverDesc.command.javaOpts.flatMap {
+      case opt if opt.trim.startsWith(s"-D${SUBMIT_PYTHON_FILES.key}=") =>
+        Utils.stringToSeq(opt.substring(opt.indexOf("=") + 1))
+      case _ => Nil
+    }
+    logInfo(s"Found python dependencies: ${pyFiles}")
+    pyFiles.map { pyFileUrl =>
+      // almost exact copy of a DriverRunner.downloadUserJar method
+      val fileName = new URI(pyFileUrl).getPath.split("/").last
+      val localFile = new File(driverDir, fileName)
+      if (!localFile.exists()) {
+        logInfo(s"Copying supplied python file $pyFileUrl to $localFile")
+        Utils.fetchFile(
+          pyFileUrl,
+          driverDir,
+          conf,
+          SparkHadoopUtil.get.newConfiguration(conf),
+          System.currentTimeMillis(),
+          useCache = false
+        )
+        if (!localFile.exists()) { // Verify copy succeeded
+          throw new IOException(s"Can not find expected file $fileName which should have been loaded in $driverDir")
+        }
+      }
+      localFile.getAbsolutePath
+    }
+  }
+
+  def addLocalPyFiles(command: java.util.List[String], localPyFiles: Seq[String]): java.util.List[String] = {
+    command.stream().map {
+      case "{{PY_FILES}}" => localPyFiles.mkString(",")
+      case other => other
+    }.collect(Collectors.toList()).asInstanceOf[java.util.List[String]]
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/DriverWrapperDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/DriverWrapperDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,27 @@
+package org.apache.spark.deploy.worker
+
+import org.apache.hadoop.fs.FileSystem
+import org.apache.spark.SparkConf
+import org.apache.spark.deploy.SparkHadoopUtil
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+import java.net.URI
+
+@Decorate
+@OriginClass("org.apache.spark.deploy.worker.DriverWrapper$")
+object DriverWrapperDecorators {
+
+  @DecoratedMethod
+  def main(args: Array[String]): Unit = {
+    val conf = new SparkConf()
+    val hadoopConf = SparkHadoopUtil.newConfiguration(conf)
+    try {
+      __main(args)
+    } finally {
+      FileSystem.get(new URI("yt:///"), hadoopConf).close()
+    }
+  }
+
+  def __main(args: Array[String]): Unit = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/ui/LogPageSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/deploy/worker/ui/LogPageSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,32 @@
+package org.apache.spark.deploy.worker.ui
+
+import org.apache.spark.ui.UIUtils
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, OriginClass, Subclass}
+
+import javax.servlet.http.HttpServletRequest
+import scala.xml.{Elem, Node}
+
+/**
+ * This is a fix for a bug in spark 3.2.2 which doesn't include utils.js file on logPage, but uses a getBaseURI
+ * function from it. This bug still exists in versions 3.2.x, 3.3.x (except 3.3.4) and 3.4.x (was fixed since 3.4.2).
+ */
+@Subclass
+@OriginClass("org.apache.spark.deploy.worker.ui.LogPage")
+@Applicability(to = "3.4.1")
+class LogPageSpyt(parent: WorkerWebUI) extends LogPage(parent) {
+
+  override def render(request: HttpServletRequest): Seq[Node] = {
+    val content = super.render(request)
+    val utilsJs = <script src={UIUtils.prependBaseUri(request, "/static/utils.js")}></script>
+    val html = content.head
+    val head = html.child.find {
+      case e: Elem if e.label == "head" => true
+      case _ => false
+    }.get
+    val body = html.child.find {
+      case e: Elem if e.label == "body" => true
+      case _ => false
+    }.get
+    <html><head>{head.child ++ utilsJs}</head>{body}</html>
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/resource/ResourcesAdapter322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/resource/ResourcesAdapter322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,12 @@
+package org.apache.spark.resource
+
+import org.apache.spark.resource.ResourceProfile.ExecutorResourcesOrDefaults
+import tech.ytsaurus.spyt.{MinSparkVersion, ResourcesAdapter}
+
+@MinSparkVersion("3.2.2")
+class ResourcesAdapter322 extends ResourcesAdapter {
+
+  override def getExecutorCores(execResources: Product): Int = {
+    execResources.asInstanceOf[ExecutorResourcesOrDefaults].cores
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcAddress.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcAddress.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,24 @@
+package org.apache.spark.rpc.spyt.patch
+
+import tech.ytsaurus.spyt.patch.annotations.OriginClass
+
+import tech.ytsaurus.spyt.Utils
+
+/**
+ * Patches:
+ * 1. Support for ipV6 addresses in hostPort
+ *
+ */
+@OriginClass("org.apache.spark.rpc.RpcAddress")
+private[spark] case class RpcAddress(host: String, port: Int) {
+
+  def hostPort: String = {
+    val newHost = Utils.addBracketsIfIpV6Host(host)
+    s"$newHost:$port"
+  }
+
+  /** Returns a string in the form of "spark://host:port". */
+  def toSparkURL: String = "spark://" + hostPort
+
+  override def toString: String = hostPort
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEndpointAddress.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEndpointAddress.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,56 @@
+package org.apache.spark.rpc.spyt.patch
+
+import org.apache.spark.SparkException
+import tech.ytsaurus.spyt.patch.annotations.OriginClass
+import tech.ytsaurus.spyt.Utils.{addBracketsIfIpV6Host, removeBracketsIfIpV6Host}
+
+
+/**
+ * Patches:
+ * 1. Support for ipV6 addresses in host
+ */
+@OriginClass("org.apache.spark.rpc.RpcEndpointAddress")
+case class RpcEndpointAddress(rpcAddress: RpcAddress, name: String) {
+
+  require(name != null, "RpcEndpoint name must be provided.")
+
+  def this(host: String, port: Int, name: String) = {
+    this(RpcAddress(host, port), name)
+  }
+
+  override val toString = if (rpcAddress != null) {
+    s"spark://$name@${addBracketsIfIpV6Host(rpcAddress.host)}:${rpcAddress.port}"
+  } else {
+    s"spark-client://$name"
+  }
+}
+
+@OriginClass("org.apache.spark.rpc.RpcEndpointAddress$")
+private[spark] object RpcEndpointAddress {
+
+  def apply(host: String, port: Int, name: String): RpcEndpointAddress = {
+    new RpcEndpointAddress(host, port, name)
+  }
+
+  def apply(sparkUrl: String): RpcEndpointAddress = {
+    try {
+      val uri = new java.net.URI(sparkUrl)
+      val host = removeBracketsIfIpV6Host(uri.getHost)
+      val port = uri.getPort
+      val name = uri.getUserInfo
+      if (uri.getScheme != "spark" ||
+        host == null ||
+        port < 0 ||
+        name == null ||
+        (uri.getPath != null && !uri.getPath.isEmpty) || // uri.getPath returns "" instead of null
+        uri.getFragment != null ||
+        uri.getQuery != null) {
+        throw new SparkException("Invalid Spark URL: " + sparkUrl)
+      }
+      new RpcEndpointAddress(host, port, name)
+    } catch {
+      case e: java.net.URISyntaxException =>
+        throw new SparkException("Invalid Spark URL: " + sparkUrl, e)
+    }
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEnvConfig.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEnvConfig.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,22 @@
+package org.apache.spark.rpc.spyt.patch
+
+import org.apache.spark.{SecurityManager, SparkConf}
+import tech.ytsaurus.spyt.patch.annotations.OriginClass
+
+/**
+ * Patches:
+ * 1. bindAddress is var and is set to null in constructor. Main reason: we need to bind RPC endpoint to wildcard
+ *    network interface for Kubernetes deployments with host network.
+ */
+@OriginClass("org.apache.spark.rpc.RpcEnvConfig")
+private[spark] case class RpcEnvConfig(
+  conf: SparkConf,
+  name: String,
+  var bindAddress: String,
+  advertiseAddress: String,
+  port: Int,
+  securityManager: SecurityManager,
+  numUsableCores: Int,
+  clientMode: Boolean) {
+  this.bindAddress = null
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackendDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackendDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,36 @@
+package org.apache.spark.scheduler.cluster
+
+import org.apache.spark.{SparkConf, SparkContext}
+import org.apache.spark.rpc.RpcAddress
+import tech.ytsaurus.spyt.adapter.ClusterSupport.{instance => cs}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend")
+private[spark] class StandaloneSchedulerBackendDecorators {
+
+  @DecoratedMethod
+  def connected(appId: String): Unit = {
+    __connected(appId)
+    val sc = this.getClass.getDeclaredField("org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$sc")
+      .get(this).asInstanceOf[SparkContext]
+    val masters = this.getClass.getDeclaredField("masters").get(this).asInstanceOf[Array[String]]
+
+    StandaloneSchedulerBackendDecorators.registerDriver(sc, masters, conf, appId)
+  }
+
+  def __connected(appId: String): Unit = ???
+  protected val conf: SparkConf = ???
+}
+
+private[spark] object StandaloneSchedulerBackendDecorators {
+  def registerDriver(sc: SparkContext, masters: Array[String], conf: SparkConf, appId: String): Unit = {
+    conf.getOption("spark.driverId").foreach { driverId =>
+      val msg = cs.msgRegisterDriverToAppId(driverId, appId)
+      masters.foreach { masterUrl =>
+        val masterAddress = RpcAddress.fromSparkURL(masterUrl)
+        sc.env.rpcEnv.setupEndpointRef(masterAddress, cs.masterEndpointName).send(msg)
+      }
+    }
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/SparkSessionBuilderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/SparkSessionBuilderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,18 @@
+package org.apache.spark.sql
+
+import tech.ytsaurus.spyt.adapter.StorageSupport
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.SparkSession$Builder")
+class SparkSessionBuilderDecorators {
+
+  @DecoratedMethod
+  def getOrCreate(): SparkSession = {
+    val spark = __getOrCreate()
+    spark.experimental.extraOptimizations ++= StorageSupport.instance.createExtraOptimizations(spark)
+    spark
+  }
+
+  def __getOrCreate(): SparkSession = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,45 @@
+package org.apache.spark.sql.catalyst
+
+import org.apache.spark.sql.catalyst.ScalaReflection.universe.Type
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.types.{DataType, ObjectType}
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
+import tech.ytsaurus.spyt.types.UInt64Long
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.ScalaReflection$")
+@Applicability(to = "3.3.4")
+object ScalaReflectionDecorators322 {
+
+  @DecoratedMethod
+  private def serializerFor(inputObject: Expression,
+                            tpe: `Type`,
+                            walkedTypePath: WalkedTypePath,
+                            seenTypeSet: Set[`Type`]): Expression = baseType(tpe) match {
+    case _ if !inputObject.dataType.isInstanceOf[ObjectType] => inputObject
+    case t if isSubtype(t, ScalaReflection.localTypeOf[UInt64Long]) => ts.uInt64Serializer(inputObject)
+    case _ => __serializerFor(inputObject, tpe, walkedTypePath, seenTypeSet)
+  }
+
+
+  private def __serializerFor(inputObject: Expression,
+                              tpe: `Type`,
+                              walkedTypePath: WalkedTypePath,
+                              seenTypeSet: Set[`Type`]): Expression = ???
+
+  @DecoratedMethod
+  private def deserializerFor(tpe: `Type`,
+                              path: Expression,
+                              walkedTypePath: WalkedTypePath): Expression = baseType(tpe) match {
+    case t if !dataTypeFor(t).isInstanceOf[ObjectType] => path
+    case t if isSubtype(t, ScalaReflection.localTypeOf[UInt64Long]) => ts.uInt64Deserializer(path)
+    case _ => __deserializerFor (tpe, path, walkedTypePath)
+  }
+
+  private def __deserializerFor(tpe: `Type`, path: Expression, walkedTypePath: WalkedTypePath): Expression = ???
+
+  private def baseType(tpe: `Type`): `Type` = ???
+  private[catalyst] def isSubtype(tpe1: `Type`, tpe2: `Type`): Boolean = ???
+  private def dataTypeFor(tpe: `Type`): DataType = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,48 @@
+package org.apache.spark.sql.catalyst.catalog
+
+import org.apache.spark.internal.Logging
+import org.apache.spark.sql.catalyst.TableIdentifier
+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException
+import org.apache.spark.sql.errors.QueryCompilationErrors
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+import java.net.URI
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.catalog.SessionCatalog")
+class SessionCatalogDecorators {
+
+  @DecoratedMethod
+  def createTable(
+      tableDefinition: CatalogTable,
+      ignoreIfExists: Boolean,
+      validateLocation: Boolean = true): Unit = {
+    try {
+      __createTable(tableDefinition, ignoreIfExists, validateLocation)
+    } catch {
+      case taee: TableAlreadyExistsException => if (validateLocation) {
+        throw taee
+      } else {
+        // Table could be already created by insert operation
+        SessionCatalogDecorators.logInsertWarning()
+      }
+    }
+  }
+
+  def __createTable(tableDefinition: CatalogTable, ignoreIfExists: Boolean, validateLocation: Boolean): Unit = ???
+
+  @DecoratedMethod()
+  private def validateName(name: String): Unit = {
+    // Replacing original method with NOP implementation because YTsaurus table names doesn't conform to
+    // original validNameFormat regex
+  }
+
+  private def __validateName(name: String): Unit = ???
+}
+
+object SessionCatalogDecorators extends Logging {
+  def logInsertWarning(): Unit = {
+    logWarning("Table existence should not be ignored, but location is already validated. " +
+      "So modifiable operation has inserted data already")
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,46 @@
+package org.apache.spark.sql.catalyst.encoders
+
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.types.{DataType, ObjectType}
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
+import tech.ytsaurus.spyt.types.UInt64Long
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.encoders.RowEncoder$")
+@Applicability(to = "3.3.4")
+object RowEncoderDecorators322 {
+
+  @DecoratedMethod
+  def externalDataTypeFor(dt: DataType): DataType = dt match {
+    case ts.uInt64DataType => ObjectType(classOf[UInt64Long])
+    case _ => __externalDataTypeFor(dt)
+  }
+
+  def __externalDataTypeFor(dt: DataType): DataType = ???
+
+  @DecoratedMethod
+  @Applicability(to = "3.2.4")
+  private def serializerFor(inputObject: Expression, inputType: DataType): Expression = inputType match {
+    case ts.uInt64DataType => ts.uInt64Serializer(inputObject)
+    case _ => __serializerFor(inputObject, inputType)
+  }
+  private def __serializerFor(inputObject: Expression, inputType: DataType): Expression = ???
+
+  @DecoratedMethod
+  @Applicability(from = "3.3.0")
+  private def serializerFor(inputObject: Expression, inputType: DataType, lenient: Boolean): Expression =
+    inputType match {
+      case ts.uInt64DataType => ts.uInt64Serializer(inputObject)
+      case _ => __serializerFor(inputObject, inputType, lenient)
+    }
+
+  private def __serializerFor(inputObject: Expression, inputType: DataType, lenient: Boolean): Expression = ???
+
+  @DecoratedMethod
+  private def deserializerFor(input: Expression, dataType: DataType): Expression = dataType match {
+    case ts.uInt64DataType => ts.uInt64Deserializer(input)
+    case _ => __deserializerFor(input, dataType)
+  }
+  private def __deserializerFor(input: Expression, dataType: DataType): Expression = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastBaseDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastBaseDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,75 @@
+package org.apache.spark.sql.catalyst.expressions
+
+import org.apache.spark.sql.catalyst.expressions.codegen.{Block, CodegenContext, ExprValue}
+import org.apache.spark.sql.errors.QueryExecutionErrors
+import org.apache.spark.sql.types.{DataType, NullType}
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.CastBase")
+@Applicability(to = "3.3.4")
+class CastBaseDecorators {
+
+  @DecoratedMethod
+  private[this] def castToString(from: DataType): Any => Any = from match {
+    case ts.uInt64DataType => ts.uInt64CastToString
+    case _ => __castToString(from)
+  }
+
+  private[this] def __castToString(from: DataType): Any => Any = ???
+
+  @DecoratedMethod
+  private[this] def castToBinary(from: DataType): Any => Any = from match {
+    case ts.ysonDataType => ts.ysonCastToBinary
+    case _ => __castToBinary(from)
+  }
+  private[this] def __castToBinary(from: DataType): Any => Any = ???
+
+  @DecoratedMethod
+  protected[this] def cast(from: DataType, to: DataType): Any => Any = {
+    if (DataType.equalsStructurally(from, to)) {
+      __cast(from, to)
+    } else if (from == NullType) {
+      YTsaurusCastUtils.cannotCastFromNullTypeError(to)
+    } else {
+      to match {
+        case ts.uInt64DataType => ts.uInt64Cast(from)
+        case ts.ysonDataType => ts.ysonCast(from)
+        case _ => __cast(from, to)
+      }
+    }
+  }
+  protected[this] def __cast(from: DataType, to: DataType): Any => Any = ???
+
+  protected[this] type CastFunction = (ExprValue, ExprValue, ExprValue) => Block
+
+  @DecoratedMethod
+  private[this] def nullSafeCastFunction(from: DataType, to: DataType, ctx: CodegenContext): CastFunction = to match {
+    case ts.ysonDataType if !(from == NullType || to == from) => ts.binaryCastToYsonCode
+    case _ => __nullSafeCastFunction(from, to, ctx)
+  }
+
+  private[this] def __nullSafeCastFunction(from: DataType, to: DataType, ctx: CodegenContext): CastFunction = ???
+
+  @DecoratedMethod
+  private[this] def castToStringCode(from: DataType, ctx: CodegenContext): CastFunction = from match {
+    case ts.uInt64DataType => ts.uInt64CastToStringCode
+    case _ => __castToStringCode(from, ctx)
+  }
+
+  private[this] def __castToStringCode(from: DataType, ctx: CodegenContext): CastFunction = ???
+
+  @DecoratedMethod
+  private[this] def castToBinaryCode(from: DataType): CastFunction = from match {
+    case ts.ysonDataType => ts.ysonCastToBinaryCode
+    case _ => __castToBinaryCode(from)
+  }
+  private[this] def __castToBinaryCode(from: DataType): CastFunction = ???
+}
+
+object YTsaurusCastUtils {
+  def cannotCastFromNullTypeError(to: DataType): Any => Any = {
+    _ => throw QueryExecutionErrors.cannotCastFromNullTypeError(to)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,19 @@
+package org.apache.spark.sql.catalyst.expressions
+
+import org.apache.spark.sql.types.{BinaryType, DataType}
+import tech.ytsaurus.spyt.adapter.TypeSupport
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.Cast$")
+object CastDecorators {
+
+  @DecoratedMethod
+  def canCast(from: DataType, to: DataType): Boolean = (from, to) match {
+    case (TypeSupport.instance.ysonDataType, BinaryType) => true
+    case (BinaryType, TypeSupport.instance.ysonDataType) => true
+    case _ => __canCast(from, to)
+  }
+
+  def __canCast(from: DataType, to: DataType): Boolean = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/HashExpressionSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/HashExpressionSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,19 @@
+package org.apache.spark.sql.catalyst.expressions
+
+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport
+import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
+
+@Subclass
+@OriginClass("org.apache.spark.sql.catalyst.expressions.HashExpression")
+abstract class HashExpressionSpyt[E] extends HashExpression[E] {
+
+  override protected def computeHash(input: String,
+                                     dataType: DataType,
+                                     result: String,
+                                     ctx: CodegenContext): String = dataType match {
+    case TypeSupport.instance.uInt64DataType => genHashLong(input, result)
+    case _ => super.computeHash(input, dataType, result, ctx)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRowDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRowDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,18 @@
+package org.apache.spark.sql.catalyst.expressions
+
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.SpecificInternalRow")
+class SpecificInternalRowDecorators {
+
+  @DecoratedMethod
+  private[this] def dataTypeToMutableValue(dataType: DataType): MutableValue = dataType match {
+    case ts.uInt64DataType => new MutableLong
+    case _ => __dataTypeToMutableValue(dataType)
+  }
+
+  private[this] def __dataTypeToMutableValue(dataType: DataType): MutableValue = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGeneratorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGeneratorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,27 @@
+package org.apache.spark.sql.catalyst.expressions.codegen
+
+import org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.JAVA_LONG
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$")
+object CodeGeneratorDecorators {
+
+  @DecoratedMethod
+  def javaType(dt: DataType): String = dt match {
+    case ts.uInt64DataType => JAVA_LONG
+    case _ => __javaType(dt)
+  }
+
+  def __javaType(dt: DataType): String = ???
+
+  @DecoratedMethod
+  def javaClass(dt: DataType): Class[_] = dt match {
+    case ts.uInt64DataType => java.lang.Long.TYPE
+    case _ => __javaClass(dt)
+  }
+
+  def __javaClass(dt: DataType): Class[_] = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenContextDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenContextDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,19 @@
+package org.apache.spark.sql.catalyst.expressions.codegen
+
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext")
+class CodegenContextDecorators {
+
+  @DecoratedMethod
+  def genComp(dataType: DataType, c1: String, c2: String): String = dataType match {
+    case ts.uInt64DataType => s"java.lang.Long.compareUnsigned($c1, $c2)"
+    case _ => __genComp(dataType, c1, c2)
+  }
+
+  def __genComp(dataType: DataType, c1: String, c2: String): String = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilderSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilderSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,23 @@
+package org.apache.spark.sql.catalyst.parser
+
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.parser.AstBuilder")
+class AstBuilderSpyt {
+
+  @DecoratedMethod
+  def visitPrimitiveDataType(ctx: SqlBaseParser.PrimitiveDataTypeContext): DataType = {
+    val uint64Opt = TypeSupport.instance.extractUint64Opt(ctx)
+
+    if (uint64Opt.isDefined) {
+      uint64Opt.get
+    } else {
+      __visitPrimitiveDataType(ctx)
+    }
+  }
+
+  def __visitPrimitiveDataType(ctx: SqlBaseParser.PrimitiveDataTypeContext): DataType = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtilDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtilDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,35 @@
+package org.apache.spark.sql.execution
+
+import org.apache.hadoop.fs.{FileStatus, Path}
+import org.apache.spark.sql.SparkSession
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.adapter.StorageSupport.{instance => ssi}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.PartitionedFileUtil$")
+object PartitionedFileUtilDecorators {
+
+  @DecoratedMethod
+  def splitFiles(sparkSession: SparkSession,
+                 file: FileStatus,
+                 filePath: Path,
+                 isSplitable: Boolean,
+                 maxSplitBytes: Long,
+                 partitionValues: InternalRow): Seq[PartitionedFile] = {
+    if (ssi.shouldUseYtSplitFiles()) {
+      ssi.splitFiles(sparkSession, file, filePath, maxSplitBytes, partitionValues)
+    } else {
+      __splitFiles(sparkSession, file, filePath, isSplitable, maxSplitBytes, partitionValues)
+    }
+  }
+
+  def __splitFiles(sparkSession: SparkSession,
+                 file: FileStatus,
+                 filePath: Path,
+                 isSplitable: Boolean,
+                 maxSplitBytes: Long,
+                 partitionValues: InternalRow): Seq[PartitionedFile] = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/ShuffleSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/ShuffleSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,8 @@
+package org.apache.spark.sql.execution
+
+import org.apache.spark.Partitioner
+
+object ShuffleSupport {
+  def createShufflePartitioner(numPartitions: Int): Partitioner =
+    new PartitionIdPassthrough(numPartitions)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGeneratorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGeneratorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,27 @@
+package org.apache.spark.sql.execution.aggregate
+
+import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.aggregate.HashMapGenerator")
+class HashMapGeneratorDecorators {
+
+  @DecoratedMethod
+  protected final def genComputeHash(ctx: CodegenContext,
+                                     input: String,
+                                     dataType: DataType,
+                                     result: String): String = dataType match {
+    case ts.uInt64DataType => s"long $result = $input;"
+    case _ => __genComputeHash(ctx, input, dataType, result)
+  }
+
+
+  protected final def __genComputeHash(ctx: CodegenContext,
+                                       input: String,
+                                       dataType: DataType,
+                                       result: String): String = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,30 @@
+package org.apache.spark.sql.execution.columnar
+
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.columnar.ColumnBuilder$")
+object ColumnBuilderDecorators {
+
+  @DecoratedMethod
+  def apply(dataType: DataType,
+            initialSize: Int,
+            columnName: String,
+            useCompression: Boolean): ColumnBuilder = {
+    dataType match {
+      case ts.uInt64DataType =>
+        val builder = new LongColumnBuilder
+        builder.initialize(initialSize, columnName, useCompression)
+        builder
+      case _ => __apply(dataType, initialSize, columnName, useCompression)
+    }
+  }
+
+  def __apply(dataType: DataType,
+            initialSize: Int,
+            columnName: String,
+            useCompression: Boolean): ColumnBuilder = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessorDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessorDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,25 @@
+package org.apache.spark.sql.execution.columnar
+
+import org.apache.spark.sql.execution.columnar.GenerateColumnAccessorDecoratorsUtils.patchColumnTypes
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import org.apache.spark.sql.types.{DataType, LongType}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.columnar.GenerateColumnAccessor$")
+object GenerateColumnAccessorDecorators {
+
+  @DecoratedMethod
+  protected def create(columnTypes: Seq[DataType]): ColumnarIterator = {
+    __create(patchColumnTypes(columnTypes))
+  }
+
+  protected def __create(columnTypes: Seq[DataType]): ColumnarIterator = ???
+}
+
+object GenerateColumnAccessorDecoratorsUtils {
+  def patchColumnTypes(columnTypes: Seq[DataType]): Seq[DataType] = columnTypes.map {
+    case ts.uInt64DataType => LongType
+    case other => other
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteTaskStatsTrackerSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteTaskStatsTrackerSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,59 @@
+package org.apache.spark.sql.execution.datasources
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.spark.TaskContext
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.execution.metric.SQLMetric
+import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
+
+import scala.collection.mutable
+
+
+@Subclass
+@OriginClass("org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker")
+class BasicWriteTaskStatsTrackerSpyt(hadoopConf: Configuration, taskCommitTimeMetric: Option[SQLMetric] = None)
+  extends BasicWriteTaskStatsTracker(hadoopConf, taskCommitTimeMetric) {
+
+  private val submittedFiles = getSuperField("submittedFiles").asInstanceOf[mutable.HashSet[String]]
+  private val updateFileStatsS = this.getClass.getSuperclass.getDeclaredMethod("updateFileStats", classOf[String])
+
+  private def getSuperField(name: String): AnyRef = {
+    val field = this.getClass.getSuperclass.getDeclaredField(name)
+    field.setAccessible(true)
+    val result = field.get(this)
+    field.setAccessible(false)
+    result
+  }
+
+  override def closeFile(filePath: String): Unit = {
+    updateFileStats(filePath)
+    submittedFiles.remove(filePath)
+  }
+
+  private def updateFileStats(filePath: String): Unit = {
+    if (filePath.startsWith("ytTable:/")) return
+
+    updateFileStatsS.setAccessible(true)
+    updateFileStatsS.invoke(this, filePath).asInstanceOf[Option[Long]]
+    updateFileStatsS.setAccessible(false)
+  }
+
+  override def getFinalStats(taskCommitTime: Long): WriteTaskStats = {
+    submittedFiles.foreach(updateFileStats)
+    submittedFiles.clear()
+
+    val partitions = getSuperField("partitions").asInstanceOf[mutable.ArrayBuffer[InternalRow]]
+    val numFiles = getSuperField("numFiles").asInstanceOf[Int]
+    var numBytes = getSuperField("numBytes").asInstanceOf[Long]
+    val numRows = getSuperField("numRows").asInstanceOf[Long]
+
+    Option(TaskContext.get()).map(_.taskMetrics().outputMetrics).foreach { metrics =>
+      numBytes += metrics.bytesWritten
+      metrics.setBytesWritten(numBytes)
+      metrics.setRecordsWritten(numRows)
+    }
+
+    taskCommitTimeMetric.foreach(_ += taskCommitTime)
+    BasicWriteTaskStats(partitions, numFiles, numBytes, numRows)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartitionDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartitionDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,22 @@
+package org.apache.spark.sql.execution.datasources
+
+import org.apache.spark.sql.SparkSession
+import tech.ytsaurus.spyt.adapter.StorageSupport.{instance => ssi}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.datasources.FilePartition$")
+object FilePartitionDecorators {
+
+  @DecoratedMethod
+  def maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = {
+    if (ssi.shouldUseYtSplitFiles()) {
+      ssi.maxSplitBytes(sparkSession, selectedPartitions)
+    } else {
+      __maxSplitBytes(sparkSession, selectedPartitions)
+    }
+  }
+
+  def __maxSplitBytes(sparkSession: SparkSession, selectedPartitions: Seq[PartitionDirectory]): Long = ???
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioningDecorator.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioningDecorator.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,12 @@
+package org.apache.spark.sql.execution.datasources.v2
+
+import tech.ytsaurus.spyt.patch.annotations.{AddInterfaces, OriginClass}
+
+import java.io.Serializable
+
+
+@AddInterfaces(Array(classOf[Serializable]))
+@OriginClass("org.apache.spark.sql.execution.datasources.v2.DataSourcePartitioning")
+class DataSourcePartitioningDecorator {
+
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePythonDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePythonDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,31 @@
+package org.apache.spark.sql.execution.python
+
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.execution.python.EvaluatePython$")
+object EvaluatePythonDecorators {
+
+  @DecoratedMethod
+  def makeFromJava(dataType: DataType): Any => Any = dataType match {
+    case ts.uInt64DataType => EvaluatePythonUint64MakeFromJava
+    case other => __makeFromJava(other)
+  }
+
+  def __makeFromJava(dataType: DataType): Any => Any = ???
+}
+
+
+object EvaluatePythonUint64MakeFromJava extends (Any => Any) {
+
+  override def apply(input: Any): Any = input match {
+    case null => null
+    case c: Byte => c.toLong
+    case c: Short => c.toLong
+    case c: Int => c.toLong
+    case c: Long => c
+    case _ => null
+  }
+}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/internal/SharedStateDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/internal/SharedStateDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,20 @@
+package org.apache.spark.sql.internal
+
+import org.apache.spark.SparkConf
+import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
+import tech.ytsaurus.spyt.adapter.StorageSupport
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.internal.SharedState$")
+object SharedStateDecorators {
+  @DecoratedMethod
+  private[sql] def org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf: SparkConf): String = {
+    conf.get(CATALOG_IMPLEMENTATION) match {
+      case "in-memory" => StorageSupport.instance.ytsaurusExternalCatalogName
+      case _ => __org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf)
+    }
+  }
+
+  private[sql] def __org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(conf: SparkConf): String = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/types/DataTypeDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/types/DataTypeDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,17 @@
+package org.apache.spark.sql.types
+
+import tech.ytsaurus.spyt.adapter.TypeSupport
+import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.types.DataType$")
+object DataTypeDecorators {
+
+  @DecoratedMethod
+  private def nameToType(name: String): DataType = name match {
+    case "uint64" => TypeSupport.instance.uInt64DataType
+    case _ => __nameToType(name)
+  }
+
+  private def __nameToType(name: String): DataType = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,4 @@
+package org.apache.spark.sql.v2
+
+case class YtPartitionReaderFactory322(adapter: PartitionReaderFactoryAdapter)
+  extends YtPartitionReaderFactoryBase(adapter)
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,18 @@
+package tech.ytsaurus.spyt
+import org.apache.hadoop.fs.Path
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.format.{YtPartitionedFile322, YtPartitioningDelegate}
+import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
+
+@MinSparkVersion("3.2.2")
+class PartitionedFileAdapter322 extends PartitionedFileAdapter {
+  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
+                                     start: Long, length: Long): PartitionedFile = {
+    PartitionedFile(partitionValues, filePath, start, length)
+  }
+
+  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] = {
+    new YtPartitionedFile322(delegate)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/PartitionedFilePathAdapter322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/PartitionedFilePathAdapter322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,11 @@
+package tech.ytsaurus.spyt
+
+import org.apache.hadoop.fs.Path
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+
+@MinSparkVersion("3.2.2")
+class PartitionedFilePathAdapter322 extends PartitionedFilePathAdapter {
+  override def getStringFilePath(pf: PartitionedFile): String = pf.filePath
+
+  override def getHadoopFilePath(pf: PartitionedFile): Path = new Path(pf.filePath)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/ShuffleAdapter322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/ShuffleAdapter322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,11 @@
+package tech.ytsaurus.spyt
+
+import org.apache.spark.Partitioner
+import org.apache.spark.sql.execution.ShuffleSupport
+
+@MinSparkVersion("3.2.2")
+class ShuffleAdapter322 extends ShuffleAdapter {
+
+  override def createShufflePartitioner(numPartitions: Int): Partitioner =
+    ShuffleSupport.createShufflePartitioner(numPartitions)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/SparkAdapter322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/SparkAdapter322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,59 +0,0 @@
-package tech.ytsaurus.spyt
-
-import org.antlr.v4.runtime.ParserRuleContext
-import org.apache.log4j.spi.LoggingEvent
-import org.apache.log4j.{Category, Level}
-import org.apache.spark.executor.{ExecutorBackendFactory, ExecutorBackendFactory322}
-import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.catalyst.expressions.Expression
-import org.apache.spark.sql.catalyst.parser.ParserUtils
-import org.apache.spark.sql.connector.read.{InputPartition, ScanBuilder}
-import org.apache.spark.sql.connector.read.partitioning.{Distribution, Partitioning}
-import org.apache.spark.sql.execution.datasources.PartitionedFile
-import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, FileScanBuilder, PushDownUtils}
-import org.apache.spark.sql.sources.Filter
-import org.apache.spark.sql.v2.{ScanBuilderAdapter, YtScanBuilder322, YtScanPartitioning}
-import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
-import tech.ytsaurus.spyt.format.{YtPartitionedFile322, YtPartitioningDelegate}
-
-class SparkAdapter322 extends SparkAdapter {
-
-  override def minSparkVersion: String = "3.2.2"
-
-  override def createYtScanOutputPartitioning(nPartitions: Int): Partitioning = YtScanPartitioning(nPartitions)
-
-  override def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder = {
-    new YtScanBuilder322(scanBuilderAdapter)
-  }
-
-  override def executorBackendFactory: ExecutorBackendFactory = ExecutorBackendFactory322
-
-  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
-                                     start: Long, length: Long): PartitionedFile = {
-    PartitionedFile(partitionValues, filePath, start, length)
-  }
-
-  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] = {
-    new YtPartitionedFile322(delegate)
-  }
-
-  override def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T = {
-    ParserUtils.withOrigin(ctx)(f)
-  }
-
-  override def checkPushedFilters(scanBuilder: ScanBuilder,
-                                  filters: Seq[Expression],
-                                  expected: Seq[Filter]): (Seq[Any], Seq[Any]) = {
-    (PushDownUtils.pushFilters(scanBuilder, filters)._1, expected)
-  }
-
-  override def getInputPartition(dsrddPartition: DataSourceRDDPartition): InputPartition = {
-    dsrddPartition.inputPartition
-  }
-
-  override def createLoggingEvent(fqnOfCategoryClass: String, logger: Category,
-                                  timeStamp: Long, level: Level,
-                                  message: String, throwable: Throwable): LoggingEvent = {
-    new LoggingEvent(fqnOfCategoryClass, logger, timeStamp, level, message, throwable)
-  }
-}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/SparkAdapterBase322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/SparkAdapterBase322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,49 @@
+package tech.ytsaurus.spyt
+
+import org.antlr.v4.runtime.ParserRuleContext
+import org.apache.log4j.spi.LoggingEvent
+import org.apache.log4j.{Category, Level}
+import org.apache.spark.executor.{ExecutorBackendFactory, ExecutorBackendFactory322}
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.catalyst.parser.ParserUtils
+import org.apache.spark.sql.connector.read.{InputPartition, ScanBuilder}
+import org.apache.spark.sql.connector.read.partitioning.{Distribution, Partitioning}
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, FileScanBuilder, PushDownUtils}
+import org.apache.spark.sql.sources.Filter
+import org.apache.spark.sql.v2.{ScanBuilderAdapter, YtScanBuilder322, YtScanPartitioning}
+import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
+import tech.ytsaurus.spyt.format.{YtPartitionedFile322, YtPartitioningDelegate}
+
+@MinSparkVersion("3.2.2")
+class SparkAdapterBase322 extends SparkAdapterBase {
+
+  override def createYtScanOutputPartitioning(nPartitions: Int): Partitioning = YtScanPartitioning(nPartitions)
+
+  override def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder = {
+    new YtScanBuilder322(scanBuilderAdapter)
+  }
+
+  override def executorBackendFactory: ExecutorBackendFactory = ExecutorBackendFactory322
+
+  override def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T = {
+    ParserUtils.withOrigin(ctx)(f)
+  }
+
+  override def checkPushedFilters(scanBuilder: ScanBuilder,
+                                  filters: Seq[Expression],
+                                  expected: Seq[Filter]): (Seq[Any], Seq[Any]) = {
+    (PushDownUtils.pushFilters(scanBuilder, filters)._1, expected)
+  }
+
+  override def getInputPartition(dsrddPartition: DataSourceRDDPartition): InputPartition = {
+    dsrddPartition.inputPartition
+  }
+
+  override def createLoggingEvent(fqnOfCategoryClass: String, logger: Category,
+                                  timeStamp: Long, level: Level,
+                                  message: String, throwable: Throwable): LoggingEvent = {
+    new LoggingEvent(fqnOfCategoryClass, logger, timeStamp, level, message, throwable)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/YtPartitionReaderFactoryCreator322.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.2.2/src/main/scala/tech/ytsaurus/spyt/YtPartitionReaderFactoryCreator322.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,10 @@
+package tech.ytsaurus.spyt
+import org.apache.spark.sql.connector.read.PartitionReaderFactory
+import org.apache.spark.sql.v2.{PartitionReaderFactoryAdapter, YtPartitionReaderFactory322}
+
+@MinSparkVersion("3.2.2")
+class YtPartitionReaderFactoryCreator322 extends YtPartitionReaderFactoryCreator {
+
+  override def createYtPartitionReaderFactory(adapter: PartitionReaderFactoryAdapter): PartitionReaderFactory =
+    YtPartitionReaderFactory322(adapter)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.PartitionedFileAdapter330
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1 +0,0 @@
-tech.ytsaurus.spyt.SparkAdapter330
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapterBase	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.SparkAdapterBase	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.SparkAdapterBase330
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter330.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter330.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,20 @@
+package tech.ytsaurus.spyt
+
+import org.apache.hadoop.fs.Path
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.format.{YtPartitionedFile330, YtPartitioningDelegate}
+import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
+
+@MinSparkVersion("3.3.0")
+class PartitionedFileAdapter330 extends PartitionedFileAdapter {
+
+  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
+                                     start: Long, length: Long): PartitionedFile = {
+    PartitionedFile(partitionValues, filePath, start, length)
+  }
+
+  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] = {
+    new YtPartitionedFile330(delegate)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/SparkAdapter330.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/SparkAdapter330.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,67 +0,0 @@
-package tech.ytsaurus.spyt
-
-import org.antlr.v4.runtime.ParserRuleContext
-import org.apache.log4j.spi.LoggingEvent
-import org.apache.log4j.{Category, Level}
-import org.apache.spark.executor.{ExecutorBackendFactory, ExecutorBackendFactory330}
-import org.apache.spark.sql.catalyst.expressions.Expression
-import org.apache.spark.sql.connector.read.{InputPartition, ScanBuilder}
-import org.apache.spark.sql.connector.read.partitioning.{Partitioning, UnknownPartitioning}
-import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, FileScanBuilder, PushDownUtils}
-import org.apache.spark.sql.sources.Filter
-import org.apache.spark.sql.v2.{ScanBuilderAdapter, YtScanBuilder330}
-import org.apache.spark.sql.Utils330
-import org.apache.spark.sql.catalyst.InternalRow
-import org.apache.spark.sql.catalyst.parser.ParserUtils
-import org.apache.spark.sql.execution.datasources.PartitionedFile
-import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
-import tech.ytsaurus.spyt.format.{YtPartitionedFile330, YtPartitioningDelegate}
-
-class SparkAdapter330 extends SparkAdapter {
-
-  override def minSparkVersion: String = "3.3.0"
-
-  override def createYtScanOutputPartitioning(nPartitions: Int): Partitioning = {
-    // TODO consider using org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning
-    new UnknownPartitioning(nPartitions)
-  }
-
-  override def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder = {
-    new YtScanBuilder330(scanBuilderAdapter)
-  }
-
-  override def checkPushedFilters(scanBuilder: ScanBuilder,
-                                  filters: Seq[Expression],
-                                  expected: Seq[Filter]): (Seq[Any], Seq[Any]) = {
-    val filtersOrPredicates =  PushDownUtils.pushFilters(scanBuilder, filters)._1
-    filtersOrPredicates match {
-      case Left(filters) => (filters, expected)
-      case Right(predicates) => (predicates, expected.map(Utils330.filterToPredicate))
-    }
-  }
-
-  override def executorBackendFactory: ExecutorBackendFactory = ExecutorBackendFactory330
-
-  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
-                                     start: Long, length: Long): PartitionedFile = {
-    PartitionedFile(partitionValues, filePath, start, length)
-  }
-
-  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] = {
-    new YtPartitionedFile330(delegate)
-  }
-
-  override def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T = {
-    ParserUtils.withOrigin(ctx)(f)
-  }
-
-  override def getInputPartition(dsrddPartition: DataSourceRDDPartition): InputPartition = {
-    dsrddPartition.inputPartitions.head
-  }
-
-  override def createLoggingEvent(fqnOfCategoryClass: String, logger: Category,
-                                  timeStamp: Long, level: Level,
-                                  message: String, throwable: Throwable): LoggingEvent = {
-    Utils330.createLoggingEvent(fqnOfCategoryClass, logger, timeStamp, level, message, throwable)
-  }
-}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/SparkAdapterBase330.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.3.0/src/main/scala/tech/ytsaurus/spyt/SparkAdapterBase330.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,57 @@
+package tech.ytsaurus.spyt
+
+import org.antlr.v4.runtime.ParserRuleContext
+import org.apache.log4j.spi.LoggingEvent
+import org.apache.log4j.{Category, Level}
+import org.apache.spark.executor.{ExecutorBackendFactory, ExecutorBackendFactory330}
+import org.apache.spark.sql.catalyst.expressions.Expression
+import org.apache.spark.sql.connector.read.{InputPartition, ScanBuilder}
+import org.apache.spark.sql.connector.read.partitioning.{Partitioning, UnknownPartitioning}
+import org.apache.spark.sql.execution.datasources.v2.{DataSourceRDDPartition, FileScanBuilder, PushDownUtils}
+import org.apache.spark.sql.sources.Filter
+import org.apache.spark.sql.v2.{ScanBuilderAdapter, YtScanBuilder330}
+import org.apache.spark.sql.Utils330
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.parser.ParserUtils
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
+import tech.ytsaurus.spyt.format.{YtPartitionedFile330, YtPartitioningDelegate}
+
+@MinSparkVersion("3.3.0")
+class SparkAdapterBase330 extends SparkAdapterBase {
+
+  override def createYtScanOutputPartitioning(nPartitions: Int): Partitioning = {
+    // TODO consider using org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning
+    new UnknownPartitioning(nPartitions)
+  }
+
+  override def createYtScanBuilder(scanBuilderAdapter: ScanBuilderAdapter): FileScanBuilder = {
+    new YtScanBuilder330(scanBuilderAdapter)
+  }
+
+  override def checkPushedFilters(scanBuilder: ScanBuilder,
+                                  filters: Seq[Expression],
+                                  expected: Seq[Filter]): (Seq[Any], Seq[Any]) = {
+    val filtersOrPredicates =  PushDownUtils.pushFilters(scanBuilder, filters)._1
+    filtersOrPredicates match {
+      case Left(filters) => (filters, expected)
+      case Right(predicates) => (predicates, expected.map(Utils330.filterToPredicate))
+    }
+  }
+
+  override def executorBackendFactory: ExecutorBackendFactory = ExecutorBackendFactory330
+
+  override def parserUtilsWithOrigin[T](ctx: ParserRuleContext)(f: => T): T = {
+    ParserUtils.withOrigin(ctx)(f)
+  }
+
+  override def getInputPartition(dsrddPartition: DataSourceRDDPartition): InputPartition = {
+    dsrddPartition.inputPartitions.head
+  }
+
+  override def createLoggingEvent(fqnOfCategoryClass: String, logger: Category,
+                                  timeStamp: Long, level: Level,
+                                  message: String, throwable: Throwable): LoggingEvent = {
+    Utils330.createLoggingEvent(fqnOfCategoryClass, logger, timeStamp, level, message, throwable)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFileAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.PartitionedFileAdapter340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFilePathAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.PartitionedFilePathAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.PartitionedFilePathAdapter340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ResourcesAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ResourcesAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+org.apache.spark.resource.ResourcesAdapter340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ShuffleAdapter	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.ShuffleAdapter	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.ShuffleAdapter340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/resources/META-INF/services/tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.YtPartitionReaderFactoryCreator340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/ShuffleSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/ShuffleSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,6 @@
+package org.apache.spark
+
+object ShuffleSupport {
+  def createShufflePartitioner(numPartitions: Int): Partitioner =
+    new PartitionIdPassthrough(numPartitions)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/api/python/spyt/patch/SimplePythonFunction.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/api/python/spyt/patch/SimplePythonFunction.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,33 @@
+package org.apache.spark.api.python.spyt.patch
+
+import java.util.{List => JList, Map => JMap}
+import org.apache.spark.api.python.{PythonAccumulatorV2, PythonBroadcast, PythonFunction}
+import org.apache.spark.broadcast.Broadcast
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, OriginClass}
+
+@OriginClass("org.apache.spark.api.python.SimplePythonFunction")
+@Applicability(from = "3.4.0")
+private[spark] case class SimplePythonFunction(
+    command: Seq[Byte],
+    envVars: JMap[String, String],
+    pythonIncludes: JList[String],
+    private val _pythonExec: String,
+    pythonVer: String,
+    broadcastVars: JList[Broadcast[PythonBroadcast]],
+    accumulator: PythonAccumulatorV2) extends PythonFunction {
+
+  def this(
+      command: Array[Byte],
+      envVars: JMap[String, String],
+      pythonIncludes: JList[String],
+      _pythonExec: String,
+      pythonVer: String,
+      broadcastVars: JList[Broadcast[PythonBroadcast]],
+      accumulator: PythonAccumulatorV2) = {
+    this(command.toSeq, envVars, pythonIncludes, _pythonExec, pythonVer, broadcastVars, accumulator)
+  }
+
+  def pythonExec: String = {
+    sys.env.getOrElse("PYSPARK_EXECUTOR_PYTHON", _pythonExec)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/resource/ResourcesAdapter340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/resource/ResourcesAdapter340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,12 @@
+package org.apache.spark.resource
+
+import org.apache.spark.resource.ResourceProfile.ExecutorResourcesOrDefaults
+import tech.ytsaurus.spyt.{MinSparkVersion, ResourcesAdapter}
+
+@MinSparkVersion("3.4.0")
+class ResourcesAdapter340 extends ResourcesAdapter {
+
+  override def getExecutorCores(execResources: Product): Int = {
+    execResources.asInstanceOf[ExecutorResourcesOrDefaults].cores.getOrElse(1)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectionDecorators340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,47 @@
+package org.apache.spark.sql.catalyst
+
+import org.apache.spark.sql.catalyst.ScalaReflection.universe.Type
+import org.apache.spark.sql.catalyst.encoders.AgnosticEncoders.BoxedLongEncoder
+import org.apache.spark.sql.catalyst.encoders.{AgnosticEncoder, UInt64Encoder}
+import org.apache.spark.sql.catalyst.expressions.Expression
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
+import tech.ytsaurus.spyt.types.UInt64Long
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.ScalaReflection$")
+@Applicability(from = "3.4.0")
+object ScalaReflectionDecorators340 {
+
+  @DecoratedMethod
+  private def serializerFor(enc: AgnosticEncoder[_], input: Expression): Expression = enc match {
+    case UInt64Encoder => ts.uInt64Serializer(input)
+    case _ => __serializerFor(enc, input)
+  }
+
+  private def __serializerFor(enc: AgnosticEncoder[_], input: Expression): Expression = ???
+
+  @DecoratedMethod
+  private def deserializerFor(enc: AgnosticEncoder[_],
+                              path: Expression,
+                              walkedTypePath: WalkedTypePath): Expression = enc match {
+    case UInt64Encoder => ts.uInt64Deserializer(path)
+    case _ => __deserializerFor(enc, path, walkedTypePath)
+  }
+
+  private def __deserializerFor(enc: AgnosticEncoder[_],
+                                path: Expression,
+                                walkedTypePath: WalkedTypePath): Expression = ???
+
+  @DecoratedMethod
+  private def encoderFor(tpe: `Type`, seenTypeSet: Set[`Type`], path: WalkedTypePath): AgnosticEncoder[_] =
+    baseType(tpe) match {
+      case t if isSubtype(t, ScalaReflection.localTypeOf[UInt64Long]) => UInt64Encoder
+      case _ =>  __encoderFor(tpe, seenTypeSet, path)
+    }
+
+  private def __encoderFor(tpe: `Type`, seenTypeSet: Set[`Type`], path: WalkedTypePath): AgnosticEncoder[_] = ???
+
+  private def baseType(tpe: `Type`): `Type` = ???
+  private[catalyst] def isSubtype(tpe1: `Type`, tpe2: `Type`): Boolean = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,19 @@
+package org.apache.spark.sql.catalyst.encoders
+
+import org.apache.spark.sql.types.DataType
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, DecoratedMethod, OriginClass}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.encoders.RowEncoder$")
+@Applicability(from = "3.4.0")
+object RowEncoderDecorators340 {
+
+  @DecoratedMethod
+  private[catalyst] def encoderForDataType(dataType: DataType, lenient: Boolean): AgnosticEncoder[_] = dataType match {
+    case ts.uInt64DataType => UInt64Encoder
+    case _ => __encoderForDataType(dataType, lenient)
+  }
+
+  private[catalyst] def __encoderForDataType(dataType: DataType, lenient: Boolean): AgnosticEncoder[_] = ???
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/encoders/UInt64Encoder.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/encoders/UInt64Encoder.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,7 @@
+package org.apache.spark.sql.catalyst.encoders
+
+import org.apache.spark.sql.catalyst.encoders.AgnosticEncoders.PrimitiveLeafEncoder
+import tech.ytsaurus.spyt.adapter.TypeSupport.{instance => ts}
+import tech.ytsaurus.spyt.types.UInt64Long
+
+case object UInt64Encoder extends PrimitiveLeafEncoder[UInt64Long](ts.uInt64DataType)
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/catalyst/expressions/CastDecorators340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,9 @@
+package org.apache.spark.sql.catalyst.expressions
+
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, Decorate, OriginClass, PatchSource}
+
+@Decorate
+@OriginClass("org.apache.spark.sql.catalyst.expressions.Cast")
+@Applicability(from = "3.4.0")
+@PatchSource("org.apache.spark.sql.catalyst.expressions.CastBaseDecorators")
+class CastDecorators340
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/types/IntegralTypeSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/types/IntegralTypeSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,15 @@
+package org.apache.spark.sql.types
+
+import org.apache.spark.sql.catalyst.types.{PhysicalDataType, PhysicalLongType}
+import tech.ytsaurus.spyt.patch.annotations.{Applicability, OriginClass, Subclass}
+
+@Subclass
+@OriginClass("org.apache.spark.sql.types.IntegralType")
+@Applicability(from = "3.4.0")
+private[sql] abstract class IntegralTypeSpyt extends IntegralType {
+  override private[sql] def physicalDataType: PhysicalDataType = {
+    // We're hooking to uint64 here instead of UInt64Type because this method was introduced in Spark 3.4.0 to
+    // preserve backward compatibility with older Spark versions
+    if (catalogString == "uint64") PhysicalLongType else super.physicalDataType
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/org/apache/spark/sql/v2/YtPartitionReaderFactory340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,10 @@
+package org.apache.spark.sql.v2
+
+import org.apache.spark.sql.catalyst.FileSourceOptions
+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
+
+case class YtPartitionReaderFactory340(adapter: PartitionReaderFactoryAdapter)
+  extends YtPartitionReaderFactoryBase(adapter) {
+  protected override def options: FileSourceOptions =
+    new FileSourceOptions(CaseInsensitiveMap[String](adapter.options))
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFileAdapter340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,20 @@
+package tech.ytsaurus.spyt
+import org.apache.hadoop.fs.Path
+import org.apache.spark.paths.SparkPath
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+import tech.ytsaurus.spyt.format.{YtPartitionedFile340, YtPartitioningDelegate}
+import tech.ytsaurus.spyt.format.YtPartitioningSupport.YtPartitionedFileBase
+
+@MinSparkVersion("3.4.0")
+class PartitionedFileAdapter340 extends PartitionedFileAdapter {
+
+  override def createPartitionedFile(partitionValues: InternalRow, filePath: String,
+                                     start: Long, length: Long): PartitionedFile = {
+    PartitionedFile(partitionValues, SparkPath.fromUrlString(filePath), start, length)
+  }
+
+  override def createYtPartitionedFile[T <: YtPartitioningDelegate](delegate: T): YtPartitionedFileBase[T] = {
+    new YtPartitionedFile340[T](delegate)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFilePathAdapter340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/PartitionedFilePathAdapter340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,11 @@
+package tech.ytsaurus.spyt
+
+import org.apache.hadoop.fs.Path
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+
+@MinSparkVersion("3.4.0")
+class PartitionedFilePathAdapter340 extends PartitionedFilePathAdapter {
+  override def getStringFilePath(pf: PartitionedFile): String = pf.filePath.urlEncoded
+
+  override def getHadoopFilePath(pf: PartitionedFile): Path = pf.filePath.toPath
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/ShuffleAdapter340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/ShuffleAdapter340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,10 @@
+package tech.ytsaurus.spyt
+
+import org.apache.spark.{Partitioner, ShuffleSupport}
+
+@MinSparkVersion("3.4.0")
+class ShuffleAdapter340 extends ShuffleAdapter {
+
+  override def createShufflePartitioner(numPartitions: Int): Partitioner =
+    ShuffleSupport.createShufflePartitioner(numPartitions)
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/YtPartitionReaderFactoryCreator340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/YtPartitionReaderFactoryCreator340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,11 @@
+package tech.ytsaurus.spyt
+import org.apache.spark.sql.connector.read.PartitionReaderFactory
+import org.apache.spark.sql.v2.{PartitionReaderFactoryAdapter, YtPartitionReaderFactory340}
+
+@MinSparkVersion("3.4.0")
+class YtPartitionReaderFactoryCreator340 extends YtPartitionReaderFactoryCreator {
+
+  override def createYtPartitionReaderFactory(adapter: PartitionReaderFactoryAdapter): PartitionReaderFactory = {
+    YtPartitionReaderFactory340(adapter)
+  }
+}
--- yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/format/YtPartitionedFile340.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-adapter/impl/spark-3.4.0/src/main/scala/tech/ytsaurus/spyt/format/YtPartitionedFile340.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,8 @@
+package tech.ytsaurus.spyt.format
+
+import org.apache.spark.paths.SparkPath
+import org.apache.spark.sql.execution.datasources.PartitionedFile
+
+class YtPartitionedFile340[T <: YtPartitioningDelegate](override val delegate: T)
+  extends PartitionedFile(delegate.partitionValues, SparkPath.fromUrlString(delegate.filePath),
+    delegate.start, delegate.byteLength) with YtPartitioningSupport[T]
--- yt/spark/spark-over-yt/spark-cluster/src/main/resources/META-INF/services/org.apache.spark.deploy.rest.RestSubmitSupport	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/resources/META-INF/services/org.apache.spark.deploy.rest.RestSubmitSupport	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+org.apache.spark.deploy.rest.YTsaurusRestSubmitSupport
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-cluster/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.ClusterSupport	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/resources/META-INF/services/tech.ytsaurus.spyt.adapter.ClusterSupport	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+tech.ytsaurus.spyt.adapter.YTsaurusClusterSupport
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/PythonRunnerDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/PythonRunnerDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,19 +0,0 @@
-package org.apache.spark.deploy
-
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.PythonRunner")
-object PythonRunnerDecorators {
-
-  @DecoratedMethod
-  def main(args: Array[String]): Unit = {
-    val redirectToStderr = System.getProperty("spark.ytsaurus.redirectToStderr", "false").toBoolean
-    if (redirectToStderr) {
-      System.setOut(System.err)
-    }
-    __main(args)
-  }
-
-  def __main(args: Array[String]): Unit = ???
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/FsHistoryProviderDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/FsHistoryProviderDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,36 +0,0 @@
-package org.apache.spark.deploy.history
-
-import org.apache.hadoop.fs.{FileSystem, Path}
-import org.apache.spark.SparkConf
-import org.apache.spark.deploy.history.YtHistoryServer.Config.CREATE_LOG_DIR
-import tech.ytsaurus.spyt.SparkVersionUtils
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import java.util
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.history.FsHistoryProvider")
-class FsHistoryProviderDecorators {
-
-  @DecoratedMethod
-  private def startPolling(): Unit = {
-    val path = new Path(logDir)
-    val confFieldName = if (SparkVersionUtils.lessThan("3.3.0")) {
-      "conf"
-    } else {
-      "org$apache$spark$deploy$history$FsHistoryProvider$$conf"
-    }
-
-    val conf: SparkConf = this.getClass.getDeclaredField(confFieldName).get(this).asInstanceOf[SparkConf]
-
-    if (!fs.exists(path) && conf.get(CREATE_LOG_DIR)) {
-      fs.mkdirs(path)
-    }
-
-    __startPolling()
-  }
-
-  private def __startPolling(): Unit = ???
-  private val logDir: String = ???
-  private[history] val fs: FileSystem = ???
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/RollingEventLogFilesWriterSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/RollingEventLogFilesWriterSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,23 +0,0 @@
-package org.apache.spark.deploy.history
-
-import org.apache.hadoop.conf.Configuration
-import org.apache.spark.SparkConf
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-import java.net.URI
-
-@Subclass
-@OriginClass("org.apache.spark.deploy.history.RollingEventLogFilesWriter")
-class RollingEventLogFilesWriterSpyt(
-    appId: String,
-    appAttemptId : Option[String],
-    logBaseDir: URI,
-    sparkConf: SparkConf,
-    hadoopConf: Configuration)
-  extends RollingEventLogFilesWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {
-
-  override def stop(): Unit = {
-    super.stop()
-    fileSystem.close()
-  }
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/SingleEventLogFileWriterSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/SingleEventLogFileWriterSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,23 +0,0 @@
-package org.apache.spark.deploy.history
-
-import org.apache.hadoop.conf.Configuration
-import org.apache.spark.SparkConf
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-import java.net.URI
-
-@Subclass
-@OriginClass("org.apache.spark.deploy.history.SingleEventLogFileWriter")
-class SingleEventLogFileWriterSpyt(
-    appId: String,
-    appAttemptId : Option[String],
-    logBaseDir: URI,
-    sparkConf: SparkConf,
-    hadoopConf: Configuration)
-  extends SingleEventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) {
-
-  override def stop(): Unit = {
-    super.stop()
-    fileSystem.close()
-  }
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/YtHistoryServer.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/history/YtHistoryServer.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -67,11 +67,4 @@ object YtHistoryServer extends Logging {
     // Wait until the end of the world... or if the HistoryServer process is manually stopped
     while(true) { Thread.sleep(Int.MaxValue) }
   }
-
-  object Config {
-    val CREATE_LOG_DIR = ConfigBuilder("spark.history.fs.createLogDirectory")
-      .version("3.0.1")
-      .booleanConf
-      .createWithDefault(false)
-  }
-}
\ No newline at end of file
+}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/master/MasterDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/master/MasterDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,64 +0,0 @@
-package org.apache.spark.deploy.master
-
-import org.apache.spark.deploy.DriverDescription
-import org.apache.spark.deploy.master.DriverState.DriverState
-import org.apache.spark.internal.Logging
-import org.apache.spark.rpc.RpcEndpointRef
-import tech.ytsaurus.spyt.launcher.DeployMessages.{RegisterDriverToAppId, UnregisterDriverToAppId}
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import scala.collection.mutable
-import scala.collection.mutable.ArrayBuffer
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.master.Master")
-class MasterDecorators {
-
-  @DecoratedMethod
-  def org$apache$spark$deploy$master$Master$$removeDriver(
-    driverId: String,
-    finalState: DriverState,
-    exception: Option[Exception]): Unit = {
-    val completedDrivers = org$apache$spark$deploy$master$Master$$completedDrivers
-    val before = new ArrayBuffer[DriverInfo]
-    completedDrivers.copyToBuffer(before)
-
-    __org$apache$spark$deploy$master$Master$$removeDriver(driverId, finalState, exception)
-
-    MasterDecorators.checkAndRemoveDriverToAppId(self, before, completedDrivers)
-  }
-
-  def __org$apache$spark$deploy$master$Master$$removeDriver(
-    driverId: String,
-    finalState: DriverState,
-    exception: Option[Exception]): Unit = ???
-
-  @DecoratedMethod
-  def org$apache$spark$deploy$master$Master$$createDriver(desc: DriverDescription): DriverInfo = {
-    val dInfo = __org$apache$spark$deploy$master$Master$$createDriver(desc)
-    val newDesc = dInfo.desc.copy(command =
-      desc.command.copy(javaOpts = s"-Dspark.driverId=${dInfo.id}" +: desc.command.javaOpts))
-    new DriverInfo(dInfo.startTime, dInfo.id, newDesc, dInfo.submitDate)
-  }
-
-  def __org$apache$spark$deploy$master$Master$$createDriver(desc: DriverDescription): DriverInfo = ???
-
-  final def self: RpcEndpointRef = ???
-
-  private val org$apache$spark$deploy$master$Master$$completedDrivers: mutable.ArrayBuffer[DriverInfo] = ???
-}
-
-object MasterDecorators extends Logging {
-  def checkAndRemoveDriverToAppId(
-    self: RpcEndpointRef,
-    completedDriversBefore: mutable.ArrayBuffer[DriverInfo],
-    completedDriversAfter: mutable.ArrayBuffer[DriverInfo]): Unit = {
-    var i = 0
-    while (i < completedDriversBefore.length && completedDriversBefore(i).id != completedDriversAfter.head.id) {
-      val driverId = completedDriversBefore(i).id
-      logInfo(s"Requesting to unregister driverId $driverId to app")
-      self.send(UnregisterDriverToAppId(driverId))
-      i += 1
-    }
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClientAppSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClientAppSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,138 +0,0 @@
-package org.apache.spark.deploy.rest
-
-import org.apache.commons.io.FileUtils
-import org.apache.hadoop.fs.FileSystem
-import org.apache.spark.deploy.SparkHadoopUtil
-import org.apache.spark.internal.Logging
-import org.apache.spark.{SparkConf, SparkException}
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-import java.io.File
-import java.net.URI
-import scala.annotation.tailrec
-import scala.concurrent.duration._
-import scala.util.{Failure, Random, Success, Try}
-
-@Subclass
-@OriginClass("org.apache.spark.deploy.rest.RestSubmissionClientApp")
-class RestSubmissionClientAppSpyt extends RestSubmissionClientApp with Logging {
-
-  override def run(appResource: String,
-                   mainClass: String,
-                   appArgs: Array[String],
-                   conf: SparkConf,
-                   env: Map[String, String]): SubmitRestProtocolResponse = {
-    // Almost exact copy of super.run(...) method, with spark.rest.master property instead of spark.master
-    val master = conf.getOption("spark.rest.master").getOrElse {
-      throw new IllegalArgumentException("'spark.rest.master' must be set.")
-    }
-    val sparkProperties = conf.getAll.toMap
-    val client = new RestSubmissionClientSpyt(master)
-    val submitRequest = client.constructSubmitRequest(
-      appResource, mainClass, appArgs, sparkProperties, env)
-    client.createSubmission(submitRequest)
-  }
-
-  @tailrec
-  private def getSubmissionStatus(submissionId: String,
-                                  client: RestSubmissionClient,
-                                  retry: Int,
-                                  retryInterval: Duration,
-                                  rnd: Random = new Random): SubmissionStatusResponse = {
-    val response = Try(client.requestSubmissionStatus(submissionId)
-      .asInstanceOf[SubmissionStatusResponse])
-    response match {
-      case Success(value) => value
-      case Failure(exception) if retry > 0 =>
-        log.error(s"Exception while getting submission status: ${exception.getMessage}")
-        val sleepInterval = if (retryInterval > 1.second) {
-          1000 + rnd.nextInt(retryInterval.toMillis.toInt - 1000)
-        } else rnd.nextInt(retryInterval.toMillis.toInt)
-        Thread.sleep(sleepInterval)
-        getSubmissionStatus(submissionId, client, retry - 1, retryInterval, rnd)
-      case Failure(exception) => throw exception
-    }
-  }
-
-  def awaitAppTermination(submissionId: String,
-                          conf: SparkConf,
-                          checkStatusInterval: Duration): Unit = {
-    import org.apache.spark.deploy.master.DriverState._
-
-    val master = conf.getOption("spark.rest.master").getOrElse {
-      throw new IllegalArgumentException("'spark.rest.master' must be set.")
-    }
-    val client = new RestSubmissionClient(master)
-    val runningStates = Set(RUNNING.toString, SUBMITTED.toString)
-    val finalStatus = Stream.continually {
-      Thread.sleep(checkStatusInterval.toMillis)
-      val response = getSubmissionStatus(submissionId, client, retry = 3, checkStatusInterval)
-      logInfo(s"Driver report for $submissionId (state: ${response.driverState})")
-      response
-    }.find(response => !runningStates.contains(response.driverState)).get
-    logInfo(s"Driver $submissionId finished with status ${finalStatus.driverState}")
-    finalStatus.driverState match {
-      case s if s == FINISHED.toString => // success
-      case s if s == FAILED.toString =>
-        throw new SparkException(s"Driver $submissionId failed")
-      case _ =>
-        throw new SparkException(s"Driver $submissionId failed with unexpected error")
-    }
-  }
-
-  def shutdownYtClient(sparkConf: SparkConf): Unit = {
-    val hadoopConf = SparkHadoopUtil.newConfiguration(sparkConf)
-    val fs = FileSystem.get(new URI("yt:///"), hadoopConf)
-    fs.close()
-  }
-
-  private def writeToFile(file: File, message: String): Unit = {
-    val tmpFile = new File(file.getParentFile, s"${file.getName}_tmp")
-    FileUtils.writeStringToFile(tmpFile, message)
-    FileUtils.moveFile(tmpFile, file)
-  }
-
-  override def start(args: Array[String], conf: SparkConf): Unit = {
-    val submissionIdFile = conf.getOption("spark.rest.client.submissionIdFile").map(new File(_))
-    val submissionErrorFile = conf.getOption("spark.rest.client.submissionErrorFile")
-      .map(new File(_))
-    try {
-      // Here starts an almost exact copy of super.start(...) method
-      if (args.length < 2) {
-        sys.error("Usage: RestSubmissionClient [app resource] [main class] [app args*]")
-        sys.exit(1)
-      }
-      val appResource = args(0)
-      val mainClass = args(1)
-      val appArgs = args.slice(2, args.length)
-      val confEnv = conf.getAll.filter {
-        case (key, _) => key.startsWith("spark.yt") || key.startsWith("spark.hadoop.yt")
-      }.map {
-        case (key, value) => key.toUpperCase().replace(".", "_") -> value
-      }.toMap
-      val env = RestSubmissionClient.filterSystemEnvironment(sys.env) ++ confEnv
-
-      val submissionId = try {
-        val response = run(appResource, mainClass, appArgs, conf, env)
-        response match {
-          case r: CreateSubmissionResponse => r.submissionId
-          case _ => throw new IllegalStateException("Job is not submitted")
-        }
-      } finally {
-        shutdownYtClient(conf)
-      }
-
-      submissionIdFile.foreach(writeToFile(_, submissionId))
-
-      if (conf.getOption("spark.rest.client.awaitTermination.enabled").forall(_.toBoolean)) {
-        val checkStatusInterval = conf.getOption("spark.rest.client.statusInterval")
-          .map(_.toInt.seconds).getOrElse(5.seconds)
-        awaitAppTermination(submissionId, conf, checkStatusInterval)
-      }
-    } catch {
-      case e: Throwable =>
-        submissionErrorFile.foreach(writeToFile(_, e.getMessage))
-        throw e
-    }
-  }
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/YTsaurusRestSubmitSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/YTsaurusRestSubmitSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,36 @@
+package org.apache.spark.deploy.rest
+
+import org.apache.spark.SparkConf
+import org.apache.spark.rpc.RpcEndpointRef
+
+class YTsaurusRestSubmitSupport extends RestSubmitSupport {
+
+  override def createSubmission(appResource: String,
+                                mainClass: String,
+                                appArgs: Array[String],
+                                conf: SparkConf,
+                                env: Map[String, String],
+                                master: String): SubmitRestProtocolResponse = {
+    val sparkProperties = conf.getAll.toMap
+    val client = new RestSubmissionClientSpyt(master)
+    val submitRequest = client.constructSubmitRequest(
+      appResource, mainClass, appArgs, sparkProperties, env)
+    client.createSubmission(submitRequest)
+  }
+
+  override def statusRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): StatusRequestServlet = {
+    new YtStatusRequestServlet(masterEndpoint, masterConf)
+  }
+
+  override def masterStateRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet = {
+    new StandaloneMasterStateRequestServlet(masterEndpoint, masterConf)
+  }
+
+  override def appIdRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet = {
+    new StandaloneAppIdRequestServlet(masterEndpoint, masterConf)
+  }
+
+  override def appStatusRequestServlet(masterEndpoint: RpcEndpointRef, masterConf: SparkConf): RestServlet = {
+    new StandaloneAppStatusRequestServlet(masterEndpoint, masterConf)
+  }
+}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/spyt/patch/StandaloneRestServer.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/rest/spyt/patch/StandaloneRestServer.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,44 +0,0 @@
-package org.apache.spark.deploy.rest.spyt.patch
-
-import org.apache.spark.SparkConf
-import org.apache.spark.deploy.rest._
-import org.apache.spark.rpc.RpcEndpointRef
-import tech.ytsaurus.spyt.patch.annotations.OriginClass
-
-/**
- * Patches:
- * 1. Set `host` parameter to null in superclass constructor. Main reason: we need to bind RPC endpoint to wildcard
- *    network interface for Kubernetes deployments with host network.
- */
-@OriginClass("org.apache.spark.deploy.rest.StandaloneRestServer")
-private[deploy] class StandaloneRestServer(host: String,
-                                           requestedPort: Int,
-                                           masterConf: SparkConf,
-                                           masterEndpoint: RpcEndpointRef,
-                                           masterUrl: String)
-  extends RestSubmissionServer(null, requestedPort, masterConf) {
-
-  protected override val submitRequestServlet =
-    new StandaloneSubmitRequestServlet(masterEndpoint, masterUrl, masterConf)
-  protected override val killRequestServlet =
-    new StandaloneKillRequestServlet(masterEndpoint, masterConf)
-  protected override val statusRequestServlet =
-    new YtStatusRequestServlet(masterEndpoint, masterConf)
-
-  protected val masterStateRequestServlet =
-    new StandaloneMasterStateRequestServlet(masterEndpoint, masterConf)
-  protected val appIdRequestServlet =
-    new StandaloneAppIdRequestServlet(masterEndpoint, masterConf)
-  protected val appStatusRequestServlet =
-    new StandaloneAppStatusRequestServlet(masterEndpoint, masterConf)
-
-  protected override lazy val contextToServlet: Map[String, RestServlet] = Map(
-    s"$baseContext/create/*" -> submitRequestServlet,
-    s"$baseContext/kill/*" -> killRequestServlet,
-    s"$baseContext/status/*" -> statusRequestServlet,
-    s"$baseContext/master/*" -> masterStateRequestServlet,
-    s"$baseContext/getAppId/*" -> appIdRequestServlet,
-    s"$baseContext/getAppStatus/*" -> appStatusRequestServlet,
-    "/*" -> new ErrorServlet // default handler
-  )
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/DriverRunnerDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/DriverRunnerDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,69 +0,0 @@
-package org.apache.spark.deploy.worker
-
-import org.apache.spark.SparkConf
-import org.apache.spark.deploy.{DriverDescription, SparkHadoopUtil}
-import org.apache.spark.deploy.worker.DriverRunnerDecorators.addLocalPyFiles
-import org.apache.spark.internal.Logging
-import org.apache.spark.internal.config.SUBMIT_PYTHON_FILES
-import org.apache.spark.util.Utils
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import java.io.{File, IOException}
-import java.net.URI
-import java.util.stream.Collectors
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.worker.DriverRunner")
-class DriverRunnerDecorators {
-
-  @DecoratedMethod
-  private def runDriver(builder: ProcessBuilder, baseDir: File, supervise: Boolean): Int = {
-    val conf = this.getClass.getDeclaredField("conf").get(this).asInstanceOf[SparkConf]
-    val driverDesc = this.getClass.getDeclaredField("driverDesc").get(this).asInstanceOf[DriverDescription]
-    val localPyFiles = DriverRunnerDecorators.downloadPythonFiles(driverDesc, baseDir, conf)
-
-    builder.command(addLocalPyFiles(builder.command(), localPyFiles))
-
-    __runDriver(builder, baseDir, supervise)
-  }
-
-  private def __runDriver(builder: ProcessBuilder, baseDir: File, supervise: Boolean): Int = ???
-}
-
-object DriverRunnerDecorators extends Logging {
-  def downloadPythonFiles(driverDesc: DriverDescription, driverDir: File, conf: SparkConf): Seq[String] = {
-    val pyFiles = driverDesc.command.javaOpts.flatMap {
-      case opt if opt.trim.startsWith(s"-D${SUBMIT_PYTHON_FILES.key}=") =>
-        Utils.stringToSeq(opt.substring(opt.indexOf("=") + 1))
-      case _ => Nil
-    }
-    logInfo(s"Found python dependencies: ${pyFiles}")
-    pyFiles.map { pyFileUrl =>
-      // almost exact copy of a DriverRunner.downloadUserJar method
-      val fileName = new URI(pyFileUrl).getPath.split("/").last
-      val localFile = new File(driverDir, fileName)
-      if (!localFile.exists()) {
-        logInfo(s"Copying supplied python file $pyFileUrl to $localFile")
-        Utils.fetchFile(
-          pyFileUrl,
-          driverDir,
-          conf,
-          SparkHadoopUtil.get.newConfiguration(conf),
-          System.currentTimeMillis(),
-          useCache = false
-        )
-        if (!localFile.exists()) { // Verify copy succeeded
-          throw new IOException(s"Can not find expected file $fileName which should have been loaded in $driverDir")
-        }
-      }
-      localFile.getAbsolutePath
-    }
-  }
-
-  def addLocalPyFiles(command: java.util.List[String], localPyFiles: Seq[String]): java.util.List[String] = {
-    command.stream().map {
-      case "{{PY_FILES}}" => localPyFiles.mkString(",")
-      case other => other
-    }.collect(Collectors.toList()).asInstanceOf[java.util.List[String]]
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/DriverWrapperDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/DriverWrapperDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,27 +0,0 @@
-package org.apache.spark.deploy.worker
-
-import org.apache.hadoop.fs.FileSystem
-import org.apache.spark.SparkConf
-import org.apache.spark.deploy.SparkHadoopUtil
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import java.net.URI
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.worker.DriverWrapper")
-object DriverWrapperDecorators {
-
-  @DecoratedMethod
-  def main(args: Array[String]): Unit = {
-    val conf = new SparkConf()
-    val hadoopConf = SparkHadoopUtil.newConfiguration(conf)
-    try {
-      __main(args)
-    } finally {
-      FileSystem.get(new URI("yt:///"), hadoopConf).close()
-    }
-  }
-
-  def __main(args: Array[String]): Unit = ???
-
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/ui/LogPageSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/deploy/worker/ui/LogPageSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,31 +0,0 @@
-package org.apache.spark.deploy.worker.ui
-
-import org.apache.spark.ui.UIUtils
-import tech.ytsaurus.spyt.patch.annotations.{OriginClass, Subclass}
-
-import javax.servlet.http.HttpServletRequest
-import scala.xml.{Elem, Node}
-
-/**
- * This is a fix for a bug in spark 3.2.2 which doesn't include utils.js file on logPage, but uses a getBaseURI
- * function from it.
- */
-@Subclass
-@OriginClass("org.apache.spark.deploy.worker.ui.LogPage")
-class LogPageSpyt(parent: WorkerWebUI) extends LogPage(parent) {
-
-  override def render(request: HttpServletRequest): Seq[Node] = {
-    val content = super.render(request)
-    val utilsJs = <script src={UIUtils.prependBaseUri(request, "/static/utils.js")}></script>
-    val html = content.head
-    val head = html.child.find {
-      case e: Elem if e.label == "head" => true
-      case _ => false
-    }.get
-    val body = html.child.find {
-      case e: Elem if e.label == "body" => true
-      case _ => false
-    }.get
-    <html><head>{head.child ++ utilsJs}</head>{body}</html>
-  }
-}
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackendDecorators.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackendDecorators.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,38 +0,0 @@
-package org.apache.spark.scheduler.cluster
-
-import org.apache.spark.{SparkConf, SparkContext}
-import org.apache.spark.deploy.master.YtMaster
-import org.apache.spark.rpc.RpcAddress
-import org.apache.spark.scheduler.TaskSchedulerImpl
-import tech.ytsaurus.spyt.launcher.DeployMessages.RegisterDriverToAppId
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-@Decorate
-@OriginClass("org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend")
-private[spark] class StandaloneSchedulerBackendDecorators {
-
-  @DecoratedMethod
-  def connected(appId: String): Unit = {
-    __connected(appId)
-    val sc = this.getClass.getDeclaredField("org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$sc")
-      .get(this).asInstanceOf[SparkContext]
-    val masters = this.getClass.getDeclaredField("masters").get(this).asInstanceOf[Array[String]]
-
-    StandaloneSchedulerBackendDecorators.registerDriver(sc, masters, conf, appId)
-  }
-
-  def __connected(appId: String): Unit = ???
-  protected val conf: SparkConf = ???
-}
-
-private[spark] object StandaloneSchedulerBackendDecorators {
-  def registerDriver(sc: SparkContext, masters: Array[String], conf: SparkConf, appId: String): Unit = {
-    conf.getOption("spark.driverId").foreach { driverId =>
-      val msg = RegisterDriverToAppId(driverId, appId)
-      masters.foreach { masterUrl =>
-        val masterAddress = RpcAddress.fromSparkURL(masterUrl)
-        sc.env.rpcEnv.setupEndpointRef(masterAddress, YtMaster.ENDPOINT_NAME).send(msg)
-      }
-    }
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/tech/ytsaurus/spark/launcher/SparkLauncher.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/tech/ytsaurus/spark/launcher/SparkLauncher.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -12,7 +12,7 @@ import tech.ytsaurus.spyt.wrapper.YtWrapper
 import tech.ytsaurus.spyt.wrapper.client.YtClientConfiguration
 import tech.ytsaurus.spyt.wrapper.discovery.{Address, CompoundDiscoveryService, CypressDiscoveryService, DiscoveryServerService, DiscoveryService}
 import tech.ytsaurus.client.CompoundClient
-import tech.ytsaurus.spyt.HostAndPort
+import tech.ytsaurus.spyt.{HostAndPort, SparkVersionUtils}
 
 import java.io.File
 import java.nio.file.{Files, Path}
@@ -83,10 +83,13 @@ trait SparkLauncher {
 
   private def getLivyClientSparkConf(): Seq[String] = {
     if (isIpv6PreferenceEnabled) {
-      Seq(
-        "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=true",
-        "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=true"
-      )
+      Seq("spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=true") ++ (
+        if (SparkVersionUtils.lessThan("3.4.0")) {
+          // Starting from spark 3.4.0 IPv6 preference for executors is dependent on driver
+          Seq("spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=true")
+        } else {
+          Nil
+        })
     } else {
       Seq()
     }
@@ -190,7 +193,7 @@ trait SparkLauncher {
       "LIVY_PID_DIR" -> livyHome,
       "PYSPARK_PYTHON" -> "python3",
       "LIVY_SERVER_JAVA_OPTS" -> livyJavaOpts,
-      "CLASSPATH" -> s"$sparkHome/jars/*",
+      "CLASSPATH" -> s"$spytHome/jars/*:$sparkHome/jars/*",
       "PYTHONPATH" -> s"$spytHome/python",
     )
     if (javaHome != null) {
--- yt/spark/spark-over-yt/spark-cluster/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusClusterSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-cluster/src/main/scala/tech/ytsaurus/spyt/adapter/YTsaurusClusterSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,17 @@
+package tech.ytsaurus.spyt.adapter
+
+import org.apache.spark.deploy.master.YtMaster
+import tech.ytsaurus.spyt.launcher.DeployMessages.{RegisterDriverToAppId, UnregisterDriverToAppId}
+
+class YTsaurusClusterSupport extends ClusterSupport {
+
+  override val masterEndpointName: String = YtMaster.ENDPOINT_NAME
+
+  override def msgRegisterDriverToAppId(driverId: String, appId: String): Any = {
+    RegisterDriverToAppId(driverId, appId)
+  }
+
+  override def msgUnregisterDriverToAppId(driverId: String): Any = {
+    UnregisterDriverToAppId(driverId)
+  }
+}
--- yt/spark/spark-over-yt/spark-patch/src/main/java/org/apache/spark/launcher/AbstractCommandBuilderForSpyt.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/java/org/apache/spark/launcher/AbstractCommandBuilderForSpyt.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,5 +1,6 @@
 package org.apache.spark.launcher;
 
+import tech.ytsaurus.spyt.patch.annotations.Applicability;
 import tech.ytsaurus.spyt.patch.annotations.OriginClass;
 import tech.ytsaurus.spyt.patch.annotations.Subclass;
 
--- yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/SparkPatchAgent.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/SparkPatchAgent.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -12,9 +12,13 @@ import java.io.*;
 import java.lang.instrument.ClassFileTransformer;
 import java.lang.instrument.Instrumentation;
 import java.lang.management.ManagementFactory;
+import java.nio.file.Files;
+import java.nio.file.Path;
 import java.security.ProtectionDomain;
+import java.util.Arrays;
 import java.util.Map;
 import java.util.Optional;
+import java.util.function.Predicate;
 import java.util.jar.JarFile;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -50,27 +54,61 @@ public class SparkPatchAgent {
     public static void premain(String args, Instrumentation inst) {
         log.info("Starting SparkPatchAgent for hooking on jvm classloader");
 
+        String classpath = System.getProperty("java.class.path");
+        String[] classpathElements = classpath.split(File.pathSeparator);
+
+        Stream<String> adapterImplPaths = Arrays.stream(classpathElements)
+                .filter(path -> path.contains("spark-adapter-impl-") || path.contains("spark-adapter/impl"))
+                .filter(path -> path.endsWith(".jar") || path.endsWith("/classes"));
+
         String patchJarPath = ManagementFactory.getRuntimeMXBean().getInputArguments().stream()
                 .filter(arg -> arg.startsWith("-javaagent") && arg.contains("spark-yt-spark-patch"))
                 .map(arg -> arg.substring(arg.indexOf(':') + 1))
                 .findFirst()
                 .orElseThrow();
 
-        try(JarFile patchJarFile = new JarFile(patchJarPath)) {
-            Stream<String> localClasses = patchJarFile.versionedStream()
+        Stream<String> patchSearchPaths = Stream.concat(Stream.of(patchJarPath), adapterImplPaths);
+
+        Stream<String> classFiles = patchSearchPaths.flatMap(patchSearchPath -> {
+            if (patchSearchPath.endsWith(".jar")) {
+                return scanJarFile(patchSearchPath);
+            } else {
+                return scanClassesDirectory(patchSearchPath);
+            }
+        });
+
+        Map<String,String> classMappings = classFiles
+                .flatMap(fileName -> SparkPatchClassTransformer.toOriginClassName(fileName).stream())
+                .collect(Collectors.toMap(s -> s[0], s -> s[1]));
+
+        inst.addTransformer(new SparkPatchClassTransformer(classMappings));
+    }
+
+    private static Predicate<String> classFileFilter = fileName ->
+            fileName.endsWith(".class") && !fileName.startsWith("tech/ytsaurus/");
+
+    private static Stream<String> scanJarFile(String jarPath) {
+        try(JarFile jarFile = new JarFile(jarPath)) {
+            return jarFile.versionedStream()
                     .map(ZipEntry::getName)
-                    .filter(fileName -> fileName.endsWith(".class") && !fileName.startsWith("tech/ytsaurus/"));
+                    .filter(classFileFilter)
+                    .collect(Collectors.toList())
+                    .stream();
+            // Here we are collecting to list and then recreating a stream because we need to close jar file
+            // inside this method.
+        } catch (IOException e) {
+            throw new SparkPatchException(e);
+        }
+    }
 
-            Stream<String> externalClasses =
-                    resourceToString("/externalClasses.txt").lines()
-                            .filter(line -> !line.isBlank())
-                            .map(className -> className + ".class");
-
-            Map<String,String> classMappings = Streams.concat(localClasses, externalClasses)
-                    .flatMap(fileName -> SparkPatchClassTransformer.toOriginClassName(fileName).stream())
-                    .collect(Collectors.toMap(s -> s[0], s -> s[1]));
-
-            inst.addTransformer(new SparkPatchClassTransformer(classMappings));
+    private static Stream<String> scanClassesDirectory(String dirPath) {
+        Path root = Path.of(dirPath);
+        try (Stream<Path> paths = Files.walk(root)) {
+            return paths
+                    .map(path -> root.relativize(path).toString())
+                    .filter(classFileFilter)
+                    .collect(Collectors.toList())
+                    .stream(); // See above on collecting and recreating a stream
         } catch (IOException e) {
             throw new SparkPatchException(e);
         }
@@ -101,6 +139,10 @@ class SparkPatchClassTransformer implements ClassFileTransformer {
             String originClass = getOriginClass(ctClass);
             boolean isApplicable = !ctClass.hasAnnotation(Applicability.class) ||
                     checkApplicability((Applicability) ctClass.getAnnotation(Applicability.class));
+            PatchSource patchSource = (PatchSource) ctClass.getAnnotation(PatchSource.class);
+            if (patchSource != null) {
+                patchClassName = patchSource.value().replace('.', File.separatorChar);
+            }
             if (originClass != null && isApplicable) {
                 return Optional.of(new String[] {patchClassName, originClass.replace('.', File.separatorChar)});
             }
@@ -114,8 +156,8 @@ class SparkPatchClassTransformer implements ClassFileTransformer {
         OriginClass originClassAnnotaion = (OriginClass) ctClass.getAnnotation(OriginClass.class);
         if (originClassAnnotaion != null) {
             String originClass = originClassAnnotaion.value();
-            if (ctClass.getName().endsWith("$") && !originClass.endsWith("$")) {
-                originClass = originClass + "$";
+            if (originClass.endsWith("$") && !ctClass.getName().endsWith("$")) {
+                return null;
             }
             return originClass;
         }
@@ -216,12 +258,12 @@ class SparkPatchClassTransformer implements ClassFileTransformer {
                 for (CtMethod method : ctClass.getDeclaredMethods()) {
                     if (checkDecoratedMethod(method)) {
                         DecoratedMethod dmAnnotation = (DecoratedMethod) method.getAnnotation(DecoratedMethod.class);
-                        String methodName = dmAnnotation.name().isEmpty() ? method.getName() : dmAnnotation.name();
-                        String methodSignature = dmAnnotation.signature();
-                        CtMethod baseMethod = methodSignature.isEmpty()
-                                ? baseCtClass.getDeclaredMethod(methodName)
-                                : baseCtClass.getMethod(methodName, methodSignature);
-                        log.debug("Patching decorated method {}", methodName);
+                        String methodName = method.getName();
+                        CtMethod baseMethod = baseCtClass.getMethod(methodName, method.getSignature());
+                        log.debug("Patching decorated method {} with signature {}",
+                                methodName,
+                                baseMethod.getSignature()
+                        );
                         String innerMethodName = "__" + methodName;
                         baseMethod.setName(innerMethodName);
 
--- yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/annotations/DecoratedMethod.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/annotations/DecoratedMethod.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -3,7 +3,5 @@ package tech.ytsaurus.spyt.patch.annotations;
 import tech.ytsaurus.spyt.patch.MethodProcesor;
 
 public @interface DecoratedMethod {
-    String name() default "";
-    String signature() default "";
     Class<? extends MethodProcesor>[] baseMethodProcessors() default {};
 }
--- yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/annotations/PatchSource.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/java/tech/ytsaurus/spyt/patch/annotations/PatchSource.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,10 @@
+package tech.ytsaurus.spyt.patch.annotations;
+
+public @interface PatchSource {
+    /**
+     * The patch class that should be used instead of this class. It is helpful when origin class was renamed,
+     * but method bodies wasn't changed, for example all methods of CastBase class for versions prior to 3.4.0
+     * were moved to Cast class since 3.4.0 version.
+     */
+    String value();
+}
--- yt/spark/spark-over-yt/spark-patch/src/main/resources/externalClasses.txt	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/resources/externalClasses.txt	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,36 +0,0 @@
-org/apache/spark/sql/catalyst/ScalaReflectionDecorators$
-org/apache/spark/sql/catalyst/expressions/codegen/CodeGeneratorDecorators$
-org/apache/spark/sql/catalyst/encoders/RowEncoderDecorators$
-org/apache/spark/sql/catalyst/expressions/SpecializedGettersReaderDecorator
-org/apache/spark/sql/catalyst/expressions/HashExpressionSpyt
-org/apache/spark/sql/catalyst/expressions/CastBaseDecorators
-org/apache/spark/sql/execution/aggregate/HashMapGeneratorDecorators
-org/apache/spark/sql/catalyst/expressions/SpecificInternalRowDecorators
-org/apache/spark/sql/catalyst/expressions/codegen/CodegenContextDecorators
-org/apache/spark/sql/catalyst/expressions/CastDecorators$
-org/apache/spark/deploy/rest/spyt/patch/StandaloneRestServer
-org/apache/spark/deploy/rest/RestSubmissionClientAppSpyt
-org/apache/spark/sql/SparkSessionBuilderDecorators
-org/apache/spark/sql/catalyst/catalog/SessionCatalogDecorators
-org/apache/spark/sql/internal/SharedStateDecorators$
-org/apache/spark/deploy/SparkSubmitSpyt
-org/apache/spark/deploy/SparkSubmitDecorators$
-org/apache/spark/sql/catalyst/parser/AstBuilderSpyt
-org/apache/spark/sql/types/DataTypeDecorators$
-org/apache/spark/sql/execution/python/EvaluatePythonDecorators$
-org/apache/spark/deploy/history/SingleEventLogFileWriterSpyt
-org/apache/spark/deploy/history/RollingEventLogFilesWriterSpyt
-org/apache/spark/deploy/history/FsHistoryProviderDecorators
-org/apache/spark/deploy/worker/DriverWrapperDecorators$
-org/apache/spark/SparkConfDecorators
-org/apache/spark/deploy/worker/ui/LogPageSpyt
-org/apache/spark/deploy/worker/DriverRunnerDecorators
-org/apache/spark/scheduler/cluster/StandaloneSchedulerBackendDecorators
-org/apache/spark/deploy/master/MasterDecorators
-org/apache/spark/deploy/PythonRunnerDecorators$
-org/apache/spark/sql/execution/PartitionedFileUtilDecorators$
-org/apache/spark/sql/execution/datasources/FilePartitionDecorators$
-org/apache/spark/sql/execution/datasources/BasicWriteTaskStatsTrackerSpyt
-org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioningDecorator
-org/apache/spark/sql/execution/columnar/ColumnBuilderDecorators$
-org/apache/spark/sql/execution/columnar/GenerateColumnAccessorDecorators$
--- yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/api/python/spyt/patch/PythonFunction.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/api/python/spyt/patch/PythonFunction.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,32 +0,0 @@
-package org.apache.spark.api.python.spyt.patch
-
-import java.util.{List => JList, Map => JMap}
-import org.apache.spark.api.python.{PythonAccumulatorV2, PythonBroadcast}
-import org.apache.spark.broadcast.Broadcast
-import tech.ytsaurus.spyt.patch.annotations.OriginClass
-
-@OriginClass("org.apache.spark.api.python.PythonFunction")
-private[spark] case class PythonFunction(
-    command: Seq[Byte],
-    envVars: JMap[String, String],
-    pythonIncludes: JList[String],
-    private val _pythonExec: String,
-    pythonVer: String,
-    broadcastVars: JList[Broadcast[PythonBroadcast]],
-    accumulator: PythonAccumulatorV2) {
-
-  def this(
-      command: Array[Byte],
-      envVars: JMap[String, String],
-      pythonIncludes: JList[String],
-      _pythonExec: String,
-      pythonVer: String,
-      broadcastVars: JList[Broadcast[PythonBroadcast]],
-      accumulator: PythonAccumulatorV2) = {
-    this(command.toSeq, envVars, pythonIncludes, _pythonExec, pythonVer, broadcastVars, accumulator)
-  }
-
-  def pythonExec: String = {
-    sys.env.getOrElse("PYSPARK_EXECUTOR_PYTHON", _pythonExec)
-  }
-}
--- yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcAddress.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcAddress.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,24 +0,0 @@
-package org.apache.spark.rpc.spyt.patch
-
-import tech.ytsaurus.spyt.patch.annotations.OriginClass
-
-import tech.ytsaurus.spyt.Utils
-
-/**
- * Patches:
- * 1. Support for ipV6 addresses in hostPort
- *
- */
-@OriginClass("org.apache.spark.rpc.RpcAddress")
-private[spark] case class RpcAddress(host: String, port: Int) {
-
-  def hostPort: String = {
-    val newHost = Utils.addBracketsIfIpV6Host(host)
-    s"$newHost:$port"
-  }
-
-  /** Returns a string in the form of "spark://host:port". */
-  def toSparkURL: String = "spark://" + hostPort
-
-  override def toString: String = hostPort
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEndpointAddress.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEndpointAddress.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,56 +0,0 @@
-package org.apache.spark.rpc.spyt.patch
-
-import org.apache.spark.SparkException
-import tech.ytsaurus.spyt.patch.annotations.OriginClass
-import tech.ytsaurus.spyt.Utils.{addBracketsIfIpV6Host, removeBracketsIfIpV6Host}
-
-
-/**
- * Patches:
- * 1. Support for ipV6 addresses in host
- */
-@OriginClass("org.apache.spark.rpc.RpcEndpointAddress")
-case class RpcEndpointAddress(rpcAddress: RpcAddress, name: String) {
-
-  require(name != null, "RpcEndpoint name must be provided.")
-
-  def this(host: String, port: Int, name: String) = {
-    this(RpcAddress(host, port), name)
-  }
-
-  override val toString = if (rpcAddress != null) {
-    s"spark://$name@${addBracketsIfIpV6Host(rpcAddress.host)}:${rpcAddress.port}"
-  } else {
-    s"spark-client://$name"
-  }
-}
-
-@OriginClass("org.apache.spark.rpc.RpcEndpointAddress$")
-private[spark] object RpcEndpointAddress {
-
-  def apply(host: String, port: Int, name: String): RpcEndpointAddress = {
-    new RpcEndpointAddress(host, port, name)
-  }
-
-  def apply(sparkUrl: String): RpcEndpointAddress = {
-    try {
-      val uri = new java.net.URI(sparkUrl)
-      val host = removeBracketsIfIpV6Host(uri.getHost)
-      val port = uri.getPort
-      val name = uri.getUserInfo
-      if (uri.getScheme != "spark" ||
-        host == null ||
-        port < 0 ||
-        name == null ||
-        (uri.getPath != null && !uri.getPath.isEmpty) || // uri.getPath returns "" instead of null
-        uri.getFragment != null ||
-        uri.getQuery != null) {
-        throw new SparkException("Invalid Spark URL: " + sparkUrl)
-      }
-      new RpcEndpointAddress(host, port, name)
-    } catch {
-      case e: java.net.URISyntaxException =>
-        throw new SparkException("Invalid Spark URL: " + sparkUrl, e)
-    }
-  }
-}
--- yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEnvConfig.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/scala/org/apache/spark/rpc/spyt/patch/RpcEnvConfig.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,22 +0,0 @@
-package org.apache.spark.rpc.spyt.patch
-
-import org.apache.spark.{SecurityManager, SparkConf}
-import tech.ytsaurus.spyt.patch.annotations.OriginClass
-
-/**
- * Patches:
- * 1. bindAddress is var and is set to null in constructor. Main reason: we need to bind RPC endpoint to wildcard
- *    network interface for Kubernetes deployments with host network.
- */
-@OriginClass("org.apache.spark.rpc.RpcEnvConfig")
-private[spark] case class RpcEnvConfig(
-  conf: SparkConf,
-  name: String,
-  var bindAddress: String,
-  advertiseAddress: String,
-  port: Int,
-  securityManager: SecurityManager,
-  numUsableCores: Int,
-  clientMode: Boolean) {
-  this.bindAddress = null
-}
--- yt/spark/spark-over-yt/spark-patch/src/main/scala/tech/ytsaurus/spyt/Utils.java	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-patch/src/main/scala/tech/ytsaurus/spyt/Utils.java	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,23 +0,0 @@
-package tech.ytsaurus.spyt;
-
-public class Utils {
-    private static boolean isIpV6Host(String host) {
-        return host != null && host.contains(":");
-    }
-
-    public static String removeBracketsIfIpV6Host(String host) {
-        if (isIpV6Host(host) && host.startsWith("[")) {
-            return host.substring(1, host.length() - 1);
-        } else {
-            return host;
-        }
-    }
-
-    public static String addBracketsIfIpV6Host(String host) {
-        if (isIpV6Host(host) && !host.startsWith("[")) {
-            return "[" + host + "]";
-        } else {
-            return host;
-        }
-    }
-}
--- yt/spark/spark-over-yt/spark-submit/src/main/resources/META-INF/services/org.apache.spark.deploy.SubmitSupport	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-submit/src/main/resources/META-INF/services/org.apache.spark.deploy.SubmitSupport	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1 @@
+org.apache.spark.deploy.YTsaurusSubmitSupport
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-submit/src/main/scala/org/apache/spark/deploy/SparkSubmitSpyt.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-submit/src/main/scala/org/apache/spark/deploy/SparkSubmitSpyt.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -1,355 +0,0 @@
-package org.apache.spark.deploy
-
-import javassist.CtMethod
-import javassist.bytecode.{CodeAttribute, ConstPool, Opcode, StackMapTable, YTsaurusBytecodeUtils}
-import org.apache.hadoop.conf.{Configuration => HadoopConfiguration}
-import org.apache.spark.{SPARK_VERSION, SparkConf}
-import org.apache.spark.deploy.ytsaurus.Config.{YTSAURUS_IS_PYTHON, YTSAURUS_IS_PYTHON_BINARY, YTSAURUS_POOL, YTSAURUS_PYTHON_BINARY_ENTRY_POINT, YTSAURUS_PYTHON_EXECUTABLE}
-import org.apache.spark.deploy.ytsaurus.YTsaurusUtils
-import org.apache.spark.internal.config._
-import org.apache.spark.launcher.SparkLauncher
-import org.apache.spark.util.{DependencyUtils, Utils}
-import tech.ytsaurus.spyt.patch.MethodProcesor
-import tech.ytsaurus.spyt.patch.annotations.{Decorate, DecoratedMethod, OriginClass}
-
-import scala.collection.mutable.ArrayBuffer
-import scala.sys.process.Process
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.SparkSubmit")
-private[spark] class SparkSubmitSpyt {
-  import YTsaurusConstants._
-  import SparkSubmitSpyt._
-
-  @DecoratedMethod(baseMethodProcessors = Array(classOf[ClusterManagerInitializerBytecodeModifier]))
-  private[deploy] def prepareSubmitEnvironment(args: SparkSubmitArguments,
-                                               conf: Option[HadoopConfiguration] = None)
-  : (Seq[String], Seq[String], SparkConf, String) = {
-
-    // Exact copy of the corresponding construct in base method
-    val deployMode: Int = args.deployMode match {
-      case "client" | null => CLIENT
-      case "cluster" => CLUSTER
-      case _ =>
-        error("Deploy mode must be either client or cluster")
-        -1
-    }
-
-    // Other cluster managers are processed inside __prepareSubmitEnvironment
-    val clusterManager: Int = if (args.master.startsWith(YTSAURUS_MASTER)) YTSAURUS else 0
-    val isYTsaurusCluster = clusterManager == YTSAURUS && deployMode == CLUSTER
-
-    // here we use args.master.startsWith instead of STANDALONE because it should be processed inside base method
-    if (args.isPython && args.master.startsWith("spark") && deployMode == CLUSTER) {
-      args.mainClass = "org.apache.spark.deploy.PythonRunner"
-      args.childArgs = ArrayBuffer("{{USER_JAR}}", "{{PY_FILES}}") ++ args.childArgs
-      args.files = DependencyUtils.mergeFileLists(args.files, args.pyFiles)
-    }
-
-    if (clusterManager == YTSAURUS) {
-
-      if (!Utils.classIsLoadable(YTSAURUS_CLUSTER_SUBMIT_CLASS) && !Utils.isTesting) {
-        error(
-          "Could not load YTSAURUS classes. " +
-            "It seems that YTSAURUS libraries are not in the environment. " +
-            "To add them the following steps should be performed:\n\n" +
-            "1. Install ytsaurus-spyt python package via \033[1mpip install ytsaurus-spyt\033[0m\n" +
-            "2. Activate SPYT configuration in environment by running " +
-            "\033[1msource spyt-env\033[0m command\n")
-      }
-
-      // This property is used  to initialize ytsaurus file system which is subclass of
-      // org.apache.hadoop.fs.FileSystem via spark hadoop configuration
-      args.sparkProperties += "spark.hadoop.yt.proxy" -> args.master.substring("ytsaurus://".length)
-    }
-
-    // This is a potential security issue, should be fixed ASAP, see SPYT-604 for details
-    val ytToken = sys.env.get("SPARK_YT_TOKEN")
-    if (ytToken.isDefined) {
-      args.sparkProperties += "spark.hadoop.yt.token" -> ytToken.get
-    }
-
-    var (childArgsSeq, childClasspath, sparkConf, childMainClass) = __prepareSubmitEnvironment(args, conf)
-
-    val childArgs = new ArrayBuffer[String]()
-
-    // This section is a copy of the corresponding section of super.prepareSubmitEnvironment
-    val options = List[OptionAssigner](
-      // Updated copy from SparkSubmit
-      OptionAssigner(args.packages, YTSAURUS, CLUSTER, confKey = JAR_PACKAGES.key),
-      OptionAssigner(args.repositories, YTSAURUS, CLUSTER, confKey = JAR_REPOSITORIES.key),
-      OptionAssigner(args.ivyRepoPath, YTSAURUS, CLUSTER, confKey = JAR_IVY_REPO_PATH.key),
-      OptionAssigner(args.packagesExclusions, YTSAURUS, CLUSTER, confKey = JAR_PACKAGES_EXCLUSIONS.key),
-
-      OptionAssigner(args.numExecutors, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_INSTANCES.key),
-      OptionAssigner(args.executorCores, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_CORES.key),
-      OptionAssigner(args.executorMemory, YTSAURUS, ALL_DEPLOY_MODES, confKey = EXECUTOR_MEMORY.key),
-      OptionAssigner(args.files, YTSAURUS, ALL_DEPLOY_MODES, confKey = FILES.key),
-      OptionAssigner(args.archives, YTSAURUS, ALL_DEPLOY_MODES, confKey = ARCHIVES.key),
-      OptionAssigner(args.jars, YTSAURUS, ALL_DEPLOY_MODES, confKey = JARS.key),
-      OptionAssigner(args.driverMemory, YTSAURUS, CLUSTER, confKey = DRIVER_MEMORY.key),
-      OptionAssigner(args.driverCores, YTSAURUS, CLUSTER, confKey = DRIVER_CORES.key),
-
-      // YTsaurus only
-      OptionAssigner(args.queue, YTSAURUS, ALL_DEPLOY_MODES, confKey = YTSAURUS_POOL.key),
-      OptionAssigner(args.isPython.toString, YTSAURUS, ALL_DEPLOY_MODES, confKey = YTSAURUS_IS_PYTHON.key),
-      OptionAssigner(System.getenv("Y_PYTHON_ENTRY_POINT"), YTSAURUS, CLUSTER,
-        confKey = YTSAURUS_PYTHON_BINARY_ENTRY_POINT.key, mergeFn = Some(confOverEnv))
-    )
-
-    childArgs ++= processOptions(options, deployMode, clusterManager, sparkConf)
-
-    if (isBinary(args.primaryResource)) {
-      sparkConf.set(YTSAURUS_IS_PYTHON_BINARY, true)
-    }
-
-    if (clusterManager == YTSAURUS && args.isPython && deployMode == CLIENT &&
-        !sparkConf.contains(YTSAURUS_PYTHON_EXECUTABLE.key) && !sparkConf.get(YTSAURUS_IS_PYTHON_BINARY)) {
-      val basePythonExecutable = getBasePythonExec(sparkConf)
-      sparkConf.set(YTSAURUS_PYTHON_EXECUTABLE.key, basePythonExecutable)
-    }
-
-    if (sparkConf.get(YTSAURUS_IS_PYTHON_BINARY) && deployMode == CLIENT) {
-      val pyBinaryWrapper = YTsaurusUtils.pythonBinaryWrapperPath(sys.env("SPYT_ROOT"))
-      sparkConf.set(PYSPARK_DRIVER_PYTHON, pyBinaryWrapper)
-    }
-
-    if (isYTsaurusCluster) {
-      childMainClass = YTSAURUS_CLUSTER_SUBMIT_CLASS
-      if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
-        if (args.isPython) {
-          childArgs ++= Array("--primary-py-file", args.primaryResource)
-          childArgs ++= Array("--main-class", "org.apache.spark.deploy.PythonRunner")
-        } else if (args.isR) {
-          childArgs ++= Array("--primary-r-file", args.primaryResource)
-          childArgs ++= Array("--main-class", "org.apache.spark.deploy.RRunner")
-        }
-        else {
-          childArgs ++= Array("--primary-java-resource", args.primaryResource)
-          childArgs ++= Array("--main-class", args.mainClass)
-        }
-      } else {
-        childArgs ++= Array("--main-class", args.mainClass)
-      }
-
-      if (args.isPython) {
-        sparkConf.set("spark.ytsaurus.redirectToStderr", "true")
-      }
-
-      if (args.childArgs != null) {
-        appendChildArgs(args, childArgs)
-      }
-    }
-
-    if (clusterManager == YTSAURUS) {
-      sparkConf.set("spark.ytsaurus.primary.resource", args.primaryResource)
-    }
-
-    (childArgsSeq ++ childArgs, childClasspath, sparkConf, childMainClass)
-  }
-
-  private[deploy] def __prepareSubmitEnvironment(args: SparkSubmitArguments, conf: Option[HadoopConfiguration] = None)
-  : (Seq[String], Seq[String], SparkConf, String) = ???
-
-  @DecoratedMethod
-  private def error(msg: String): Unit = {
-    if (!msg.equals("Cluster deploy mode is currently not supported for python applications on standalone clusters.")) {
-      __error(msg)
-    }
-  }
-
-  def __error(msg: String): Unit = ???
-}
-
-object SparkSubmitSpyt {
-  import YTsaurusConstants._
-
-  // YTsaurus additions
-  private val ALL_CLUSTER_MGRS: Int = {
-    val cls = SparkSubmit.getClass
-    val field = cls.getDeclaredField("org$apache$spark$deploy$SparkSubmit$$ALL_CLUSTER_MGRS")
-    field.setAccessible(true)
-    val value = field.getInt(SparkSubmit) | YTSAURUS
-    field.set(SparkSubmit, value)
-    field.setAccessible(false)
-    value
-  }
-
-  // Exact copy from object SparkSubmit
-  private val CLIENT = 1
-  private val CLUSTER = 2
-  private val ALL_DEPLOY_MODES = CLIENT | CLUSTER
-
-  private[deploy] val YTSAURUS_CLUSTER_SUBMIT_CLASS =
-    "org.apache.spark.deploy.ytsaurus.YTsaurusClusterApplication"
-
-  private[deploy] def processOptions(options: List[OptionAssigner],
-                                     deployMode: Int,
-                                     clusterManager: Int,
-                                     sparkConf: SparkConf): Seq[String] = {
-    val childArgs = new ArrayBuffer[String]()
-    for (opt <- options) {
-      if (opt.value != null &&
-        (deployMode & opt.deployMode) != 0 &&
-        (clusterManager & opt.clusterManager) != 0) {
-        if (opt.clOption != null) { childArgs += (opt.clOption, opt.value) }
-        if (opt.confKey != null) {
-          if (opt.mergeFn.isDefined && sparkConf.contains(opt.confKey)) {
-            sparkConf.set(opt.confKey, opt.mergeFn.get.apply(sparkConf.get(opt.confKey), opt.value))
-          } else {
-            sparkConf.set(opt.confKey, opt.value)
-          }
-        }
-      }
-    }
-    childArgs
-  }
-
-  private[deploy] def appendChildArgs(args: SparkSubmitArguments, childArgs: ArrayBuffer[String]): Unit = {
-    args.childArgs.foreach { arg =>
-      childArgs += ("--arg", arg)
-    }
-  }
-
-  private[deploy] def getBasePythonExec(sparkConf: SparkConf): String = {
-    // Exact copy from main method of org.apache.spark.deploy.PythonRunner
-    val pythonExec = sparkConf.get(PYSPARK_DRIVER_PYTHON)
-      .orElse(sparkConf.get(PYSPARK_PYTHON))
-      .orElse(sys.env.get("PYSPARK_DRIVER_PYTHON"))
-      .orElse(sys.env.get("PYSPARK_PYTHON"))
-      .getOrElse("python3")
-
-    val pythonVersion = Process(pythonExec, Seq("-V")).!!.replace("Python","").trim()
-    val pythonVersionShort = pythonVersion.substring(0, pythonVersion.indexOf('.', pythonVersion.indexOf('.') + 1))
-    s"python$pythonVersionShort"
-  }
-
-  private val knownExtensions = Set("jar", "py", "r")
-
-  private[deploy] def isBinary(res: String): Boolean = {
-    if (res == null || SparkSubmit.isShell(res) || SparkSubmit.isInternal(res)) {
-      false
-    } else {
-      val lastDot = res.lastIndexOf(".")
-      lastDot == -1 || !knownExtensions.contains(res.substring(lastDot + 1).toLowerCase)
-    }
-  }
-
-  private val confOverEnv: (String, String) => String = (confValue, _) => confValue
-}
-
-@Decorate
-@OriginClass("org.apache.spark.deploy.SparkSubmit$")
-object SparkSubmitDecorators {
-
-  @DecoratedMethod
-  private[deploy] def isPython(res: String): Boolean = {
-    __isPython(res) || SparkSubmitSpyt.isBinary(res)
-  }
-
-  private[deploy] def __isPython(res: String): Boolean = ???
-}
-
-
-object YTsaurusConstants {
-  val YTSAURUS = 32
-  val YTSAURUS_MASTER = "ytsaurus"
-}
-
-class ClusterManagerInitializerBytecodeModifier extends MethodProcesor {
-  import YTsaurusBytecodeUtils._
-  import YTsaurusConstants._
-
-  override def process(method: CtMethod): Unit = {
-    val methodInfo = method.getMethodInfo
-    val cp = methodInfo.getConstPool
-    val ca = methodInfo.getCodeAttribute
-
-    updateErrorMessage(cp)
-    addYTsaurusClause(cp, ca)
-  }
-
-  private def addYTsaurusClause(cp: ConstPool, ca: CodeAttribute): Unit = {
-    val localConstId = findStringConstant(cp, _.equals("local"))
-
-    val ci = ca.iterator()
-
-    // Looking for start of the "local" cluster manager clause
-    var found: Boolean = false
-    var samplePos = 0
-    while (ci.hasNext && !found) {
-      val index = ci.next()
-      val op = ci.byteAt(index);
-      if (op == Opcode.ALOAD && ci.byteAt(ci.lookAhead()) == Opcode.LDC_W) {
-        val ldcwOpIndex = ci.next()
-        val ldcwArgument = ci.u16bitAt(ldcwOpIndex + 1)
-        if (ldcwArgument == localConstId) {
-          found = true
-          samplePos = index
-        }
-      }
-    }
-
-    // Looking for the end of the "local" cluster manager clause
-    found = false
-    var insertPos = 0
-    while (ci.hasNext && !found) {
-      val index = ci.next()
-      val op = ci.byteAt(index)
-      if (op == Opcode.GOTO && ci.u16bitAt(index + 1) == 3) { // found goto 3
-        found = true
-        insertPos = ci.lookAhead()
-      }
-    }
-
-    val sampleLength = insertPos - samplePos
-    val sample = new Array[Byte](sampleLength)
-
-    System.arraycopy(ca.getCode, samplePos, sample, 0, sampleLength)
-
-    val ytsaurusConstId = cp.addStringInfo(YTSAURUS_MASTER)
-    val ytsaurusCodeId = cp.addIntegerInfo(YTSAURUS)
-
-    val si = new CodeAttribute(cp, ca.getMaxStack, ca.getMaxLocals, sample, ca.getExceptionTable).iterator
-
-    var skipInvokeVirtual = false
-    val sameFramePositions = new java.util.ArrayList[Integer]()
-    while (si.hasNext) {
-      val index = si.next
-      val op = si.byteAt(index)
-
-      if (op == Opcode.LDC_W) {
-        si.write16bit(ytsaurusConstId, index + 1)
-      } else if (op == Opcode.GETSTATIC) {
-        skipInvokeVirtual = true
-        si.writeByte(Opcode.LDC_W, index)
-        si.write16bit(ytsaurusCodeId, index + 1)
-      } else if (op == Opcode.INVOKEVIRTUAL && skipInvokeVirtual) {
-        // fill second invokevirtual with nop because we use constant instead of accessing object fields
-        for (pos <- index until si.lookAhead) {
-          si.writeByte(Opcode.NOP, pos)
-        }
-      } else if (op == Opcode.IFEQ || op == Opcode.GOTO) {
-        val offset = si.u16bitAt(index + 1)
-        if (index + offset <= sampleLength) {
-          sameFramePositions.add(insertPos + index + offset)
-        }
-      }
-    }
-
-    ci.insertAt(insertPos, sample)
-
-    val originTable = ca.getAttribute(StackMapTable.tag).asInstanceOf[StackMapTable]
-    val modifiedTable = addStackMapTableFrames(originTable, sameFramePositions, insertPos, sampleLength)
-    ca.setAttribute(modifiedTable)
-  }
-
-  private def updateErrorMessage(cp: ConstPool): Unit = {
-    val invalidClusterManagerMsgId = findStringConstant(cp, _.startsWith("Master must either"))
-    val invalidClusterManagerMsg = cp.getStringInfo(invalidClusterManagerMsgId);
-    updateUtf8Constant(
-      cp, getUtf8ConstantId(cp, invalidClusterManagerMsgId),
-      invalidClusterManagerMsg.replace("k8s,", "k8s, ytsaurus")
-    )
-  }
-}
\ No newline at end of file
--- yt/spark/spark-over-yt/spark-submit/src/main/scala/org/apache/spark/deploy/YTsaurusSubmitSupport.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-submit/src/main/scala/org/apache/spark/deploy/YTsaurusSubmitSupport.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -0,0 +1,18 @@
+package org.apache.spark.deploy
+
+import org.apache.spark.deploy.ytsaurus.{Config, YTsaurusUtils}
+import org.apache.spark.internal.config.{ConfigEntry, OptionalConfigEntry}
+
+class YTsaurusSubmitSupport extends SubmitSupport {
+  override val YTSAURUS_IS_PYTHON: ConfigEntry[Boolean] = Config.YTSAURUS_IS_PYTHON
+  override val YTSAURUS_IS_PYTHON_BINARY: ConfigEntry[Boolean] = Config.YTSAURUS_IS_PYTHON_BINARY
+  override val YTSAURUS_POOL: OptionalConfigEntry[String] = Config.YTSAURUS_POOL
+  override val YTSAURUS_PYTHON_BINARY_ENTRY_POINT: OptionalConfigEntry[String] =
+    Config.YTSAURUS_PYTHON_BINARY_ENTRY_POINT
+  override val YTSAURUS_PYTHON_EXECUTABLE: OptionalConfigEntry[String] =
+    Config.YTSAURUS_PYTHON_EXECUTABLE
+
+  def pythonBinaryWrapperPath(spytHome: String): String = {
+    YTsaurusUtils.pythonBinaryWrapperPath(sys.env("SPYT_ROOT"))
+  }
+}
--- yt/spark/spark-over-yt/spark-submit/src/main/scala/tech/ytsaurus/spyt/submit/SubmissionClient.scala	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spark-submit/src/main/scala/tech/ytsaurus/spyt/submit/SubmissionClient.scala	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -9,6 +9,7 @@ import tech.ytsaurus.spyt.wrapper.client.{YtClientConfiguration, YtClientProvide
 import tech.ytsaurus.spyt.wrapper.config.Utils.{parseRemoteConfig, remoteClusterConfigPath, remoteGlobalConfigPath, remoteVersionConfigPath}
 import tech.ytsaurus.spyt.wrapper.discovery.CypressDiscoveryService
 import tech.ytsaurus.core.cypress.YPath
+import tech.ytsaurus.spyt.SparkVersionUtils
 
 import java.io.File
 import java.util.UUID
@@ -66,7 +67,10 @@ class SubmissionClient(proxy: String,
     if (ipv6Enabled) {
       log.debug("preferIPv6Addresses was added to extraJavaOptions")
       launcher.setConf("spark.driver.extraJavaOptions", "-Djava.net.preferIPv6Addresses=true")
-      launcher.setConf("spark.executor.extraJavaOptions", "-Djava.net.preferIPv6Addresses=true")
+      if (SparkVersionUtils.lessThan("3.4.0")) {
+        // Starting from spark 3.4.0 IPv6 preference for executors is dependent on driver
+        launcher.setConf("spark.executor.extraJavaOptions", "-Djava.net.preferIPv6Addresses=true")
+      }
     }
 
     launcher.setConf("spark.master.rest.enabled", "true")
--- yt/spark/spark-over-yt/spyt-package/src/main/python/setup.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/setup.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -39,7 +39,7 @@ setuptools.setup(
     ],
     extras_require={
         "all": [
-            'pyspark>=3.2.2,<3.4.0',
+            'pyspark>=3.2.2,<3.5.0',
             'ytsaurus-client',
             'ytsaurus-yson'
         ]
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/__init__.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/__init__.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -11,7 +11,8 @@ require_pyspark()
 from .client import connect, spark_session, connect_direct, direct_spark_session, \
     info, stop, jvm_process_pid, yt_client, is_stopped  # noqa: E402
 from .extensions import read_yt, read_schema_hint, write_yt, sorted_by, optimize_for, withYsonColumn, transform, \
-    _extract_code_globals, _code_reduce, write_schema_hint  # noqa: E402
+    write_schema_hint  # noqa: E402
+from .utils import check_spark_version  # noqa: E402
 from spyt.types import UInt64Type  # noqa: E402
 import pyspark.context  # noqa: E402
 import pyspark.sql.types  # noqa: E402
@@ -52,10 +53,13 @@ def initialize():
 
     pyspark.context.SparkContext.PACKAGE_EXTENSIONS += ('.whl',)
 
-    pyspark.cloudpickle.cloudpickle._extract_code_globals = _extract_code_globals
-    pyspark.cloudpickle.cloudpickle_fast._extract_code_globals = _extract_code_globals
-    pyspark.cloudpickle.cloudpickle_fast._code_reduce = _code_reduce
-    pyspark.cloudpickle.cloudpickle_fast.CloudPickler.dispatch_table[CodeType] = _code_reduce
+    if check_spark_version(less_than="3.4.0"):
+        from .extensions import _extract_code_globals, _code_reduce
+
+        pyspark.cloudpickle.cloudpickle._extract_code_globals = _extract_code_globals
+        pyspark.cloudpickle.cloudpickle_fast._extract_code_globals = _extract_code_globals
+        pyspark.cloudpickle.cloudpickle_fast._code_reduce = _code_reduce
+        pyspark.cloudpickle.cloudpickle_fast.CloudPickler.dispatch_table[CodeType] = _code_reduce
 
 
 initialize()
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/client.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/client.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -14,7 +14,8 @@ from yt.wrapper.http_helpers import get_token, get_user_name, get_proxy_address_
 
 from .arcadia import checked_extract_spark  # noqa: E402
 from .utils import default_token, default_discovery_dir, get_spark_master, set_conf, \
-    SparkDiscovery, parse_memory, format_memory, base_spark_conf, parse_bool, get_spyt_home  # noqa: E402
+    SparkDiscovery, parse_memory, format_memory, base_spark_conf, parse_bool, get_spyt_home, \
+    check_spark_version  # noqa: E402
 from .conf import read_remote_conf, read_global_conf, validate_versions_compatibility, \
     read_cluster_conf, SELF_VERSION  # noqa: E402
 
@@ -286,7 +287,9 @@ def _build_spark_conf(num_executors=None,
     ipv6_preference_enabled = parse_bool(spark_conf.get('spark.hadoop.yt.preferenceIpv6.enabled'))
     if ipv6_preference_enabled:
         spark_conf.set('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=true')
-        spark_conf.set('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=true')
+        if check_spark_version(less_than="3.4.0"):
+            # Starting from spark 3.4.0 IPv6 preference for executors is dependent on driver
+            spark_conf.set('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=true')
 
     return spark_conf
 
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/dependency_utils.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/dependency_utils.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -10,7 +10,7 @@ def require_yt_client():
 
 
 def require_pyspark():
-    compatible_versions_str = '>=3.2.2,<3.4.0'
+    compatible_versions_str = '>=3.2.2,<3.5.0'
     try:
         import pyspark
         try:
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/enabler.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/enabler.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -46,6 +46,8 @@ class SpytEnablers(object):
         self.enable_solomon_agent = self._get_enabler(self.enable_solomon_agent, "enable_solomon_agent",
                                                       self.SOLOMON_AGENT_KEY)
         self.enable_tcp_proxy = self._get_enabler(self.enable_tcp_proxy, "enable_tcp_proxy", self.TCP_PROXY_KEY)
+        self.enable_preference_ipv6 = self._get_enabler(self.enable_preference_ipv6, "enable_preference_ipv6",
+                                                        self.IPV6_KEY)
 
     def __str__(self):
         return json.dumps(self.get_conf())
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/extensions.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/extensions.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -8,6 +8,8 @@ from pyspark.sql.dataframe import DataFrame
 from pyspark.sql.types import StructType, StructField
 from pyspark.sql import SparkSession
 
+from .utils import check_spark_version
+
 
 # DataFrameReader extensions
 def read_yt(self, *paths):
@@ -74,119 +76,117 @@ def transform(self, func):
     return func(self)
 
 
-# Cloudpickle extensions. Must be applied if Spark version is less than 3.4.0
-def _extract_code_globals(co):
-    """
-    Find all globals names read or written to by codeblock co
-    """
-    out_names = _extract_code_globals_cache.get(co)
-    if out_names is None:
-        # SPYT-415: From Spark 3.4
-        if sys.version_info < (3, 11):
-            names = co.co_names
-            out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
-        else:
-            # We use a dict with None values instead of a set to get a
-            # deterministic order (assuming Python 3.6+) and avoid introducing
-            # non-deterministic pickle bytes as a results.
-            out_names = {name: None for name in _walk_global_ops(co)}
-
-        # Declaring a function inside another one using the "def ..."
-        # syntax generates a constant code object corresonding to the one
-        # of the nested function's As the nested function may itself need
-        # global variables, we need to introspect its code, extract its
-        # globals, (look for code object in it's co_consts attribute..) and
-        # add the result to code_globals
-        if co.co_consts:
-            for const in co.co_consts:
-                if isinstance(const, types.CodeType):
-                    out_names.update(_extract_code_globals(const))
-
-        _extract_code_globals_cache[co] = out_names
-
-    return out_names
-
-
-# SPYT-415: From Spark 3.4
-if sys.version_info < (3, 11):
-    def _walk_global_ops(code):
+if check_spark_version(less_than="3.4.0"):
+    def _extract_code_globals(co):
         """
-        Yield (opcode, argument number) tuples for all
-        global-referencing instructions in *code*.
+        Find all globals names read or written to by codeblock co
         """
-        for instr in dis.get_instructions(code):
-            op = instr.opcode
-            if op in GLOBAL_OPS:
-                yield op, instr.arg
-else:
-    def _walk_global_ops(code):
-        """
-        Yield referenced name for all global-referencing instructions in *code*.
-        """
-        for instr in dis.get_instructions(code):
-            op = instr.opcode
-            if op in GLOBAL_OPS:
-                yield instr.argval
+        out_names = _extract_code_globals_cache.get(co)
+        if out_names is None:
+            # SPYT-415: From Spark 3.4
+            if sys.version_info < (3, 11):
+                names = co.co_names
+                out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
+            else:
+                # We use a dict with None values instead of a set to get a
+                # deterministic order (assuming Python 3.6+) and avoid introducing
+                # non-deterministic pickle bytes as a results.
+                out_names = {name: None for name in _walk_global_ops(co)}
+
+            # Declaring a function inside another one using the "def ..."
+            # syntax generates a constant code object corresonding to the one
+            # of the nested function's As the nested function may itself need
+            # global variables, we need to introspect its code, extract its
+            # globals, (look for code object in it's co_consts attribute..) and
+            # add the result to code_globals
+            if co.co_consts:
+                for const in co.co_consts:
+                    if isinstance(const, types.CodeType):
+                        out_names.update(_extract_code_globals(const))
+
+            _extract_code_globals_cache[co] = out_names
 
+        return out_names
 
-def _code_reduce(obj):
-    """codeobject reducer"""
     # SPYT-415: From Spark 3.4
-    # If you are not sure about the order of arguments, take a look at help
-    # of the specific type from types, for example:
-    # >>> from types import CodeType
-    # >>> help(CodeType)
-    if hasattr(obj, "co_exceptiontable"):  # pragma: no branch
-        # Python 3.11 and later: there are some new attributes
-        # related to the enhanced exceptions.
-        args = (
-            obj.co_argcount, obj.co_posonlyargcount,
-            obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
-            obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
-            obj.co_varnames, obj.co_filename, obj.co_name, obj.co_qualname,
-            obj.co_firstlineno, obj.co_linetable, obj.co_exceptiontable,
-            obj.co_freevars, obj.co_cellvars,
-        )
-    elif hasattr(obj, "co_linetable"):  # pragma: no branch
-        # Python 3.10 and later: obj.co_lnotab is deprecated and constructor
-        # expects obj.co_linetable instead.
-        args = (
-            obj.co_argcount, obj.co_posonlyargcount,
-            obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
-            obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
-            obj.co_varnames, obj.co_filename, obj.co_name,
-            obj.co_firstlineno, obj.co_linetable, obj.co_freevars,
-            obj.co_cellvars
-        )
-    elif hasattr(obj, "co_nmeta"):  # pragma: no cover
-        # "nogil" Python: modified attributes from 3.9
-        args = (
-            obj.co_argcount, obj.co_posonlyargcount,
-            obj.co_kwonlyargcount, obj.co_nlocals, obj.co_framesize,
-            obj.co_ndefaultargs, obj.co_nmeta,
-            obj.co_flags, obj.co_code, obj.co_consts,
-            obj.co_varnames, obj.co_filename, obj.co_name,
-            obj.co_firstlineno, obj.co_lnotab, obj.co_exc_handlers,
-            obj.co_jump_table, obj.co_freevars, obj.co_cellvars,
-            obj.co_free2reg, obj.co_cell2reg
-        )
-    elif hasattr(obj, "co_posonlyargcount"):
-        # Backward compat for 3.9 and older
-        args = (
-            obj.co_argcount, obj.co_posonlyargcount,
-            obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
-            obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
-            obj.co_varnames, obj.co_filename, obj.co_name,
-            obj.co_firstlineno, obj.co_lnotab, obj.co_freevars,
-            obj.co_cellvars
-        )
+    if sys.version_info < (3, 11):
+        def _walk_global_ops(code):
+            """
+            Yield (opcode, argument number) tuples for all
+            global-referencing instructions in *code*.
+            """
+            for instr in dis.get_instructions(code):
+                op = instr.opcode
+                if op in GLOBAL_OPS:
+                    yield op, instr.arg
     else:
-        # Backward compat for even older versions of Python
-        args = (
-            obj.co_argcount, obj.co_kwonlyargcount, obj.co_nlocals,
-            obj.co_stacksize, obj.co_flags, obj.co_code, obj.co_consts,
-            obj.co_names, obj.co_varnames, obj.co_filename,
-            obj.co_name, obj.co_firstlineno, obj.co_lnotab,
-            obj.co_freevars, obj.co_cellvars
-        )
-    return types.CodeType, args
+        def _walk_global_ops(code):
+            """
+            Yield referenced name for all global-referencing instructions in *code*.
+            """
+            for instr in dis.get_instructions(code):
+                op = instr.opcode
+                if op in GLOBAL_OPS:
+                    yield instr.argval
+
+    def _code_reduce(obj):
+        """codeobject reducer"""
+        # SPYT-415: From Spark 3.4
+        # If you are not sure about the order of arguments, take a look at help
+        # of the specific type from types, for example:
+        # >>> from types import CodeType
+        # >>> help(CodeType)
+        if hasattr(obj, "co_exceptiontable"):  # pragma: no branch
+            # Python 3.11 and later: there are some new attributes
+            # related to the enhanced exceptions.
+            args = (
+                obj.co_argcount, obj.co_posonlyargcount,
+                obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
+                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
+                obj.co_varnames, obj.co_filename, obj.co_name, obj.co_qualname,
+                obj.co_firstlineno, obj.co_linetable, obj.co_exceptiontable,
+                obj.co_freevars, obj.co_cellvars,
+            )
+        elif hasattr(obj, "co_linetable"):  # pragma: no branch
+            # Python 3.10 and later: obj.co_lnotab is deprecated and constructor
+            # expects obj.co_linetable instead.
+            args = (
+                obj.co_argcount, obj.co_posonlyargcount,
+                obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
+                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
+                obj.co_varnames, obj.co_filename, obj.co_name,
+                obj.co_firstlineno, obj.co_linetable, obj.co_freevars,
+                obj.co_cellvars
+            )
+        elif hasattr(obj, "co_nmeta"):  # pragma: no cover
+            # "nogil" Python: modified attributes from 3.9
+            args = (
+                obj.co_argcount, obj.co_posonlyargcount,
+                obj.co_kwonlyargcount, obj.co_nlocals, obj.co_framesize,
+                obj.co_ndefaultargs, obj.co_nmeta,
+                obj.co_flags, obj.co_code, obj.co_consts,
+                obj.co_varnames, obj.co_filename, obj.co_name,
+                obj.co_firstlineno, obj.co_lnotab, obj.co_exc_handlers,
+                obj.co_jump_table, obj.co_freevars, obj.co_cellvars,
+                obj.co_free2reg, obj.co_cell2reg
+            )
+        elif hasattr(obj, "co_posonlyargcount"):
+            # Backward compat for 3.9 and older
+            args = (
+                obj.co_argcount, obj.co_posonlyargcount,
+                obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,
+                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names,
+                obj.co_varnames, obj.co_filename, obj.co_name,
+                obj.co_firstlineno, obj.co_lnotab, obj.co_freevars,
+                obj.co_cellvars
+            )
+        else:
+            # Backward compat for even older versions of Python
+            args = (
+                obj.co_argcount, obj.co_kwonlyargcount, obj.co_nlocals,
+                obj.co_stacksize, obj.co_flags, obj.co_code, obj.co_consts,
+                obj.co_names, obj.co_varnames, obj.co_filename,
+                obj.co_name, obj.co_firstlineno, obj.co_lnotab,
+                obj.co_freevars, obj.co_cellvars
+            )
+        return types.CodeType, args
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/standalone.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/standalone.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -28,7 +28,7 @@ from .conf import read_remote_conf, validate_cluster_version, \
     latest_ytserver_proxy_path, read_global_conf, \
     worker_num_limit, validate_worker_num, read_cluster_conf, validate_ssd_config, cuda_toolkit_version  # noqa: E402
 from .utils import get_spark_master, base_spark_conf, SparkDiscovery, SparkCluster, call_get_proxy_address_url, \
-    parse_bool, _add_conf  # noqa: E402
+    parse_bool, _add_conf, check_spark_version  # noqa: E402
 from .enabler import SpytEnablers  # noqa: E402
 from .spec import SparkDefaultArguments, CommonComponentConfig, MasterConfig, WorkerConfig, HistoryServerConfig, \
     LivyConfig, build_spark_operation_spec, WorkerResources  # noqa: E402
@@ -210,10 +210,11 @@ def _add_python_version(python_version, spark_args):
 
 def _add_ipv6_preference(ipv6_preference_enabled, spark_args):
     if ipv6_preference_enabled:
-        _add_conf({
-            'spark.driver.extraJavaOptions': '-Djava.net.preferIPv6Addresses=true',
-            'spark.executor.extraJavaOptions': '-Djava.net.preferIPv6Addresses=true'
-        }, spark_args)
+        ipv6_conf = {'spark.driver.extraJavaOptions': '-Djava.net.preferIPv6Addresses=true'}
+        if check_spark_version(less_than="3.4.0"):
+            # Starting from spark 3.4.0 IPv6 preference for executors is dependent on driver
+            ipv6_conf['spark.executor.extraJavaOptions'] = '-Djava.net.preferIPv6Addresses=true'
+        _add_conf(ipv6_conf, spark_args)
 
 
 def wrap_cached_jar(path, jar_caching_enabled):
--- yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/utils.py	(79d41f54f128fe83d10d51f46f7a650771b5f80c)
+++ yt/spark/spark-over-yt/spyt-package/src/main/python/spyt/utils.py	(44610e19de8751ebe6b912d1075558cd1e4b711f)
@@ -3,6 +3,8 @@ import logging
 import os
 import re
 import subprocess
+from packaging.version import Version
+from pyspark import __version__ as spark_version
 
 from spyt.dependency_utils import require_yt_client
 require_yt_client()
@@ -344,3 +346,13 @@ def _add_conf(spark_conf, spark_args):
         for k, v in spark_conf.items():
             spark_args.append("--conf")
             spark_args.append("{}={}".format(k, v))
+
+
+def check_spark_version(less_than=None, greater_than_or_equal=None):
+    result = True
+    current_version = Version(spark_version)
+    if less_than:
+        result = result and current_version < Version(less_than)
+    if greater_than_or_equal:
+        result = result and current_version >= Version(greater_than_or_equal)
+    return result
